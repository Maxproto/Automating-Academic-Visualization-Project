# Project: Vision-Language Model Evaluation for Data Visualization Reproduction

## Overview

This project focuses on evaluating vision-language models, specifically ChatGPT-Vision, for their ability to reproduce data visualization figures from academic papers. We employ Zero-shot Chain-of-Thought (CoT) Prompting and direct prompting methods. Our code base involves parsing academic papers, running generated code, and conducting human-led evaluations of the reproduced figures and the effectiveness of prompting techniques.

## Folder Structure

- `combined_cot.json`, `combined_direct.json`: Experiment outputs from ChatGPT-vision using CoT and direct prompting, respectively.
- Jupyter Notebooks:
  - `label_data.ipynb`: Collects outputs from ChatGPT-vision.
  - `compare.ipynb`: Enables human evaluators to compare the effectiveness of CoT vs. direct prompting.
  - `eval.ipynb`: Allows human evaluators to assess the quality of figures generated from the model's output.
- Python Scripts:
  - `parse_from_web.py`: Parses ArXiv information of top-conference papers and downloads their source code.
  - `run_code.py`: Executes code generated by vision-language models.
  - `test_api.py`: Utilizes the latest GPT-4-Vision API to generate code for reproducing figures.
  - `extract_from_folder_neurips.py`: Extracts figures and captions from paper's source code using regular expressions.
- `llava_eval_output_direct.json`: Contains evaluation results from the LLaVa model.
- `dataset_file.json`: The main dataset for the project.

## Key Components

### Combined JSON Files

- **Purpose**: Store the outputs from ChatGPT-vision experiments using different prompting techniques.
- **Usage**: Analyze and compare the effectiveness of CoT and direct prompting in generating accurate data visualizations.

### Jupyter Notebooks

- **Interactive Elements**: Utilize ipywidgets for enhancing human interaction during the evaluation process.
- **Functionality**:
  - `label_data.ipynb`: Used for manually collecting code generated by ChatGPT. This process involves copying code outputs directly from the ChatGPT interface and organizing them in the notebook for further analysis and execution.
  - `compare.ipynb`: Facilitates side-by-side comparisons of CoT and direct prompting methods, enabling human evaluators to assess which approach yields better results.
  - `eval.ipynb`: Provides a platform for human evaluators to judge the visual accuracy of figures generated by the model's code, as well as the effectiveness of the reproduced visualizations.

### Python Scripts

- **Data Preparation**: Scripts like `parse_from_web.py` and `extract_from_folder_neurips.py` are essential for preparing the dataset by extracting figures and academic paper information.
- **Model Execution**: `run_code.py` and `test_api.py` are central to executing the model-generated code and testing the latest API functionalities. (We also use `label_data.ipynb` to Manually copy the code generated by ChatGPT-Vision)

### Evaluation and Dataset Files

- **Evaluation Output**: `llava_eval_output_direct.json` provides insights into the performance of the LLaVa model in this context.
- **Dataset**: `dataset_file.json` forms the backbone of the project, containing all necessary data for evaluations and comparisons.

## Dataset Structure

The dataset is structured as a JSON file, where each entry corresponds to a figure extracted from academic papers. Each entry in the dataset includes the following fields:

- `figure_path`: The path to the extracted figure image, e.g., "neurips_figures/2307.04204/single_depth3_width256_scale0.5.png".
- `caption`: The caption associated with the figure, often containing LaTeX-formatted mathematical expressions, e.g., "$\\alpha=1.0$".
- `source`: The path to the LaTeX source file from which the figure was extracted, e.g., "neurips/2307.04204/A1_exp.tex".
- `arxiv_id`: The identifier for the paper within the NeurIPS dataset, e.g., "neurips/2307.04204".
- `type`: The type of the figure, such as "Line Chart" or "Scatter Plot".

## Evaluation JSON Structure

The evaluation JSON files are integral to the project, offering an in-depth assessment of the figures generated by vision-language models. Each entry in these files includes:

- `code`: The Python code generated by the vision-language model to reproduce the figure.
- `runnable`: A boolean indicating whether the generated code successfully produced a figure.
- `output_figure_path`: Path to the generated figure, e.g., "chat-gpt-v/acl_20/1905.01978/tag_direct.png".
- `eval`: A detailed evaluation of the generated figure, including ratings for axes grids, tick marks, plot type correctness, text elements, color, line styles, numerical accuracy, and figure adaptability.

## Usage Instructions

1. **Data Extraction**: Use `parse_acl_neurips.py` and `extract_from_folder_neurips.py` to prepare your dataset from the specified conference proceedings.
2. **Model Interaction**: Employ `test_api.py` or Manually copy the code generated by ChatGPT using `label_data.ipynb to generate code using GPT-4-Vision API.
3. **Code Execution**: Run the model-generated code using `run_code.py`.
4. **Evaluation**: Use the Jupyter Notebooks to compare prompting methods and evaluate the final visual outputs.
5. **Analysis**: Analyze the experiment outputs stored in `combined_cot.json` and `combined_direct.json` for comprehensive insights.

## Requirements

- Python 3.10
- Jupyter Notebooks
- ipywidgets
- requests
- re
- BeautifulSoup

This project aims to provide a comprehensive framework for evaluating the efficacy of vision-language models in reproducing complex data visualizations, with a focus on academic research papers. The interactive nature of the notebooks and the structured approach to data handling and model evaluation make it a robust tool for researchers and enthusiasts in the field of AI and data visualization.
