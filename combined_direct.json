[
    {
        "figure_path": "acl_20_figures/1905.01978/tag.png",
        "caption": "Details of Tag action tree",
        "source": "acl_20/1905.01978/action_tree.tex",
        "arxiv_id": "acl_20/1905.01978",
        "llava_output": "0</s>",
        "is_data_visual": "Yes (Based on the figure caption \"Details of Tag action tree.\")",
        "code": "import matplotlib.pyplot as plt\nimport networkx as nx\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes and edges to the graph\nG.add_edge(\"Tag\", \"action_reference_object\")\nG.add_edge(\"action_reference_object\", \"location\")\n# Continue adding edges to build your specific tree structure...\n\n# Create position layout for nodes\npos = nx.spring_layout(G)\n\n# Draw the nodes and edges\nnx.draw(G, pos, with_labels=True)\n\n# Save the plot to a PNG file with 300dpi\nplt.savefig(\"tree_diagram.png\", dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1905.01978/tag_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1910.11476/freq_filter.png",
        "caption": "Effect of varying percentage of training samples on Chinese OntoNotes 4.0. BERT-MRC can achieve the same F1-score comparing to BERT-Tagger with fewer training samples.",
        "source": "acl_20/1910.11476/ner.tex",
        "arxiv_id": "acl_20/1910.11476",
        "llava_output": "The image is a line chart showing the influence of training sample numbers on the F1-score of BERT-MRC and BERT-Tagger.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Hypothetical F1-scores for BERT-MRC and BERT-Tagger models.\nbert_mrc_scores = [83, 81, 79, 78.5, 78]\nbert_tagger_scores = [80, 79, 77, 76, 75.5]\ntraining_samples = ['100%', '50%', '25%', '12.5%', '6.75%']\n\nplt.figure(figsize=(10, 6))\n\n# Plot BERT-MRC scores\nplt.plot(training_samples, bert_mrc_scores, 'b-o', label='BERT-MRC')\n\n# Plot BERT-Tagger scores\nplt.plot(training_samples, bert_tagger_scores, 'r-o', label='BERT-Tagger')\n\n# Add labels and title\nplt.xlabel('percentage of training samples')\nplt.ylabel('F1 measure on Chinese OntoNotes')\nplt.title('Influence of training sample numbers')\nplt.legend()\n\n# Add a grid for better readability\nplt.grid(True)\n\n# Save the plot as a .png file with 300 DPI\nplt.savefig('training_influence_plot.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1910.11476/freq_filter_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1910.13267/rare_bpe.jpeg",
        "caption": "BPE",
        "source": "acl_20/1910.13267/analysis.tex",
        "arxiv_id": "acl_20/1910.13267",
        "llava_output": "The image is a scatter plot, which is a graphical representation of data points that are plotted against each other. In this case, the data points are represented by dots, and the plot is color-coded to show the rarity of each data point. The plot is used to display the distribution",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some random data for demonstration purposes\nnp.random.seed(0)\nn = 1000  # total number of points\ntop_10_percent = int(0.1 * n)\n\n# Random data for 'other' group\nx_other = np.random.randn(n - top_10_percent)\ny_other = np.random.randn(n - top_10_percent)\n\n# Random data for 'rare, top 10%' group\nx_rare = np.random.randn(top_10_percent) * 0.5\ny_rare = np.random.randn(top_10_percent) * 0.5\n\n# Plot\nplt.figure(figsize=(10, 10))\nplt.scatter(x_other, y_other, color='red', alpha=0.5, label='other')\nplt.scatter(x_rare, y_rare, color='blue', alpha=0.5, label='rare, top 10%')\n\n# Legend\nplt.legend(loc='upper right')\n\n# Save the figure with 300 DPI\nplt.savefig('scatter_plot.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1910.13267/rare_bpe_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "acl_20_figures/1910.13267/corp_voc_size_v3.png",
        "caption": "BLEU scores. Models trained on random subsets of WMT14 En-Fr.",
        "source": "acl_20/1910.13267/experiments.tex",
        "arxiv_id": "acl_20/1910.13267",
        "llava_output": "3. The image is a line chart showing the BLEU scores of different models.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data - replace these with your actual data points\ntraining_sizes = [500000, 1000000, 2000000, 4000000]\nbpe_4k_scores = [29, 31, 32.5, 33]\nbpe_32k_scores = [29.5, 31.5, 33, 34]\nkudo_4k_scores = [29.2, 31.2, 32.7, 33.2]\nkudo_32k_scores = [29.7, 31.7, 33.2, 33.7]\nbpe_dropout_4k_scores = [29.1, 31.1, 32.6, 33.1]\nbpe_dropout_32k_scores = [29.6, 31.6, 33.1, 33.6]\n\n# Creating the plot\nplt.figure()\n\nplt.plot(training_sizes, bpe_4k_scores, 'o-', label=\"BPE, 4k\", color='#FF7F0E')\nplt.plot(training_sizes, bpe_32k_scores, 'x-', label=\"BPE, 32k\", color='#9467BD')\nplt.plot(training_sizes, kudo_4k_scores, 'o--', label=\"Kudo (2018), 4k\", color='#1F77B4')\nplt.plot(training_sizes, kudo_32k_scores, 'v--', label=\"Kudo (2018), 32k\", color='#8C564B')\nplt.plot(training_sizes, bpe_dropout_4k_scores, 'o-', label=\"BPE-dropout, 4k\", color='#2CA02C')\nplt.plot(training_sizes, bpe_dropout_32k_scores, 'D-', label=\"BPE-dropout, 32k\", color='#2CA02C')\n\nplt.xlabel('Training corpora size')\nplt.ylabel('BLEU')\nplt.title('BLEU scores vs. Training corpora size')\nplt.legend()\nplt.grid(True)\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('bleu_scores_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1910.13267/corp_voc_size_v3_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1910.13267/compare_segm_len_cb.png",
        "caption": "",
        "source": "acl_20/1910.13267/experiments.tex",
        "arxiv_id": "acl_20/1910.13267",
        "llava_output": "The image is a histogram showing the distribution of sentence lengths in a dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some mock data\nnp.random.seed(0)\nsentence_lengths = np.arange(0, 150, 25)\nbpe_counts = np.random.randint(200, 1000, len(sentence_lengths))\nbpe_dropout_counts = np.random.randint(100, 800, len(sentence_lengths))\n\n# Create the bar plot\nfig, ax = plt.subplots()\n\n# Bar plots with some transparency (alpha) and a pattern (hatch)\nax.bar(sentence_lengths - 5, bpe_counts, width=10, label='BPE segmentation', color='orange', alpha=0.7, hatch='/', edgecolor='black')\nax.bar(sentence_lengths + 5, bpe_dropout_counts, width=10, label='BPE-dropout segmentation', color='red', alpha=0.7, edgecolor='black')\n\n# Add some random scatter points on top of the bars like in the original image\nfor i, length in enumerate(sentence_lengths):\n    y_vals_bpe = np.random.uniform(bpe_counts[i] - 50, bpe_counts[i] + 50, size=7)\n    y_vals_dropout = np.random.uniform(bpe_dropout_counts[i] - 50, bpe_dropout_counts[i] + 50, size=7)\n    ax.scatter(np.full_like(y_vals_bpe, length - 5), y_vals_bpe, color='black')\n    ax.scatter(np.full_like(y_vals_dropout, length + 5), y_vals_dropout, color='black')\n\n# Set labels and title (if necessary)\nax.set_xlabel('Sentence length (tokens)')\nax.set_ylabel('# of sentences')\n\n# Add a legend\nax.legend()\n\n# Save the plot\nplt.savefig('plot.png', dpi=300)\n\n# To show the plot interactively (if running in a Jupyter notebook or similar)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1910.13267/compare_segm_len_cb_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.00536/sampleimage.png",
        "caption": "Human evaluation template",
        "source": "acl_20/1911.00536/LSP.tex",
        "arxiv_id": "acl_20/1911.00536",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Create a figure and a set of subplots.\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Create a rectangle for outline\nax.add_patch(Rectangle((0.05, 0.05), 0.9, 0.9, fill=None, edgecolor='black'))\n\n# Create radio buttons like rectangles\nax.add_patch(Rectangle((0.1, 0.6), 0.02, 0.04, facecolor='gray'))\nax.add_patch(Rectangle((0.1, 0.55), 0.02, 0.04, facecolor='gray'))\nax.add_patch(Rectangle((0.1, 0.45), 0.02, 0.04, facecolor='gray'))\n# and so on for each option you want to display...\n\n# Add fake text for context and options with text() method\nax.text(0.15, 0.62, 'RESPONSE #1', fontsize=12)\nax.text(0.15, 0.57, 'Neither or Equally', fontsize=12)\nax.text(0.15, 0.52, 'RESPONSE #2', fontsize=12)\n# and so on for each option label...\n\n# Hide the axes\nax.axis('off')\n\n# Save the figure\nplt.savefig('survey_figure.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.00536/sampleimage_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/layernorm_variant.png",
        "caption": "Two variants of layer normalization",
        "source": "acl_20/1911.02692/appendix.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Create a new figure and axis\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Hide the axes\nax.axis('off')\n\n# Function to draw a block with two components\ndef draw_block(ax, x, y, width, height, block_color, text1, text2):\n    rect = Rectangle((x, y), width, height, facecolor=block_color, edgecolor='black', joinstyle='round')\n    ax.add_patch(rect)\n    plt.text(x + width/4, y + height/2, text1, va='center', ha='center', color='white')\n    plt.text(x + 3*width/4, y + height/2, text2, va='center', ha='center', color='white')\n\n# Function to draw a labeled rectangle\ndef draw_rect(ax, x, y, width, height, label, label_color):\n    rect = Rectangle((x, y), width, height, facecolor=label_color, edgecolor='black', joinstyle='round')\n    ax.add_patch(rect)\n    plt.text(x + width/2, y + height/2, label, va='center', ha='center', color='black')\n\n# Set up colors and positions\nnorm_color = '#ADD8E6'\nsoftmax_linear_color = '#87CEEB'\nattention_color = '#FFA07A'\nnorm_label_color = '#5F9EA0'\nsoftmax_label_color = '#4682B4'\nattention_label_color = '#CD5C5C'\n\n# Draw the blocks and rectangles\ndraw_block(ax, 0.2, 0.6, 0.2, 0.1, softmax_linear_color, 'Softmax', 'Linear')\ndraw_block(ax, 0.8, 0.6, 0.2, 0.1, softmax_linear_color, 'Softmax', 'Linear')\n\ndraw_rect(ax, 0.1, 0.8, 0.4, 0.1, 'Add & Norm', norm_label_color)\ndraw_rect(ax, 0.7, 0.8, 0.2, 0.1, 'Add', norm_label_color)\ndraw_rect(ax, 0.7, 0.7, 0.2, 0.1, 'Norm', norm_color)\n\ndraw_rect(ax, 0.3, 0.7, 0.2, 0.1, 'Multihead\\nAttention', attention_label_color)\ndraw_rect(ax, 0.9, 0.7, 0.2, 0.1, 'Multihead\\nAttention', attention_label_color)\n\n# Set the limits of the plot\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\n\n# Save the figure\nplt.savefig('layer_normalization_variants.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/layernorm_variant_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/explain.png",
        "caption": "Domain proportion of a sentence from the TED domain for English-to-French task. The domain proportion is extracted from all layers of the encoder.",
        "source": "acl_20/1911.02692/experiment.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Assuming we have some data for the heatmap.\n# The example below uses a random 5x5 matrix for demonstration purposes.\ndata = np.random.rand(5, 5)\n\n# Generate a figure and axis with matplotlib.\nfig, ax = plt.subplots()\n\n# Create a heatmap using the pcolor method.\ncmap = mcolors.LinearSegmentedColormap.from_list(\"mycmap\", [\"white\", \"black\"])\nheatmap = ax.pcolor(data, cmap=cmap)\n\n# Add color bar to the right of the heatmap.\nplt.colorbar(heatmap)\n\n# Optionally set the aspect of the plot to 'equal' to scale the plot nicely.\nax.set_aspect('equal')\n\n# You can add labels, titles or a grid if you want to.\nax.set_title(\"Domain Proportion for English-to-French Task\")\n\n# Save the figure as a PNG file with 300 DPI.\nplt.savefig(\"heatmap.png\", dpi=300)\n\n# Display the plot.\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/explain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/prop.png",
        "caption": "Domain proportions of a sentence pair for English-to-German task. White represents the News domain and black represents the TED domain. The domain proportions of both the encoder (bottom) and the decoder (top) are presented.",
        "source": "acl_20/1911.02692/experiment.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Example random data to mimic the domain proportions\nencoder_data = np.random.rand(10, 50)  # replace with actual data for the encoder\ndecoder_data = np.random.rand(10, 50)  # replace with actual data for the decoder\n\n# Set the color map to grayscale, with 'white' representing the News domain\n# and 'black' representing the TED domain\ncmap = mcolors.ListedColormap(['black', 'white'])\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 4))\n\n# Remove ticks and labels\nax1.axis('off')\nax2.axis('off')\n\n# Plot the decoder data\nax1.imshow(decoder_data, aspect='auto', cmap=cmap, interpolation='nearest')\n\n# Plot the encoder data\nax2.imshow(encoder_data, aspect='auto', cmap=cmap, interpolation='nearest')\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('domain_proportions.png', dpi=300)\n\n# Show the plot (omit this line if you don't want to display the image)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/prop_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/gradientflow.png",
        "caption": "Back-propagation for different embedding based methods.",
        "source": "acl_20/1911.02692/experiment.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "The image is a line chart showing the back-propagation for different embedding based methods. The chart has several lines, each representing a different method, and the y-axis shows the back-propagation values. The chart is black and white, and the lines are labeled with the names of the",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Create a new figure\nfig, ax = plt.subplots(figsize=(10, 3))\n\n# Function to draw a rectangle with a centered text inside\ndef draw_rect(ax, center, width, height, color, text):\n    rect = Rectangle((center[0] - width / 2, center[1] - height / 2), width, height, linewidth=1,\n                     edgecolor=color, facecolor=color, alpha=0.5)\n    ax.add_patch(rect)\n    ax.text(center[0], center[1], text, ha='center', va='center', color='black', weight='bold')\n\n# Function to draw an arrow between two points\ndef draw_arrow(ax, from_point, to_point, color):\n    ax.annotate('', xy=to_point, xycoords='data', xytext=from_point, textcoords='data',\n                arrowprops=dict(arrowstyle=\"->\", color=color, lw=2))\n\n# Define coordinates and sizes for the rectangles\nsoftmax_linear_coords = [(1, 0.5), (4, 0.5), (7, 0.5), (10, 0.5)]\nembedding_coords = [(1, -1.5), (4, -1.5), (7, -1.5), (10, -1.5)]\n\nrec_width, rec_height = 2, 0.8\n\n# Draw rectangles and arrows\nfor sl, em in zip(softmax_linear_coords, embedding_coords):\n    draw_rect(ax, sl, rec_width, rec_height, 'blue', 'Softmax\\nLinear')\n    draw_rect(ax, em, rec_width, rec_height, 'yellow', '')\n\n    # Draw 5 arrows from each softmax/linear box to each embedding box\n    for i in range(5):\n        offset = (i - 2) * 0.25  # To spread the arrows\n        draw_arrow(ax, (sl[0] + offset, rec_height / 2), (em[0] + offset, -rec_height / 2), 'red')\n\n# Add dashed box around the last set of softmax/linear and embedding boxes\ndashed_box = Rectangle((softmax_linear_coords[-1][0] - rec_width / 2, embedding_coords[-1][1] - rec_height / 2),\n                       rec_width, rec_height * 3, linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\nax.add_patch(dashed_box)\n\n# Set aspect of the plot to be equal\nax.set_aspect('equal')\n\n# Hide axes\nax.axis('off')\n\n# Save the figure\nplt.savefig('backprop_diagram.png', dpi=300)\n\n# Display the figure\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/gradientflow_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/wordlevelmix.png",
        "caption": "Word-level mixing with 3 domains. For simplicity, we omit the subscripts $Q,i$.",
        "source": "acl_20/1911.02692/methodology.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "The image is a line chart showing the results of an experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Define colors\ncolor_yellow = '#FFD700'\ncolor_peach = '#FFDAB9'\ncolor_gray = '#808080'\ncolor_dark_gray = '#A9A9A9'\ncolor_light_gray = '#D3D3D3'\ncolor_dark_blue = '#0000FF'\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Function to draw a grid of colored boxes\ndef draw_grid(ax, x, y, num_rows, num_cols, width, height, color, label=None):\n    for i in range(num_rows):\n        for j in range(num_cols):\n            rect = patches.Rectangle((x + j * width, y - i * height), width, height, linewidth=1, edgecolor='black', facecolor=color)\n            ax.add_patch(rect)\n    if label is not None:\n        ax.text(x + num_cols * width + 0.05, y - (num_rows - 1) * height / 2, label, va='center', fontsize=12, bbox=dict(boxstyle=\"round\", facecolor=\"wheat\"))\n\n# Draw grids of boxes\ngrid_width, grid_height = 0.1, 0.1\ndraw_grid(ax, 0, 1, 1, 5, grid_width, grid_height, color_yellow)\ndraw_grid(ax, 0, 0.7, 1, 4, grid_width, grid_height, color_peach, 'W1')\ndraw_grid(ax, 0, 0.4, 1, 3, grid_width, grid_height, color_peach, 'W2')\ndraw_grid(ax, 0, 0.1, 1, 2, grid_width, grid_height, color_peach, 'W3')\n\ndraw_grid(ax, 0.8, 1, 1, 5, grid_width, grid_height, color_dark_gray)\ndraw_grid(ax, 0.8, 0.7, 1, 4, grid_width, grid_height, color_gray)\ndraw_grid(ax, 0.8, 0.4, 1, 3, grid_width, grid_height, color_light_gray)\n\n# Softmax/Linear label\nax.text(1.2, 0.55, 'Softmax\\nLinear', va='center', ha='center', fontsize=12, bbox=dict(boxstyle=\"round\", facecolor=color_dark_blue))\n\n# Set limits and turn off axes\nax.set_xlim(0, 1.5)\nax.set_ylim(0, 1.2)\nax.axis('off')\n\n# Save the figure with 300 DPI\nplt.savefig('word_level_mixing.png', dpi=300, bbox_inches='tight', pad_inches=0)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/wordlevelmix_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/explain.png",
        "caption": "Domain proportion of a sentence from the TED domain for English-to-French task. The domain proportion is extracted from all layers of the encoder.",
        "source": "acl_20/1911.02692/tech report/experiment.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have some data for the heatmap in the form of a 2D array\n# Replace this with your actual data from the paper.\ndata = np.random.rand(10, 6)  # 10 layers and 6 domains as an example\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nplt.imshow(data, cmap='Greys', interpolation='nearest')\n\n# Optionally add labels, title, etc.\nplt.title('Domain Proportion of a Sentence from the TED Domain')\nplt.xlabel('Domains')\nplt.ylabel('Encoder Layers')\n\n# Adding annotations if needed, based on your data's specifics\nfor i in range(data.shape[0]):\n    for j in range(data.shape[1]):\n        plt.text(j, i, f'{data[i, j]:.2f}', ha='center', va='center', color='black')\n\n# Save the figure with high resolution\nplt.savefig('heatmap.png', dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/explain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/prop.png",
        "caption": "Domain proportions of a sentence pair for English-to-German task. White represents the News domain and black represents the TED domain. The domain proportions of both the encoder (bottom) and the decoder (top) are presented.",
        "source": "acl_20/1911.02692/tech report/experiment.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "The image is a bar chart that shows the domain proportions of a sentence pair for an English-to-German task. The chart is divided into two sections, with the top section representing the encoder domain proportions and the bottom section representing the decoder domain proportions. The chart is black and",
        "is_data_visual": "Yes. Based on the justification and figure caption provided, the image is a data visualization bar chart presenting experimental results for the domain proportions of a sentence pair for an English-to-German task.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Setting the dimensions of the data\nn_sentences = 50  # Number of sentence pairs\ndomain_proportion = 0.7  # Approximate average domain proportion for News\n\n# Randomly generating data. This part should be replaced with actual data.\nencoder_news = np.random.binomial(1, domain_proportion, (2, n_sentences))\ndecoder_news = np.random.binomial(1, domain_proportion, (2, n_sentences))\n\n# Setting the color map to grayscale where 0 is black, 1 is white\ncmap = 'gray'\n\nfig, ax = plt.subplots(2, 1, figsize=(15, 2), sharex=True)\n\n# Encoder\nax[0].imshow(encoder_news, aspect='auto', cmap=cmap)\nax[0].set_ylabel('Encoder')\nax[0].set_yticks([])\nax[0].set_yticklabels([])\nax[0].set_xticks([])\n\n# Decoder\nax[1].imshow(decoder_news, aspect='auto', cmap=cmap)\nax[1].set_ylabel('Decoder')\nax[1].set_yticks([])\nax[1].set_yticklabels([])\nax[1].set_xticks([])\n\n# Remove the space between subplots\nplt.subplots_adjust(wspace=0, hspace=0)\n\n# Layout settings and saving the figure\nplt.tight_layout()\nplt.savefig('domain_proportions.png', dpi=300)\n\n# Show plot for verification (optional)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/prop_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/gradientflow.png",
        "caption": "Back-propagation for different embedding based methods.",
        "source": "acl_20/1911.02692/tech report/experiment.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "The image is a line chart showing the back-propagation for different embedding based methods.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyBboxPatch\n\n# Define a function to draw a single network diagram\ndef draw_network(ax, pos, dashed=False):\n    # Draw the rectangles for Softmax and Linear layers\n    softmax_rect = FancyBboxPatch((pos, 0.4), 0.8, 0.2, boxstyle=\"round,pad=0.1\", color=\"blue\", ec=\"black\")\n    linear_rect = FancyBboxPatch((pos, -0.2), 0.8, 0.2, boxstyle=\"round,pad=0.1\", color=\"yellow\", ec=\"black\")\n    \n    # Draw annotations\n    for i in range(5):\n        # Draw embedding rectangles\n        rect = FancyBboxPatch((pos + 0.1 * i, 1), 0.05, 0.2, boxstyle=\"square,pad=0.02\", color=\"gray\", ec=\"black\")\n        ax.add_patch(rect)\n        # Draw arrows\n        ax.annotate('', xy=(pos + 0.1 * i, 0.4), xytext=(pos + 0.1 * i, 0.6), \n                    arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n        \n        # Draw dashed lines if needed\n        if dashed and i > 1:\n            ax.annotate('', xy=(pos + 0.1 * i, 0), xytext=(pos + 0.1 * i, -0.2), \n                        arrowprops=dict(arrowstyle=\"->\", linestyle='dashed', color=\"red\"))\n        else:\n            ax.annotate('', xy=(pos + 0.1 * i, 0), xytext=(pos + 0.1 * i, -0.2), \n                        arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n    \n    # Add the drawn layers to the axis\n    ax.add_patch(softmax_rect)\n    ax.add_patch(linear_rect)\n    ax.text(pos + 0.4, 0.5, \"Softmax\", ha='center', va='center', color=\"white\")\n    ax.text(pos + 0.4, -0.1, \"Linear\", ha='center', va='center', color=\"black\")\n\n# Define the figure and axis\nfig, ax = plt.subplots(figsize=(12, 3))\nax.set_xlim(0, 2.4)\nax.set_ylim(-0.5, 1.5)\nax.axis('off')\n\n# Draw two network diagrams\ndraw_network(ax, 0.2)\ndraw_network(ax, 1.4, dashed=True)\n\n# Save the figure\nplt.savefig('backpropagation_diagram.png', dpi=300)\n\n# Optionally display the plot if you like\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/gradientflow_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02692/wordlevelmix.png",
        "caption": "Word-level mixing with 3 domains. For simplicity, we omit the subscripts $Q,i$.",
        "source": "acl_20/1911.02692/tech report/methodology.tex",
        "arxiv_id": "acl_20/1911.02692",
        "llava_output": "The image is a line chart showing the results of an experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef create_rectangles(ax, centers, color, label=None):\n    for center in centers:\n        rect = patches.Rectangle(center, 0.8, 0.4, linewidth=1, edgecolor='none', facecolor=color, label=label)\n        ax.add_patch(rect)\n        if label:\n            label = None  # To prevent repeating labels\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Set axis limits and remove axes\nax.set_xlim(0, 5)\nax.set_ylim(0, 3)\nax.axis('off')\n\n# Create rectangles (these coordinates and colors are placeholders)\ncreate_rectangles(ax, [(0.5, 2), (1.5, 2), (2.5, 2)], 'yellow', 'W1')\ncreate_rectangles(ax, [(0.5, 1), (1.5, 1), (2.5, 1)], 'orange', 'W2')\ncreate_rectangles(ax, [(0.5, 0), (1.5, 0), (2.5, 0)], 'peachpuff', 'W3')\ncreate_rectangles(ax, [(3.5, 2), (4.0, 2), (4.5, 2)], 'gray', None)\ncreate_rectangles(ax, [(3.5, 1), (4.0, 1), (4.5, 1)], 'darkgray', None)\ncreate_rectangles(ax, [(3.5, 0), (4.0, 0)], 'lightgray', None)\n\n# Add legend for linear and softmax\nax.legend([patches.Patch(color='blue', label='Softmax'), \n           patches.Patch(color='lightblue', label='Linear')],\n          loc='lower right', frameon=False)\n\n# Save figure\nplt.savefig(\"word_level_mixing.png\", dpi=300)\n\nplt.show()",
        "runnable": false,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02692/wordlevelmix_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02707/central.png",
        "caption": "Distribution of Attention Score. The distributions of Overall (all concepts of the certain part), Golden (concepts in the golden response) and Zero-hop (concepts appear in the post) are presented. The attention score is calculated by scaling the mean of attention scores of $n$ step decoding.",
        "source": "acl_20/1911.02707/evaluation.tex",
        "arxiv_id": "acl_20/1911.02707",
        "llava_output": "The image is a line chart that presents the distribution of attention score for different concepts.</s>",
        "is_data_visual": "Based on the provided justification and caption, yes. The image represents a data visualization plot (line chart) presenting experimental results in the form of the distribution of attention scores for different concepts. The plot shows three distributions for the overall attention score, golden attention score, and zero-hop attention score, which are calculated based on the mean of focus weighted attention scores of $n$ step decoding.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulating some data for demonstration purposes\nnp.random.seed(0)  # For reproducibility\noverall_scores = np.random.beta(a=2, b=5, size=1000)\ngolden_scores = np.random.beta(a=2, b=2, size=1000)\nzerohop_scores = np.random.beta(a=1, b=3, size=1000)\n\n# Creating a histogram of the simulated data with KDE-like plot\ncount_bins = 30\n\nplt.hist(overall_scores, bins=count_bins, density=True, alpha=0.5, color='blue', linestyle='dashed', linewidth=2, histtype='step', label='Overall')\nplt.hist(golden_scores, bins=count_bins, density=True, alpha=0.5, color='orange', linestyle='dashed', linewidth=2, histtype='step', label='Golden')\nplt.hist(zerohop_scores, bins=count_bins, density=True, alpha=0.5, color='green', linewidth=2, histtype='step', label='Zero-hop')\n\n# Adding labels and title (omitted in this example)\nplt.xlabel('Scaled Attention Score')\nplt.ylabel('Prob Dense Func')\n\n# Adding a legend\nplt.legend()\n\n# Saving the plot with 300 DPI\nplt.savefig('distribution_of_attention_score.png', dpi=300)\n\n# Displaying the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02707/central_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02782/oa_distro_percs.png",
        "caption": "Distribution of papers by Microsoft Academic field of study.",
        "source": "acl_20/1911.02782/main.tex",
        "arxiv_id": "acl_20/1911.02782",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data\nfields = [\n    'Medicine', 'Biology', 'Physics', 'Mathematics', 'Unclassified', 'CompSci',\n    'Chemistry', 'Psychology', 'Engineering', 'MatSci', 'Economics', 'Geology',\n    'Business', 'Sociology', 'PoliSci', 'Geography', 'EnvSci', 'History', 'Art',\n    'Philosophy'\n]\npercent_values = [\n    20, 18, 15, 12, 10, 8, 7, 6, 5, 4, 4, 3, 3, 2, 2, 2, 1, 1, 0.5, 0.2\n]\n\n# Sort the sample data by the percentage values (assuming the image shows sorted categories)\nfields = [x for _, x in sorted(zip(percent_values, fields), reverse=True)]\npercent_values = sorted(percent_values, reverse=True)\n\n# Create the horizontal bar plot\nplt.figure(figsize=(10, 8))\nplt.barh(fields, percent_values, color='blue')\n\n# Set the title and labels\nplt.xlabel('Percent')\nplt.title('Distribution of papers by field of study')\n\n# Remove the spines\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\n\n# Draw the data values as text\nfor i, v in enumerate(percent_values):\n    plt.text(v + 0.2, i, str(v), color='blue', va='center')\n\n# Save the figure with 300 DPI\nplt.savefig('distribution_of_papers.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02782/oa_distro_percs_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.02782/numeric_representations.png",
        "caption": "Visualization of contextual representations from layer 9 of \\gorbert on numeric surface forms in a subsample of body text from \\gorc. Labels are heuristics based on token-level patterns.",
        "source": "acl_20/1911.02782/main.tex",
        "arxiv_id": "acl_20/1911.02782",
        "llava_output": "The image is a scatter plot, which shows the relationships between different variables.</s>",
        "is_data_visual": "Answer: Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate mock data: 1000 points with 2 dimensions\nnp.random.seed(0)\ndata_points = np.random.randn(1000, 2)\nlabels = np.random.choice(\n    ['other', 'numeric_citation', 'year', 'word', 'time', 'pdf_reference', 'decimal', 'latex'],\n    size=1000\n)\n\n# Plot style to match image as closely as possible\nplt.figure(figsize=(10, 8))\ncolors = {'other': 'blue', 'numeric_citation': 'orange', 'year': 'green', \n          'word': 'red', 'time': 'purple', 'pdf_reference': 'brown', \n          'decimal': 'pink', 'latex': 'gray'}\n\n# Plot each label with a different color\nfor label in np.unique(labels):\n    idx = labels == label\n    plt.scatter(data_points[idx, 0], data_points[idx, 1], \n                label=label, alpha=0.5, c=colors[label])\n\n# Title and legend to match the image\nplt.title('Contextual Numeric Surface Forms, Layer 9')\nplt.legend(title='Legend', loc='lower center', bbox_to_anchor=(0.5, -0.05),\n           fancybox=True, shadow=True, ncol=4)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('plot.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.02782/numeric_representations_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.03642/sec33fig2.png",
        "caption": "Proportion of sentences corresponding to a given relation over total sentences in WikiGenderBias for each entity. This demonstrates that, of the entities we sampled to create WikiGenderBias, the spouse relation is expressed more often relative to the birthdate, birthplace, and hypernym relations in articles about female entities than in articles about male entities. Additionally, hypernym is mentioned more often relative to the other relations in articles about male entities than in articles about female entities.",
        "source": "acl_20/1911.03642/acl2020.tex",
        "arxiv_id": "acl_20/1911.03642",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# The dummy data - these would differ according to your actual data set\ncategories = ['Spouse', 'Birthdate', 'Birthplace', 'Hypernym', 'NA']\nmale_data = [0.1, 0.2, 0.3, 0.4, 0.0]\nfemale_data = [0.15, 0.15, 0.2, 0.3, 0.2]\n\n# Indices of the categories\nind = range(len(categories))\n\n# Stacked bar plot\np1 = plt.bar(ind, male_data, color='black')\np2 = plt.bar(ind, female_data, bottom=male_data, color='grey', hatch='//')\n\n# Create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# Stacked bar plot\nax.bar(0, sum(male_data), color='grey')\nax.bar(0, male_data[0], color='black', hatch='//')\n\nax.bar(1, sum(female_data), color='grey')\nax.bar(1, female_data[0], color='black', hatch='//')\n\n# Getting each individual section by subtracting the previous bottom from the cumulative sum\nmale_cumulative = [sum(male_data[:i]) for i in range(1, len(male_data)+1)]\nfemale_cumulative = [sum(female_data[:i]) for i in range(1, len(female_data)+1)]\n\n# We start from 1 since the first section is already plotted\nfor i in range(1, len(categories)):\n    ax.bar(0, male_data[i], bottom=male_cumulative[i-1], color='grey')\n    ax.bar(1, female_data[i], bottom=female_cumulative[i-1], color='grey')\n\n# Set labels for the x-axis\nax.set_xticks([0, 1])\nax.set_xticklabels(['Male', 'Female'])\n\n# Set labels, legend and grid\nplt.ylabel('Proportion')\nplt.title('Proportion of relations by gender')\nplt.legend((p2[0], p1[0]), categories, loc='lower right')\n\n# Show grid\nplt.grid(axis='y', which='both')\n\n# Save figure as a png file with 300 dpi\nplt.savefig('stacked_bar_plot.png', dpi=300)\n\n# Show plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.03642/sec33fig2_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.03850/notation3.png",
        "caption": "Progression of steps taken during a scientific assessment of claims from empirical observations.",
        "source": "acl_20/1911.03850/main.tex",
        "arxiv_id": "acl_20/1911.03850",
        "llava_output": "The image is a flow chart that outlines the progression of steps taken during a scientific assessment of claims from empirical observations.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Create a figure and axes with a specific size\nfig, ax = plt.subplots(figsize=(6, 8))\n\n# Hide the axes\nax.axis('off')\n\n# Function to draw rectangle elements with text inside\ndef draw_rect(ax, center, width, height, text, linestyle='solid'):\n    # Create a rectangle\n    rect = Rectangle((center[0] - width / 2, center[1] - height / 2), width, height, \n                     linewidth=1, edgecolor='black', facecolor='none', linestyle=linestyle)\n    # Add the rectangle to the axes\n    ax.add_patch(rect)\n    # Add text inside the rectangle\n    ax.text(center[0], center[1], text, ha='center', va='center')\n\n# Define positions and sizes\nrect_width, rect_height = 1.6, 0.5\nrect_centers = [(2, 7), (2, 6), (2, 3)]\n\n# Draw rectangles\ndraw_rect(ax, rect_centers[0], rect_width, rect_height, 'Input\\nInstances')\ndraw_rect(ax, (1, 5), rect_width * 0.4, rect_height, '$S_1$')\ndraw_rect(ax, (3, 5), rect_width * 0.4, rect_height, '$S_2$')\ndraw_rect(ax, rect_centers[1], rect_width, rect_height, 'Metric $M(\\\\cdot)$')\ndraw_rect(ax, rect_centers[2], rect_width, rect_height, 'Observations', linestyle='dotted')\n\n# Draw dashed rectangles\ndraw_rect(ax, (5, 7), rect_width, rect_height, 'Claims about the inherent properties\\n$\\\\theta_1, \\\\theta_2$ of the two systems', linestyle='dotted')\ndraw_rect(ax, (5, 5), rect_width, rect_height, 'Hypotheses:\\n$H_0: \\\\theta_1 = \\\\theta_2$\\n...', linestyle='dotted')\ndraw_rect(ax, (5, 3), rect_width, rect_height, 'Conclusions validating\\n(or not) the hypotheses.', linestyle='dotted')\n\n# Draw arrows between rectangles\nax.annotate('', xy=(2, 6.5), xytext=(2, 6.5 - rect_height), arrowprops=dict(arrowstyle='->'))\nax.annotate('', xy=(1, 5 - rect_height / 2), xytext=(2, 4), arrowprops=dict(arrowstyle='->'))\nax.annotate('', xy=(3, 5 - rect_height / 2), xytext=(2, 4), arrowprops=dict(arrowstyle='->'))\nax.annotate('', xy=(2, 3 - rect_height / 2), xytext=(2, 3 - rect_height), arrowprops=dict(arrowstyle='->'))\nax.annotate('', xy=(2, 7 - rect_height / 2), xytext=(5, 7 - rect_height / 2), arrowprops=dict(arrowstyle='->'))\n\n# Render the plot and save it as a PNG file with 300dpi resolution\nplt.axis('equal')\nplt.savefig('data_visualization.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.03850/notation3_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/1911.03850/BinomialQACompare_posterior_theta2.png",
        "caption": "Posterior distributions of two systems (bottom row) and their difference (top row) after observing the performances on both datasets.",
        "source": "acl_20/1911.03850/main.tex",
        "arxiv_id": "acl_20/1911.03850",
        "llava_output": "The image is a scatter plot showing the posterior distributions of two systems and their difference after observing the performances on both datasets.</s>",
        "is_data_visual": "Yes.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Generate mock data for the histograms\ndata1 = np.random.normal(0.644, 0.005, 1000)\ndata2 = np.random.normal(0.602, 0.005, 1000)\ndata_diff = data1 - data2\n\n# Function to calculate Highest Posterior Density (HPD) interval\ndef hpd(data, level=0.95):\n    sorted_data = np.sort(data)\n    ci_idx = int(np.ceil(level * len(sorted_data)))\n    n_cis = len(sorted_data) - ci_idx\n    ci_width = [sorted_data[i + ci_idx] - sorted_data[i] for i in range(n_cis)]\n    ci_min_idx = ci_width.index(min(ci_width))\n    hpd_interval = (sorted_data[ci_min_idx], sorted_data[ci_min_idx + ci_idx])\n    return hpd_interval\n\n# Plotting the histograms\nfig, axes = plt.subplots(2, 1, figsize=(8, 8))\n\n# Histogram for the difference\nhpd_interval_diff = hpd(data_diff)\nmode_diff = np.round(data_diff.mean(), 3)\naxes[0].hist(data_diff, bins=30, color='skyblue', edgecolor='black')\naxes[0].set_xlim(-0.1, 0.1)\naxes[0].axvline(x=hpd_interval_diff[0], color='red')\naxes[0].axvline(x=hpd_interval_diff[1], color='green')\naxes[0].text(0.03, 200, f'mode={mode_diff}\\n0.0% <0< 100.0%\\n95% HPD')\n\n# Histograms for the two systems\nhpd_interval_1 = hpd(data1)\nmode_1 = np.round(data1.mean(), 3)\naxes[1].hist(data1, bins=30, color='skyblue', edgecolor='black')\naxes[1].set_xlim(0.62, 0.68)\naxes[1].text(0.629, 60, f'mode={mode_1}\\n95% HPD')\n\nhpd_interval_2 = hpd(data2)\nmode_2 = np.round(data2.mean(), 3)\naxes[1].hist(data2, bins=30, color='skyblue', edgecolor='black')\naxes[1].set_xlim(0.58, 0.62)\naxes[1].text(0.585, 60, f'mode={mode_2}\\n95% HPD')\n\n# Save the plot as a PNG file with 300 dpi\nplt.tight_layout()\nplt.savefig('posterior_distributions.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/1911.03850/BinomialQACompare_posterior_theta2_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2002.02031/regression-big.png",
        "caption": "Results of re-training BERT funniness regression as more data becomes available in FunLines.",
        "source": "acl_20/2002.02031/sec5-experiments.tex",
        "arxiv_id": "acl_20/2002.02031",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes. The image represents a data visualization plot, specifically a line chart, depicting the results of re-training BERT funniness regression as more data becomes available in FunLines.",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming these are the data points from the image.\ndata_points = [\n    (1000, 0.56),\n    (2000, 0.54),\n    (3000, 0.55),\n    (4000, 0.53),\n    (5000, 0.51),\n    (6000, 0.49),\n    (7000, 0.48),\n]\n\n# Separate the x and y values.\nx_values, y_values = zip(*data_points)\n\n# Create the plot.\nplt.figure(figsize=(10, 5))\nplt.plot(x_values, y_values, marker='o', color='blue')\n\n# Label the axes.\nplt.xlabel('No. of earliest fully rated headlines used in training')\nplt.ylabel('RMSE')\n\n# Save the figure with 300 dpi resolution.\nplt.savefig('plot.png', dpi=300)\n\n# Optionally display the plot.\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2002.02031/regression-big_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "acl_20_figures/2002.04793/framework.jpeg",
        "caption": "Framework of \\toolkitname. The top block shows different approaches to build a dialogue system.",
        "source": "acl_20/2002.04793/ms.tex",
        "arxiv_id": "acl_20/2002.04793",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(8, 10))\n\n# Hide the axes\nax.axis('off')\n\n# Add rectangles for \"Build\" \"Evaluate\" and \"Diagnose\"\nax.add_patch(Rectangle((0.1, 0.7), 0.2, 0.05, linewidth=1, edgecolor='none', facecolor='grey'))\nax.add_patch(Rectangle((0.1, 0.6), 0.2, 0.05, linewidth=1, edgecolor='none', facecolor='grey'))\nax.add_patch(Rectangle((0.1, 0.5), 0.2, 0.05, linewidth=1, edgecolor='none', facecolor='grey'))\n\n# Add text inside the rectangles\nax.text(0.2, 0.725, 'Build', va='center', ha='center', weight='bold')\nax.text(0.2, 0.625, 'Evaluate', va='center', ha='center', weight='bold')\nax.text(0.2, 0.525, 'Diagnose', va='center', ha='center', weight='bold')\n\n# Create other plot elements as needed, however, replicating the exact diagram as shown in the image\n# is a complex task and requires detailed drawing commands which aren't trivial to generate\n# without knowing more specifics about the components like the icons and text boxes.\n# Therefore, the following is just an illustrative placeholder:\n\n# Placeholder for system agent - gear icon is not available in matplotlib by default\nax.text(0.05, 0.775, 'System\\nAgent', bbox={'facecolor': 'white'}, ha='center')\n\n# Placeholder for user agent - robot icon is not available in matplotlib by default\nax.text(0.95, 0.775, 'User\\nAgent', bbox={'facecolor': 'white'}, ha='center')\n\n# Placeholder for human - person icon is not available in matplotlib by default\nax.text(0.95, 0.625, 'Human', bbox={'facecolor': 'white'}, ha='center')\n\n# Placeholder for configurations box\nax.text(0.5, 0.775, 'Different\\nconfigurations', bbox={'facecolor': 'white'}, ha='center')\n\n# Placeholder for analysis and interactive tool\nax.text(0.5, 0.525, 'Analysis &\\nInteractive Tool', bbox={'facecolor': 'white'}, ha='center')\n\n# Placeholder for flow lines\nax.arrow(0.3, 0.725, 0.15, 0, head_width=0.02, head_length=0.02, fc='k', ec='k')\nax.arrow(0.5, 0.625, 0.3, 0, head_width=0.02, head_length=0.02, fc='k', ec='k')\nax.arrow(0.5, 0.525, 0.3, 0, head_width=0.02, head_length=0.02, fc='k', ec='k')\n\n# Save the figure with 300 DPI\nplt.savefig('dialogue_system.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2002.04793/framework_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.02015/AOPC_lstm_sst.png",
        "caption": "The AOPC and log-odds for LSTM on the SST dataset.",
        "source": "acl_20/2004.02015/appendix.tex",
        "arxiv_id": "acl_20/2004.02015",
        "llava_output": "The image is a line chart showing the AOPC and log-odds for LSTM on the SST dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder datasets (create your own data based on your research findings)\nk = np.array([5, 10, 20, 30, 40, 50])\nleave_one_out = np.array([0.1, 0.3, 0.45, 0.55, 0.6, 0.65])\nlime = np.array([0.12, 0.32, 0.47, 0.57, 0.62, 0.66])\nl_shapley = np.array([0.11, 0.31, 0.46, 0.56, 0.61, 0.67])\nc_shapley = np.array([0.13, 0.34, 0.49, 0.59, 0.64, 0.69])\ncd = np.array([0.09, 0.29, 0.44, 0.54, 0.59, 0.64])\nhedge = np.array([0.14, 0.35, 0.5, 0.6, 0.65, 0.7])\nkernel_shap = np.array([0.08, 0.28, 0.43, 0.53, 0.58, 0.63])\nsample_shapley = np.array([0.15, 0.36, 0.51, 0.61, 0.66, 0.71])\n\n# Plotting each set of data with different markers\nplt.plot(k, leave_one_out, '^-', label='Leave-one-out')\nplt.plot(k, lime, 'o-', label='LIME')\nplt.plot(k, l_shapley, 's-', label='L-Shapley')\nplt.plot(k, c_shapley, 'x-', label='C-Shapley')\nplt.plot(k, cd, 'D-', label='CD')\nplt.plot(k, hedge, 'v-', label='HEDGE')\nplt.plot(k, kernel_shap, 'h-', label='KernelSHAP')\nplt.plot(k, sample_shapley, 'p-', label='SampleShapley')\n\n# Adding titles and labels\nplt.xlabel('k')\nplt.ylabel('AOPC')\nplt.title('AOPC and log-odds for LSTM on the SST dataset')\nplt.legend()\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.02015/AOPC_lstm_sst_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": null,
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.02015/AOPC_lstm_imdb.png",
        "caption": "The AOPC and log-odds for LSTM on the IMDB dataset.",
        "source": "acl_20/2004.02015/appendix.tex",
        "arxiv_id": "acl_20/2004.02015",
        "llava_output": "The image is a scatter plot showing the AOPC and log-odds for LSTM on the IMDB dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming we have k values\nk = np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50])\n\n# Random data for each method, replace these with your actual data\nnp.random.seed(0)  # For reproducibility of random data\nleave_one_out = np.sort(np.random.rand(len(k)))\nlime = np.sort(np.random.rand(len(k)))\nl_shapley = np.sort(np.random.rand(len(k)))\nc_shapley = np.sort(np.random.rand(len(k)))\ncd = np.sort(np.random.rand(len(k)))\nhedge = np.sort(np.random.rand(len(k)))\nkernel_shap = np.sort(np.random.rand(len(k)))\nsample_shapley = np.sort(np.random.rand(len(k)))\n\n# Plotting each method\nplt.plot(k, leave_one_out, label='Leave-one-out', marker='o')\nplt.plot(k, lime, label='LIME', marker='o')\nplt.plot(k, l_shapley, label='L-Shapley', marker='o')\nplt.plot(k, c_shapley, label='C-Shapley', marker='o')\nplt.plot(k, cd, label='CD', marker='o')\nplt.plot(k, hedge, label='HEDGE', marker='o')\nplt.plot(k, kernel_shap, label='KernelSHAP', marker='o')\nplt.plot(k, sample_shapley, label='SampleShapley', marker='o')\n\n# Adding labels and title\nplt.xlabel('k')\nplt.ylabel('AOPC')\nplt.title('The AOPC and log-odds for LSTM on the IMDB dataset')\n\n# Adding a legend\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('aopc_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.02015/AOPC_lstm_imdb_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.02015/AOPC_cnn_sst.png",
        "caption": "The AOPC and log-odds for CNN on the SST dataset.",
        "source": "acl_20/2004.02015/appendix.tex",
        "arxiv_id": "acl_20/2004.02015",
        "llava_output": "The image is a scatter plot showing the AOPC and log-odds for CNN on the SST dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - Replace these with your actual data values\nk_values = np.linspace(5, 50, num=10)\nleave_one_out_values = np.random.rand(10)*0.6 + 0.2\nlime_values = np.random.rand(10)*0.6 + 0.2\nl_shapley_values = np.random.rand(10)*0.6 + 0.2\nc_shapley_values = np.random.rand(10)*0.6 + 0.2\nhedge_values = np.random.rand(10)*0.6 + 0.2\nkernelshap_values = np.random.rand(10)*0.6 + 0.2\nsampleshapley_values = np.random.rand(10)*0.6 + 0.2\n\n# Plot each line\nplt.plot(k_values, leave_one_out_values, marker='s', label='Leave-one-out')\nplt.plot(k_values, lime_values, marker='o', label='LIME')\nplt.plot(k_values, l_shapley_values, marker='^', label='L-Shapley')\nplt.plot(k_values, c_shapley_values, marker='X', label='C-Shapley')\nplt.plot(k_values, hedge_values, marker='D', label='HEDGE')\nplt.plot(k_values, kernelshap_values, marker='P', label='KernelSHAP')\nplt.plot(k_values, sampleshapley_values, marker='*', label='SampleShapley')\n\n# Add labels and title if needed\nplt.xlabel('k')\nplt.ylabel('AOPC')\n# plt.title('The AOPC and log-odds for CNN on the SST dataset')\n\n# Add a legend\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.02015/AOPC_cnn_sst_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2004.02015/AOPC_cnn_imdb.png",
        "caption": "The AOPC and log-odds for CNN on the IMDB dataset.",
        "source": "acl_20/2004.02015/appendix.tex",
        "arxiv_id": "acl_20/2004.02015",
        "llava_output": "The image is a line chart showing the AOPC and log-odds for CNN on the IMDB dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming these are the x-values\nx_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n\n# Placeholder y-values for each method. Please replace these lists with the real data.\nleave_one_out = [0.25, 0.55, 0.7, 0.8, 0.85, 0.87, 0.88, 0.9, 0.91, 0.92]\nlime = [0.26, 0.56, 0.71, 0.81, 0.86, 0.88, 0.89, 0.91, 0.92, 0.93]\nl_shapley = [0.27, 0.57, 0.72, 0.82, 0.87, 0.89, 0.9, 0.92, 0.93, 0.94]\nc_shapley = [0.28, 0.58, 0.73, 0.83, 0.88, 0.9, 0.91, 0.93, 0.94, 0.95]\nhedge = [0.29, 0.59, 0.74, 0.84, 0.89, 0.91, 0.92, 0.94, 0.95, 0.96]\nkernelshap = [0.3, 0.6, 0.75, 0.85, 0.9, 0.92, 0.93, 0.95, 0.96, 0.97]\nsample_shapley = [0.31, 0.61, 0.76, 0.86, 0.91, 0.93, 0.94, 0.96, 0.97, 0.98]\n\n# Plotting all the lines\nplt.plot(x_values, leave_one_out, marker='s', label='Leave-one-out')\nplt.plot(x_values, lime, marker='o', label='LIME')\nplt.plot(x_values, l_shapley, marker='s', label='L-Shapley')\nplt.plot(x_values, c_shapley, marker='x', label='C-Shapley')\nplt.plot(x_values, hedge, marker='o', label='HEDGE')\nplt.plot(x_values, kernelshap, marker='d', label='KernelSHAP')\nplt.plot(x_values, sample_shapley, marker='s', label='SampleShapley')\n\n# Adding legend, labels, title if needed\nplt.legend()\nplt.xlabel('k')\nplt.ylabel('AOPC')\nplt.title('The AOPC and log-odds for CNN on the IMDB dataset')\n\n# Save the plot as a PNG file\nplt.savefig('imdb_cnn_performance.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.02015/AOPC_cnn_imdb_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.02015/AOPC_bert_sst.png",
        "caption": "The AOPC and log-odds for BERT on the SST dataset.",
        "source": "acl_20/2004.02015/appendix.tex",
        "arxiv_id": "acl_20/2004.02015",
        "llava_output": "The image is a scatter plot that shows the AOPC and log-odds for BERT on the SST dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Sample placeholder data for illustration purposes\nk_values = range(5, 55, 5)\ny_values = {\n    \"Leave-one-out\": [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55],\n    \"LIME\": [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5],\n    \"L-Shapley\": [0.07, 0.12, 0.18, 0.22, 0.27, 0.33, 0.39, 0.42, 0.48, 0.53],\n    \"C-Shapley\": [0.08, 0.13, 0.19, 0.23, 0.29, 0.35, 0.41, 0.44, 0.51, 0.57],\n    \"HEDGE\": [0.06, 0.11, 0.17, 0.21, 0.28, 0.32, 0.38, 0.43, 0.49, 0.54],\n    \"KernelSHAP\": [0.09, 0.14, 0.2, 0.24, 0.3, 0.36, 0.42, 0.46, 0.52, 0.58],\n    \"SampleShapley\": [0.04, 0.09, 0.14, 0.18, 0.23, 0.27, 0.33, 0.37, 0.42, 0.46]\n}\n\n# Plotting each method with a placeholder data\nfor method, values in y_values.items():\n    plt.plot(k_values, values, marker='o', label=method)\n\n# Adding title, legend, and labels\nplt.title('The AOPC and log-odds for BERT on the SST dataset')\nplt.xlabel('k')\nplt.ylabel('AOPC')\nplt.legend()\n\n# Saving the figure with 300 dpi\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.02015/AOPC_bert_sst_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.02015/AOPC_bert_imdb.png",
        "caption": "The AOPC and log-odds for BERT on the IMDB dataset.",
        "source": "acl_20/2004.02015/appendix.tex",
        "arxiv_id": "acl_20/2004.02015",
        "llava_output": "The image is a scatter plot showing the AOPC and log-odds for BERT on the IMDB dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create example data points - arbitrary values for illustration\nk_values = np.arange(5, 55, 5)\nleave_one_out = np.linspace(0.1, 0.8, len(k_values))\nlime = np.linspace(0.1, 0.6, len(k_values))\nl_shapley = np.linspace(0.1, 0.5, len(k_values))\nc_shapley = np.linspace(0.1, 0.7, len(k_values))\nhedge = np.linspace(0.1, 0.4, len(k_values))\nkernel_shap = np.linspace(0.1, 0.5, len(k_values))\nsample_shapley = np.linspace(0.1, 0.3, len(k_values))\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, leave_one_out, marker='o', label='Leave-one-out')\nplt.plot(k_values, lime, marker='o', label='LIME')\nplt.plot(k_values, l_shapley, marker='o', label='L-Shapley')\nplt.plot(k_values, c_shapley, marker='o', label='C-Shapley')\nplt.plot(k_values, hedge, marker='o', label='HEDGE')\nplt.plot(k_values, kernel_shap, marker='o', label='KernelSHAP')\nplt.plot(k_values, sample_shapley, marker='o', label='SampleShapley')\n\n# Add a legend, title, and axis labels\nplt.legend()\nplt.title('The AOPC and log-odds for BERT on the IMDB dataset')\nplt.xlabel('k')\nplt.ylabel('AOPC')\n\n# Save the figure\nplt.tight_layout() # Adjust the layout to fit everything nicely\nplt.savefig('aopc_plot.png', dpi=300)\n\n# Optionally show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.02015/AOPC_bert_imdb_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/implicature_controls.png",
        "caption": "Results on Controls (Implicatures)",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart that presents the results of the experiment on controls (implicatures). The chart shows the accuracy of the model in terms of the number of correct and incorrect answers. The model's accuracy is measured by the percentage of correct answers, which is displayed on the y-axis. The x",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Assuming placeholder data based on the image provided:\ndata = np.array([\n    [0.99, 0.32, 0.55],\n    [0.99, 0.47, 0.87],\n    [1, 0.52, 1],\n    [1, 0.56, 0.91],\n    [1, 0.59, 0.89],\n    [1, 0.59, 0.92],\n    [1, 0.36, 0.46],\n    [0.91, 0.2, 0],\n    [0.98, 0.28, 0.52],\n    [0.76, 0.3, 0.72],\n    [0.95, 0.65, 0.55],\n    [1, 0.8, 0.85],\n])\n\n# Placeholder labels based on the image provided:\nrow_labels = [\n    'Connectives', 'Connectives',\n    'Determiners', 'Determiners',\n    'Gradable adjectives', 'Gradable adjectives',\n    'Gradable verbs', 'Gradable verbs',\n    'Modals', 'Modals',\n    'Numerals', 'Numerals'\n]\ncolumn_labels = ['BERT', 'BOW model', 'InferSent']\n\n# Create a heatmap using seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(data, annot=True, fmt=\".2f\", cmap='Blues',\n            xticklabels=column_labels, yticklabels=row_labels)\n\n# Add text labels manually for Control Type\nfor i in range(data.shape[0]):\n    plt.text(3.5, i, 'Opposite' if i % 2 else 'Negation', va='center')\n\n# To fix the display of the labels and to keep the correct order display\nplt.yticks(rotation=0)\n\nplt.title('Implicatures Controls (Accuracy)')\nplt.xlabel('')\nplt.ylabel('Implicature Trigger')\n\n# Save the figure\nplt.savefig('heatmap.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/implicature_controls_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/presupposition_controls.png",
        "caption": "Results on Controls (Presuppositions).",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart showing the results of presuppositions controls.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from the heatmap in the image, represented as a 2D list\ndata = [\n    [0.61, 0.22, 0.013],\n    [0.8, 0.24, 0.0044],\n    [0.82, 0.17, 0],\n    [1, 0.28, 0.32]\n]\n\n# Labels for rows and columns from the heatmap in the image\nrows = [\"conditional\", \"interrogative\", \"modal\", \"negated\"]\ncolumns = [\"BERT\", \"BOW\", \"InferSent\"]\n\n# Create a numpy array from the data\ndata_array = np.array(data)\n\n# Create the heatmap\nfig, ax = plt.subplots()\ncax = ax.matshow(data_array, cmap='Blues')\n\n# Add color bar\nplt.colorbar(cax)\n\n# Set tick labels\nax.set_xticklabels([''] + columns)\nax.set_yticklabels([''] + rows)\n\n# Display the data values on the heatmap\nfor i in range(len(rows)):\n    for j in range(len(columns)):\n        c = data_array[i][j]\n        ax.text(j, i, str(c), va='center', ha='center')\n\n# Set titles and labels\nax.set_title('Presupposition Controls (Accuracy)', pad=20)\nax.set_ylabel('Operator')\n\n# Save the figure\nplt.savefig('heatmap.png', dpi=300)\n\n# Show the plot (not necessary if only saving the figure)\n# plt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/presupposition_controls_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/presupposition_trigger_results.png",
        "caption": "Results for the unembedded trigger paired with positive presupposition.",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart showing the results of a presupposition trigger experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data approximated from the image\ndata = np.array([\n    [0.27, 0.74, 0.67],\n    [0, 0.86, 0.55],\n    [0.13, 0.65, 0.55],\n    [1, 0.9, 0.96],\n    [0.04, 0.95, 0.99],\n    [1, 0.78, 0.91],\n    [1, 0.78, 0.88],\n    [0, 0.66, 0.01],\n    [0.86, 0.56, 0.78]\n])\n\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='Blues')\n\n# Add colorbar\nplt.colorbar(cax)\n\n# Set axis labels\nax.set_xticklabels([''] + ['BERT', 'BOW', 'InferSent'], rotation=45)\nax.set_yticklabels([''] + [\n    'All N',\n    'Both',\n    'Change of state',\n    'Cleft existence',\n    'Cleft uniqueness',\n    'Only',\n    'Possess. existence',\n    'Possess. uniqueness',\n    'Question'\n])\n\n# Display the data values on the heat map\nfor (i, j), val in np.ndenumerate(data):\n    ax.text(j, i, f'{val:.2f}', ha='center', va='center', color='white')\n\n# Set titles\nplt.title('Presupposition Trigger Results (Accuracy)', pad=20)\nplt.xlabel('Model')\nplt.ylabel('Trigger')\n\nplt.savefig('heatmap.png', dpi=300) # Save the figure with 300 dpi\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/presupposition_trigger_results_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/presupposition_projection_results.png",
        "caption": "Results for presupposition target conditions involving projection.",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart presenting the results of an experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\n# Data to be plotted (assuming a 3x3 grid to match your image)\ndata = np.array([\n    [0.94, 0.35, 0.88],\n    [0.17, 0.17, 0.5],\n    [0.94, 0.94, 0.5],\n    [0.95, 0.36, 0.67],\n    [0.17, 0.16, 0],\n    [0.81, 0.92, 0.33],\n    [0.96, 0.35, 0],\n    [0.15, 0.075, 0],\n    [0.8, 0.99, 0],\n    [0.93, 0.52, 0.19],\n    [0.2, 0.078, 0.06],\n    [0.94, 1, 0.43]\n])\n\n# Configuring the figure and axes\nfig, ax = plt.subplots(figsize=(8, 9))\n\n# Create custom colormap\ncmap = plt.cm.Blues\n\n# Define the bounds of your colorbar\nbounds = np.array([0, 0.25, 0.5, 0.75, 1])\nnorm = Normalize(vmin=bounds.min(), vmax=bounds.max())\n\n# Creating the heatmap\nheatmap = ax.imshow(data, interpolation='nearest', cmap=cmap, norm=norm)\n\n# Adding the color bar, which needs the norm object to ensure it maps correctly\ncbar = plt.colorbar(heatmap, ax=ax)\n\n# Set up the labels for rows and columns\nrow_labels = [\n    'conditional - negated', 'conditional - neutral', 'conditional - positive',\n    'interrogative - negated', 'interrogative - neutral', 'interrogative - positive',\n    'modal - negated', 'modal - neutral', 'modal - positive',\n    'negated - negated', 'negated - neutral', 'negated - positive'\n]\ncol_labels = ['BERT', 'BOW', 'InferSent']\n\nax.set_xticks(np.arange(len(col_labels)))\nax.set_yticks(np.arange(len(row_labels)))\n\nax.set_xticklabels(col_labels)\nax.set_yticklabels(row_labels)\n\n# Rotate the tick labels and set their alignment\nplt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\")\nplt.setp(ax.get_yticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations\nfor i in range(len(row_labels)):\n    for j in range(len(col_labels)):\n        text = ax.text(j, i, data[i, j], ha=\"center\", va=\"center\", color=\"w\")\n\n# Setting the title and labels\nax.set_title(\"Projection Results (Accuracy)\")\n\n# Tweaking the subplot layout for a tight fit and to accommodate the title\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n# Save the figure with a 300 DPI\nplt.savefig(\"heatmap.png\", dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/presupposition_projection_results_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/determiners_implicature_results.png",
        "caption": "Results for the scalar implicatures triggered by determiners, by target condition.",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a Bar Chart, showing the results for the scalar implicatures triggered by determiners, by target condition.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data as observed in the image provided\ndata = np.array([\n    [0.00, 0.85, 1.00],\n    [0.76, 0.02, 0.00],\n    [1.00, 0.12, 0.84],\n    [0.00, 0.83, 0.16],\n    [0.15, 0.00, 0.00],\n    [0.56, 0.99, 0.40],\n    [0.65, 0.11, 0.00],\n    [0.34, 0.89, 0.86],\n    [0.01, 0.04, 0.00],\n    [0.99, 0.89, 0.83],\n    [0.33, 0.02, 0.00],\n    [0.67, 0.03, 0.33]\n])\n\n# Labels for the rows and columns as seen in the image\nrow_labels = [\n    \"all/some \u2013 logical\", \"all/some \u2013 pragmatic\",\n    \"none/not all \u2013 logical\", \"none/not all \u2013 pragmatic\",\n    \"not all/none \u2013 logical\", \"not all/none \u2013 pragmatic\",\n    \"not all/some \u2013 logical\", \"not all/some \u2013 pragmatic\",\n    \"some/all \u2013 logical\", \"some/all \u2013 pragmatic\",\n    \"some/not all \u2013 logical\", \"some/not all \u2013 pragmatic\"\n]\ncolumn_labels = [\"BERT\", \"BOW\", \"InferSent\"]\n\nfig, ax = plt.subplots()\n# Use a blue color map (the 'Blues' can be adjusted to a more matching color scale)\ncax = ax.matshow(data, cmap='Blues')\n\n# Add color bar\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + column_labels)\nax.set_yticklabels([''] + row_labels)\n\n# Rotate the x-axis labels so they show up more clearly\nplt.xticks(rotation=45)\n\n# Draw the values in the cells\nfor i in range(data.shape[0]):\n    for j in range(data.shape[1]):\n        c = data[i, j]\n        ax.text(j, i, str(c), va='center', ha='center')\n\nplt.title('Determiners Implicature Results (Accuracy)')\nplt.tight_layout()\n\n# Save the figure with 300 dpi\nplt.savefig('heatmap.png', dpi=300)\n\n# Show the plot (this can be commented out if not needed)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/determiners_implicature_results_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/modals_implicature_results.png",
        "caption": "Results for the scalar implicatures triggered by modals, by target condition.",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart that presents the results of an experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Your data from the image\nconditions = [\n    'can/have to \u2212 logical', 'can/have to \u2212 pragmatic',\n    'can/not have to \u2212 logical', 'can/not have to \u2212 pragmatic',\n    'cannot/not have to \u2212 logical', 'cannot/not have to \u2212 pragmatic',\n    'have to/can \u2212 logical', 'have to/can \u2212 pragmatic',\n    'not have to/can \u2212 logical', 'not have to/can \u2212 pragmatic',\n    'not have to/cannot \u2212 logical', 'not have to/cannot \u2212 pragmatic'\n]\n\n# Assuming the values are in the same order as the conditions\nbert_results = [0.37, 0.41, 0.12, 0.08, 0.48, 0.31, 0.3, 0.22, 0.03, 0.4, 0.01, 0.25]\nbow_results = [0.11, 0.17, 0.01, 0.05, 0.07, 0.91, 0.83, 0.14, 0.02, 0.88, 0.01, 0.19]\ninfersent_results = [0, 0, 0, 0.07, 0.11, 0.89, 1, 0, 0, 0.17, 0, 0.79]\n\n# Number of groups\nnum_groups = len(conditions)\n\n# Create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# Set the figure size\nfig.set_size_inches(10, 8)\n\n# Index for the groups\nindex = np.arange(num_groups)\n\n# Bar width\nbar_width = 0.25\n\n# Opacity\nopacity = 0.8\n\n# Bar plots\nrects1 = ax.barh(index, bert_results, bar_width, alpha=opacity, color='b', label='BERT')\nrects2 = ax.barh(index + bar_width, bow_results, bar_width, alpha=opacity, color='g', label='BOW')\nrects3 = ax.barh(index + bar_width * 2, infersent_results, bar_width, alpha=opacity, color='r', label='InferSent')\n\n# Set the y axis labels\nax.set_yticks(index + bar_width)\nax.set_yticklabels(conditions)\n\n# Set the labels and title\nax.set_xlabel('Results (Accuracy)')\nax.set_title('Modals Implicature Results (Accuracy)')\n\n# Create a legend\nax.legend()\n\n# Layout adjustments\nplt.tight_layout()\n\n# Save the figure with 300 dpi\nplt.savefig('modal_implicature_results.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/modals_implicature_results_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/numerals_implicature_results.png",
        "caption": "Results for the scalar triggered by numerals, by target condition.",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart that presents the results of an experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Set the data\ndata = np.array([\n    [0.02, 0.14, 0.00],\n    [0.97, 0.48, 0.26],\n    [0.00, 0.18, 0.00],\n    [1.00, 0.065, 0.30],\n    [0.00, 0.56, 0.53],\n    [1.00, 0.14, 0.47],\n    [0.00, 0.52, 0.84],\n    [1.00, 0.39, 0.15],\n    [0.11, 0.095, 0.00],\n    [0.48, 0.49, 0.28],\n    [0.46, 0.32, 0.01],\n    [0.00, 0.57, 0.22]\n])\n\n# A list of row labels.\nrows = [\n    \"10/100 -\",\n    \"10/100 -\",\n    \"10/not 100 -\",\n    \"10/not 100 -\",\n    \"100/10 -\",\n    \"100/10 -\",\n    \"not 10/not 100 -\",\n    \"not 10/not 100 -\",\n    \"not 100/10 -\",\n    \"not 100/10 -\",\n    \"not 100/not 10 -\",\n    \"not 100/not 10 -\",\n]\n\n# A list of column labels.\ncols = [\"BERT\", \"BOW\", \"InferSent\"]\n\n# A list of condition labels on the right side.\nconditions = [\n    \"logical\",\n    \"pragmatic\",\n    \"logical\",\n    \"pragmatic\",\n    \"logical\",\n    \"pragmatic\",\n    \"logical\",\n    \"pragmatic\",\n    \"logical\",\n    \"pragmatic\",\n    \"logical\",\n    \"pragmatic\",\n]\n\n# Create the figure and axis\nfig, ax = plt.subplots()\n\n# Plot the heatmap with appropriate colormap\ncax = ax.imshow(data, cmap=\"Blues\", aspect=\"auto\")\n\n# Set the ticks\nax.set_xticks(np.arange(len(cols)))\nax.set_yticks(np.arange(len(rows)))\n\n# Set the tick labels\nax.set_xticklabels(cols)\nax.set_yticklabels(rows)\n\n# Rotate the tick labels and set their alignment\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Add the text labels inside the squares\nfor i in range(len(rows)):\n    for j in range(len(cols)):\n        text = ax.text(j, i, data[i, j],\n                       ha=\"center\", va=\"center\", color=\"black\")\n\n# Add the condition labels to the right side\nfor i, condition in enumerate(conditions):\n    ax.text(len(cols)+0.1, i, condition, va='center', ha='left')\n\n# Set the title of the heatmap\nax.set_title('Numerals Implicature Results (Accuracy)')\n\n# Save the figure with 300 DPI\nplt.savefig('heatmap.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/numerals_implicature_results_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.03066/verbs_implicature_results.png",
        "caption": "Results for the scalar implicatures triggered by verbs, by target condition.",
        "source": "acl_20/2004.03066/naaclhlt2018.tex",
        "arxiv_id": "acl_20/2004.03066",
        "llava_output": "The image is a bar chart that presents the results for the scalar implicatures triggered by verbs, by target condition.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data from your image (you should replace this with your actual data)\ndata = np.array([\n    [0.99, 0.028, 1],\n    [0, 0.94, 0],\n    [0.095, 0.019, 0],\n    [0, 0.95, 0],\n    [0.019, 0.065, 0],\n    [0.33, 0.77, 0.32],\n    [0.33, 0.0088, 0],\n    [0, 0, 0.069],\n    [0.32, 0.13, 0],\n    [0.0095, 0.31, 0.057],\n    [1, 0.65, 1],\n    [0, 0.27, 0]\n])\n\n# Labels for rows and columns\nrows = [\n    \"not run/not sprint -\",\n    \"not run/not sprint -\",\n    \"not sprint/not run -\",\n    \"not sprint/not run -\",\n    \"not sprint/run -\",\n    \"not sprint/run -\",\n    \"run/not sprint -\",\n    \"run/not sprint -\",\n    \"run/sprint -\",\n    \"sprint/run -\",\n    \"sprint/run -\",\n    \"sprint/run -\"\n]\ncolumns = [\"BERT\", \"BOW\", \"InferSent\"]\n\n# The right column labels\nright_column = [\n    \"logical\", \"pragmatic\", \"logical\", \"pragmatic\", \"logical\", \"pragmatic\",\n    \"logical\", \"pragmatic\", \"logical\", \"pragmatic\", \"logical\", \"pragmatic\"\n]\n\nfig, ax = plt.subplots(figsize=(8, 8))\nheatmap = ax.imshow(data, cmap='Blues')\n\n# We want to show all ticks...\nax.set_xticks(np.arange(len(columns)))\nax.set_yticks(np.arange(len(rows)))\n# ... and label them with the respective list entries\nax.set_xticklabels(columns)\nax.set_yticklabels(rows)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\n# Right column labels\nfor i, label in enumerate(right_column):\n    ax.text(len(columns), i, label, ha='center', va='center', color='black')\n\n# Turn spines off and create white grid.\nfor edge, spine in ax.spines.items():\n    spine.set_visible(False)\n\nax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\nax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\nax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\nax.tick_params(which=\"minor\", size=0)\n\n# Loop over data dimensions and create text annotations.\nfor i in range(len(rows)):\n    for j in range(len(columns)):\n        text = ax.text(j, i, data[i, j],\n                       ha=\"center\", va=\"center\", color=\"w\")\n\n# Add title\nplt.title('Verbs Implicature Results (Accuracy)', pad=20, fontsize=16)\n\n# Save the figure\nplt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.03066/verbs_implicature_results_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.04100/turn2topic_bar.png",
        "caption": "Statistics of the number of dialogues where at least $k (k=2,3,4)$ topics have been discussed in the first $n$ turns. The proportions of dialogues that contain 3 or 4 topics become larger when the dialog turn becomes longer.",
        "source": "acl_20/2004.04100/acl2020.tex",
        "arxiv_id": "acl_20/2004.04100",
        "llava_output": "The image is a bar chart that presents the statistics of the number of dialogues where at least $k (k=2,3,4)$ topics have been discussed in the first $n$ turns. The proportions of dialogues that contain 3 or 4 topics become larger when the dialog turn becomes longer",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data: replace with your actual data.\ndata = {\n    'film': {5: (150, 50, 10), 10: (800, 200, 100), 15: (1200, 300, 150), 20: (1300, 350, 150), 25: (1400, 375, 150), 30: (1450, 400, 150)},\n    'music': {5: (130, 40, 5), 10: (700, 160, 90), 15: (1150, 290, 140), 20: (1250, 340, 140), 25: (1350, 365, 140), 30: (1400, 390, 140)},\n    'travel': {5: (120, 30, 5), 10: (650, 150, 80), 15: (1100, 280, 130), 20: (1200, 330, 130), 25: (1300, 355, 130), 30: (1350, 380, 130)}\n}\n\n# Create a figure and a set of subplots.\nfig, axes = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n\n# Plot the data.\nfor ax, (category, values) in zip(axes, data.items()):\n    turns = list(values.keys())\n    counts = list(values.values())\n    count_series = list(zip(*counts))\n    \n    ax.bar(turns, count_series[0], label=f'{category}-2', color='skyblue')\n    ax.bar(turns, count_series[1], bottom=count_series[0], label=f'{category}-3', color='orange')\n    ax.bar(turns, count_series[2], bottom=[i+j for i,j in zip(count_series[0], count_series[1])], label=f'{category}-4', color='green')\n    \n    ax.set_title(category)\n    ax.set_xlabel('num of turns')\n    ax.set_ylabel('num of dialogues')\n    ax.legend()\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI.\nplt.savefig('dialogue_statistics.png', dpi=300)\n\n# Show the figure\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.04100/turn2topic_bar_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.04305/cl-architecture-small.png",
        "caption": "The architecture of Conversation Learner (Top) and the development of DMs using Conversation Learner (Bottom).",
        "source": "acl_20/2004.04305/acl2020.tex",
        "arxiv_id": "acl_20/2004.04305",
        "llava_output": "0</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Initialize figure and axis\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create rectangles\nrect_a = patches.Rectangle((0.1, 0.7), 0.3, 0.2, linewidth=1, edgecolor='blue', facecolor='none')\nrect_b = patches.Rectangle((0.5, 0.7), 0.4, 0.2, linewidth=1, edgecolor='blue', facecolor='none')\nrect_c = patches.Rectangle((0.1, 0.4), 0.8, 0.2, linewidth=1, edgecolor='blue', linestyle='dashed', facecolor='none')\n\n# Add rectangles to the plot\nax.add_patch(rect_a)\nax.add_patch(rect_b)\nax.add_patch(rect_c)\n\n# Add text inside rectangles\nax.text(0.25, 0.8, 'Placeholder A', ha='center', va='center')\nax.text(0.7, 0.8, 'Placeholder B', ha='center', va='center')\nax.text(0.5, 0.5, 'Placeholder C', ha='center', va='center')\n\n# Draw arrows\nax.annotate('', xy=(0.4, 0.7), xytext=(0.4, 0.9), arrowprops=dict(arrowstyle='->'))\nax.annotate('', xy=(0.6, 0.9), xytext=(0.6, 0.7), arrowprops=dict(arrowstyle='->'))\n\n# Set limits, hide axes and save the figure\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\nplt.savefig('data_visualization.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.04305/cl-architecture-small_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.04494/length.png",
        "caption": "Ablation of context information. w/o context means all contexts are removed, so models just predict correct choice based on four candidates. context-n denotes the earlist n utterances are removed.",
        "source": "acl_20/2004.04494/discussion.tex",
        "arxiv_id": "acl_20/2004.04494",
        "llava_output": "The image is a line chart showing the performance of a machine learning model in predicting the correct choice between four candidates. The chart is labeled with the context information that was used in the experiment, including \"w/o context\" which means no context information is used, and \"context-n\" which means the",
        "is_data_visual": "Based on the justification and figure caption, Yes.The image is a line chart showing the performance of a machine learning model in predicting the correct choice between four candidates. The chart is labeled with the context information that was used in the experiment, including \"w/o context\" and \"context-n\".",
        "code": "import matplotlib.pyplot as plt\n\n# Assume some performance scores for RoBERTa and RoBERTa-MC\nroberta_scores = [0.70, 0.63, 0.57, 0.48]  # Replace with actual values from your paper\nroberta_mc_scores = [0.68, 0.61, 0.53, 0.44]  # Replace with actual values from your paper\n\n# Context labels for the x-axis\ncontexts = ['context', 'context-1', 'context-2', 'w/o context']\n\n# Plotting the scores\nplt.plot(contexts, roberta_scores, marker='o', color='blue', label='RoBERTa')\nplt.plot(contexts, roberta_mc_scores, marker='o', color='red', label='RoBERTa-MC')\n\n# Adding legend\nplt.legend()\n\n# Adding title and labels (assuming the title was not provided in the original plot)\nplt.title('Ablation of Context Information')\nplt.xlabel('Context Information')\nplt.ylabel('Model Performance')\n\n# Save the figure with 300 DPI\nplt.savefig('ablation_of_context_information.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.04494/length_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 3,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.07180/tsne_citerator.png",
        "caption": "\\sys",
        "source": "acl_20/2004.07180/5-analysis.tex",
        "arxiv_id": "acl_20/2004.07180",
        "llava_output": "The image is a scatter plot, which is used to present experiment results. The scatter plot is colorful and features a variety of colors, indicating different data points. The plot is labeled with various scientific fields, such as business, chemistry, computer science, engineering, environmental science, and medicine. This suggests",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data - Replace with your actual data points and categories\ncategories = ['Business', 'Chemistry', 'Sociology', 'Economics', 'Computer science', \n              'Physics', 'Environmental science', 'Mathematics', 'Engineering', 'Medicine']\ncolors = plt.cm.tab10(np.linspace(0, 1, len(categories)))  # Adjust the colormap as needed\n\n# Generate random clusters of points for the sake of example\ndata = {category: np.random.randn(100, 2) for category in categories}\n\n# Create a scatter plot\nfig, ax = plt.subplots()\nfor category, color in zip(categories, colors):\n    x, y = data[category].T\n    ax.scatter(x, y, label=category, color=color)\n\n# Add legend\nax.legend()\n\n# Save the figure\nplt.savefig('scatter_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.07180/tsne_citerator_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.07667/nullspace-projection.png",
        "caption": "Nullspace projection for a 2-dimensional binary classifier. The decision boundary of $W$ is $W$ 's null-space.",
        "source": "acl_20/2004.07667/main.tex",
        "arxiv_id": "acl_20/2004.07667",
        "llava_output": "0</s>",
        "is_data_visual": "Yes, based on the figure caption, the image appears to be presenting a data visualization plot for a 2-dimensional binary classifier, specifically showing the decision boundary of a nullspace projection.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Setup the figure and axis\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Set the axis limits\nax.set_xlim(-3, 3)\nax.set_ylim(-2, 2)\n\n# Draw some points representing two different classes\nclass1 = np.array([[-2, -1], [-2, 1], [-1.5, -0.5], [-2.5, 0.5]])\nclass2 = np.array([[1, -1], [2, 1], [1.5, 0.5], [2.5, -0.5]])\n\n# Plot points using green triangles and brown circles\nax.scatter(class1[:, 0], class1[:, 1], marker='^', color='green', s=100)\nax.scatter(class2[:, 0], class2[:, 1], marker='o', color='brown', s=100)\n\n# Draw decision boundary (W's null-space)\nax.axhline(0, color='black', linewidth=2)\n\n# Draw the weight vector W\nW = np.array([0, 1])\nax.quiver(0, 0, W[0], W[1], angles='xy', scale_units='xy', scale=1, color='blue')\n\n# Draw the null-space of W (V)\nV = np.array([1, 0])\nax.quiver(0, 0, V[0], V[1], angles='xy', scale_units='xy', scale=1, color='green')\n\n# Choose a point X from class 1 and draw it with the label\nX = class1[1]\nax.scatter(X[0], X[1], marker='^', color='white', edgecolor='black', s=100)\nax.text(X[0] + 0.1, X[1], 'X =', ha='right', va='center')\n\n# Calculating projections\nproj_W = np.dot(X, W) * W / np.linalg.norm(W)**2\nproj_V = np.dot(X, V) * V / np.linalg.norm(V)**2\n\n# Draw the projections\nax.quiver(0, 0, proj_V[0], proj_V[1], angles='xy', scale_units='xy', scale=1, color='brown', linestyle='dashed')\nax.text(proj_V[0], proj_V[1]-0.2, 'Proj$_V$(X)', ha='center', va='top')\nax.quiver(X[0], X[1], proj_W[0]-X[0], proj_W[1]-X[1], angles='xy', scale_units='xy', scale=1, color='red', linestyle='dashed')\nax.text(X[0]+proj_W[0]-X[0], X[1]+proj_W[1]-X[1] + 0.1, 'Proj$_W$(X)', ha='left', va='bottom')\n\n# Hide top and right spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Label Nullspace of W\nax.text(-2, 1, 'V = Nullspace(W)', color='green')\n\n# Save the plot with 300 dpi\nplt.savefig('nullspace_projection.png', dpi=300)\n\n# Show plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.07667/nullspace-projection_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.07667/comparison-bert_small.png",
        "caption": "t-SNE projection of BERT representations for the profession ``professor\" (left) and for a random sample of all professions (right), before and after the projection.",
        "source": "acl_20/2004.07667/main.tex",
        "arxiv_id": "acl_20/2004.07667",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data: Replace these with your actual data arrays\nmale_professor = {'x': np.random.randn(100), 'y': np.random.randn(100)}\nfemale_professor = {'x': np.random.randn(100), 'y': np.random.randn(100)}\nmale_all = {'x': np.random.randn(300), 'y': np.random.randn(300)}\nfemale_all = {'x': np.random.randn(300), 'y': np.random.randn(300)}\n\n# Setting up the subplots.\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\n# Plotting \"Professor (Original)\"\naxs[0, 0].scatter(male_professor['x'], male_professor['y'], c='blue', label='Male', alpha=0.5)\naxs[0, 0].scatter(female_professor['x'], female_professor['y'], c='red', marker='s', label='Female', alpha=0.5)\naxs[0, 0].set_title('Professor (Original)')\naxs[0, 0].legend()\n\n# Plotting \"All (Original)\"\naxs[0, 1].scatter(male_all['x'], male_all['y'], c='blue', label='Male', alpha=0.5)\naxs[0, 1].scatter(female_all['x'], female_all['y'], c='red', marker='s', label='Female', alpha=0.5)\naxs[0, 1].set_title('All (Original)')\naxs[0, 1].legend()\n\n# Plotting \"Professor (Projected)\"\naxs[1, 0].scatter(male_professor['x'], male_professor['y'], c='blue', label='Male', alpha=0.5)\naxs[1, 0].scatter(female_professor['x'], female_professor['y'], c='red', marker='s', label='Female', alpha=0.5)\naxs[1, 0].set_title('Professor (Projected)')\naxs[1, 0].legend()\n\n# Plotting \"All (Projected)\"\naxs[1, 1].scatter(male_all['x'], male_all['y'], c='blue', label='Male', alpha=0.5)\naxs[1, 1].scatter(female_all['x'], female_all['y'], c='red', marker='s', label='Female', alpha=0.5)\naxs[1, 1].set_title('All (Projected)')\naxs[1, 1].legend()\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('tsne_projection.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.07667/comparison-bert_small_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.08097/plot-a-times-cpu-unilm.png",
        "caption": "Running times according to the number of words for uniLM and T-TA.",
        "source": "acl_20/2004.08097/plot-a-times-cpu-unilm.tex",
        "arxiv_id": "acl_20/2004.08097",
        "llava_output": "The image is a Line Chart that shows the running times according to the number of words for uniLM and T-TA.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Example data (hypothetical)\nwords = list(range(1, 61)) # Assuming the x-axis represents word counts from 1 to 60\nuniLM_rrk = [(w ** 0.8 + 5) for w in words] # Hypothetical data for uniLMrrk\nuniLM_sts = [(w ** 0.5 + 3) for w in words] # Hypothetical data for uniLMsts\nT_TA_rrk = [(w ** 0.8 + 8) for w in words]  # Hypothetical data for T-TArrk\nT_TA_sts = [(w ** 0.5 + 6) for w in words]  # Hypothetical data for T-TAsts\n\n# Plotting\nplt.plot(words, uniLM_rrk, 'r--', label='uniLMrrk')\nplt.plot(words, uniLM_sts, 'b-', label='uniLMsts')\nplt.plot(words, T_TA_rrk, 'y-.', label='T-TArrk')\nplt.plot(words, T_TA_sts, 'orange', label='T-TAsts')\n\nplt.xlabel('# of words')\nplt.ylabel('Time (ms)')\nplt.title('Running times according to the number of words')\nplt.legend()\n\n# Save the plot as a PNG file\nplt.savefig('running_times.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.08097/plot-a-times-cpu-unilm_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.08097/plot-a-times-gpu.png",
        "caption": "Running times according to the number of words for biLM and T-TA on GPU-augmented environment.",
        "source": "acl_20/2004.08097/plot-a-times-gpu.tex",
        "arxiv_id": "acl_20/2004.08097",
        "llava_output": "The image is a line chart showing the running times of biLM and T-TA on a GPU-augmented environment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulated data\n# You'll need to replace this with the actual data\nwords = np.arange(1, 61)\ntime_biLMrrk = np.random.normal(loc=8.0 + words * 0.08, scale=0.5)  # Adjust to match trend\ntime_biLMsts = np.random.normal(loc=15.0 + words * 0.1, scale=1.0)  # Adjust to match trend\ntime_TTArrk = np.random.normal(loc=3.0 + words * 0.05, scale=0.3)   # Adjust to match trend\ntime_TTAsts = np.random.normal(loc=4.0 + words * 0.07, scale=0.4)   # Adjust to match trend\n\n# Creating the plot\nplt.figure(figsize=(10, 6))\n\n# Plotting each line with specific styles\nplt.plot(words, time_biLMrrk, 'C0--', label='biLMrrk', linewidth=2, markersize=5)\nplt.plot(words, time_biLMsts, 'C0-.', label='biLMsts', linewidth=2, markersize=5)\nplt.plot(words, time_TTArrk, 'C1--', label='T-TArrk', linewidth=2, markersize=5)\nplt.plot(words, time_TTAsts, 'C1:', label='T-TAsts', linewidth=2, markersize=5)\n\n# Adding labels and title\nplt.xlabel('# of words')\nplt.ylabel('Time (ms)')\nplt.title('Running Times According to the Number of Words')\n\n# Adding a legend\nplt.legend()\n\n# Setting the log scale for y-axis\nplt.yscale('log')\n\n# Setting the grid style\nplt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n\n# Saving the plot with high resolution\nplt.savefig('running_times_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.08097/plot-a-times-gpu_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.08500/break_h.png",
        "caption": "Intuition of the supports of $A,B$ and $C$.",
        "source": "acl_20/2004.08500/anbn.tex",
        "arxiv_id": "acl_20/2004.08500",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Create the main box (C) with dashed lines\nmain_box = patches.Rectangle((0, 0), 2, 2, linewidth=2, edgecolor='black', facecolor='none', linestyle='--')\nax.add_patch(main_box)\nax.text(1.6, 0.1, 'C', fontsize=12, verticalalignment='bottom', horizontalalignment='right')\n\n# Create box A\nbox_A = patches.Rectangle((0, 1), 1, 1, linewidth=2, edgecolor='black', facecolor='none')\nax.add_patch(box_A)\nax.text(0.5, 1.5, 'A', fontsize=12, verticalalignment='center', horizontalalignment='center')\n\n# Create box B\nbox_B = patches.Rectangle((0, 0), 1, 1, linewidth=2, edgecolor='black', facecolor='none')\nax.add_patch(box_B)\nax.text(0.5, 0.5, 'B', fontsize=12, verticalalignment='center', horizontalalignment='center')\n\n# Add dashed dividing line at the midpoint (dotted line)\nplt.axvline(x=1, color='gray', linestyle=':', linewidth=1)\nplt.axhline(y=1, color='gray', linestyle=':', linewidth=1)\n\n# Setting the limits\nax.set_xlim(0, 2)\nax.set_ylim(0, 2)\n\n# Removing the axis markings\nax.axis('off')\n\n# Labeling\nax.text(0, 1, 'a*', fontsize=12, verticalalignment='center', horizontalalignment='right')\nax.text(0, 0, 'a* b+', fontsize=12, verticalalignment='center', horizontalalignment='right')\nax.text(1,2, 'b*', fontsize=12, verticalalignment='bottom', horizontalalignment='center')\nax.text(2,2, 'a+ b*', fontsize=12, verticalalignment='bottom', horizontalalignment='center')\n\n# Adding text outside of the borders\nax.text(-0.1, -0.1, 'contains\\nba', fontsize=12, verticalalignment='top', horizontalalignment='right')\nax.text(2.1, 2.1, 'contains\\nba', fontsize=12, verticalalignment='bottom', horizontalalignment='left')\n\n# Saving the figure\nplt.savefig('visualization.png', dpi=300, bbox_inches='tight')\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.08500/break_h_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.09036/attention_prompts.png",
        "caption": "Attention on the prompt.",
        "source": "acl_20/2004.09036/acl2020.tex",
        "arxiv_id": "acl_20/2004.09036",
        "llava_output": "The image is a line chart displaying data.</s>",
        "is_data_visual": "Yes.",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create some sample data, replace this with actual data from the paper\ndata = np.random.rand(21, 6) * 0.3  # Assume 21 attention values for 6 words\n\n# Create the figure and the axis\nfig, ax = plt.subplots()\n\n# Create the heatmap\ncax = ax.matshow(data, cmap='Oranges')\n\n# Add color bar, which needs to be accounted for in the layout\nfig.colorbar(cax)\n\n# Set tick labels (assuming the horizontal axis is for words in the prompt)\nax.set_xticklabels([''] + ['what', 'do', 'you', 'do', 'in', 'your', 'spare', 'time'], rotation=90)\nax.set_yticklabels([''] + list(map(str, range(21))))\n\n# Turn the tick labels to vertical (as the image shows them horizontally)\nplt.xticks(rotation=90)\n\n# Save the plot as a PNG file with 300 DPI resolution\nplt.savefig('heatmap.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.09036/attention_prompts_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2004.09036/good_original_resp_distribution.png",
        "caption": "True resp distribution on clear-semantic topic prompt.",
        "source": "acl_20/2004.09036/acl2020.tex",
        "arxiv_id": "acl_20/2004.09036",
        "llava_output": "The image is a scatter plot, which shows the distribution of two different types of data points. The scatter plot has two different colors of dots, one representing the first type of data and the other representing the second type of data. The plot is displayed on a white background, and the data points are spread across",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some data (randomly generated for demonstration purposes)\nnp.random.seed(0)  # for reproducible results\nx1 = np.random.normal(-20, 10, size=100)\ny1 = np.random.normal(10, 10, size=100)\nx2 = np.random.normal(30, 10, size=100)\ny2 = np.random.normal(10, 10, size=100)\n\n# Create the plot\nplt.figure(figsize=(10, 7))\nplt.scatter(x1, y1, color='yellow')\nplt.scatter(x2, y2, color='navy')\n\n# Set the same limits as in the provided image\nplt.xlim(-70, 70)\nplt.ylim(-35, 45)\n\n# Save the plot with 300dpi\nplt.savefig('scatter_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.09036/good_original_resp_distribution_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.09036/datasize_variation.jpeg",
        "caption": "Trends of AOR (Average Off-topic Recall) on seen and unseen prompts with datasize variation.",
        "source": "acl_20/2004.09036/acl2020.tex",
        "arxiv_id": "acl_20/2004.09036",
        "llava_output": "1. The image is a line chart showing the trends of AOR (Average Off-topic Recall) on seen and unseen prompts with datasize variation.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Data for plotting\ndataset_sizes = ['10%', '50%', '80%', '100%']\nseen_values = [60, 70, 75, 80]\nunseen_values = [20, 20, 30, 40]\n\n# Plotting the lines\nplt.plot(dataset_sizes, seen_values, marker='D', color='r', label='seen')\nplt.plot(dataset_sizes, unseen_values, marker='o', color='b', label='unseen')\n\n# Adding the legend\nplt.legend()\n\n# Title and labels\nplt.title('Average of Off-topic Recall')\nplt.xlabel('Dataset size')\nplt.ylabel('Average Off-topic Recall')\n\n# Save the plot as a PNG file\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.09036/datasize_variation_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10454/ADR_NOKD.png",
        "caption": "Attention density ratio $R(p)$ under different $p$ in different tasks for performance gap analysis.",
        "source": "acl_20/2004.10454/acl2020.tex",
        "arxiv_id": "acl_20/2004.10454",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes, based on the image and the provided justification, it appears that this image presents the results of an attention density ratio experiment and would fit into the dataset for data visualization, specifically a Line Chart.",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data; please replace with your actual data\np_values = [0.1, 0.2, 0.3, 0.4, 0.5]\nnmt_values = [0.65, 0.66, 0.67, 0.67, 0.68]\nasr_values = [0.60, 0.59, 0.58, 0.58, 0.57]\ntts_values = [0.50, 0.49, 0.48, 0.48, 0.47]\n\n# Plotting the data\nplt.plot(p_values, nmt_values, 'o-', label='NMT', color='blue')\nplt.plot(p_values, asr_values, 's-', label='ASR', color='orange')\nplt.plot(p_values, tts_values, '^-', label='TTS', color='green')\n\n# Customizing the plot\nplt.xlabel('p')\nplt.ylabel('R(p)')\nplt.title('Attention density ratio R(p) under different p in different tasks')\nplt.legend()\n\n# Saving the figure with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Optionally, display the plot\n# plt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10454/ADR_NOKD_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10454/ADR_KD.png",
        "caption": "Attention density ratio $R(p)$ for NMT and TTS tasks under different $p$ with and without knowledge distillation, where ``KD\" means knowledge distillation.",
        "source": "acl_20/2004.10454/acl2020.tex",
        "arxiv_id": "acl_20/2004.10454",
        "llava_output": "1. The image is a Line Chart that presents the Attention density ratio $R(p)$ for NMT and TTS tasks under different $p$ with and without knowledge distillation.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data that resembles the trends seen in the provided plot\np_values = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\nnmt_values = np.array([0.63, 0.61, 0.59, 0.58, 0.57])\nnmt_kd_values = np.array([0.60, 0.58, 0.56, 0.55, 0.54])\ntts_values = np.array([0.52, 0.51, 0.50, 0.49, 0.48])\ntts_kd_values = np.array([0.47, 0.46, 0.45, 0.44, 0.43])\n\n# Plotting the lines\nplt.plot(p_values, nmt_values, marker='o', label='NMT', color='blue')\nplt.plot(p_values, nmt_kd_values, marker='o', label='NMT (KD)', color='orange')\nplt.plot(p_values, tts_values, marker='^', label='TTS', color='green')\nplt.plot(p_values, tts_kd_values, marker='^', label='TTS (KD)', color='red')\n\n# Adding the legend\nplt.legend()\n\n# Labeling the axes\nplt.xlabel('p')\nplt.ylabel('R(p)')\n\n# Title and grid (optional)\n#plt.title('Attention density ratio R(p) for NMT and TTS tasks')\n#plt.grid(True)\n\n# Saving the figure with 300 DPI\nplt.savefig('attention_density_ratio.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10454/ADR_KD_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10454/ADR_AC.png",
        "caption": "Attention density ratio $R(p)$ for NMT, ASR and TTS tasks under different $p$ with and without alignment constraint (AC).",
        "source": "acl_20/2004.10454/acl2020.tex",
        "arxiv_id": "acl_20/2004.10454",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Simulated data points\np_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n\n# Assuming some random values for demonstration\nnmt_values = [0.60, 0.59, 0.58, 0.57, 0.56]\nasr_values = [0.65, 0.64, 0.63, 0.62, 0.61]\ntts_values = [0.55, 0.54, 0.53, 0.52, 0.51]\n\nnmt_ac_values = [0.50, 0.49, 0.48, 0.47, 0.46]\nasr_ac_values = [0.45, 0.44, 0.43, 0.42, 0.41]\ntts_ac_values = [0.40, 0.39, 0.38, 0.37, 0.36]\n\n# Plotting the NMT, ASR, and TTS tasks with and without alignment constraint (AC)\nplt.plot(p_values, nmt_values, marker='o', color='blue', label='NMT')\nplt.plot(p_values, asr_values, marker='^', color='green', label='ASR')\nplt.plot(p_values, tts_values, marker='s', color='brown', label='TTS')\n\nplt.plot(p_values, nmt_ac_values, marker='o', linestyle='--', color='orange', label='NMT (AC)')\nplt.plot(p_values, asr_ac_values, marker='^', linestyle='--', color='red', label='ASR (AC)')\nplt.plot(p_values, tts_ac_values, marker='s', linestyle='--', color='purple', label='TTS (AC)')\n\n# Adding legend, labels, and setting plot limits\nplt.legend()\nplt.xlabel('p')\nplt.ylabel('R(p)')\nplt.xlim(min(p_values), max(p_values))\nplt.ylim(0.35, 0.70)  # Adjust the limits according to simulated data ranges\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('attention_density_ratio.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10454/ADR_AC_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10813/wiki_comp_configurable.png",
        "caption": "BLI performance in the comparable setting.",
        "source": "acl_20/2004.10813/acl2020_mine.tex",
        "arxiv_id": "acl_20/2004.10813",
        "llava_output": "The image is a line chart that shows the performance of the BLI in the comparable setting.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Replacing these placeholder arrays with your actual data\nwindow_sizes = np.array([1, 2, 3, 4, 5, 7, 10, 15, 20])\nfr_performance = np.array([0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.45, 0.47, 0.475])\nde_performance = np.array([0.01, 0.03, 0.08, 0.15, 0.25, 0.3, 0.35, 0.36, 0.365])\nru_performance = np.array([0.005, 0.025, 0.06, 0.12, 0.18, 0.22, 0.24, 0.245, 0.247])\nja_performance = np.array([0.002, 0.015, 0.04, 0.08, 0.12, 0.15, 0.16, 0.161, 0.162])\nerrors = np.array([0.005]*9)  # Placeholder for the error bars, replace with actual errors\n\n# Plotting the data\nplt.errorbar(window_sizes, fr_performance, yerr=errors, fmt='-o', label='fr', color='#1f77b4')\nplt.errorbar(window_sizes, de_performance, yerr=errors, fmt='--s', label='de', color='#ff7f0e')\nplt.errorbar(window_sizes, ru_performance, yerr=errors, fmt='-.d', label='ru', color='#2ca02c')\nplt.errorbar(window_sizes, ja_performance, yerr=errors, fmt=':x', label='ja', color='#d62728')\n\n# Setting the x-axis to be logarithmic\nplt.xscale('log')\n\n# Adding labels and title\nplt.xlabel('source/target window size')\nplt.ylabel('BLI (mrr)')\n\n# Adding a legend\nplt.legend()\n\n# Setting the x-axis limits\nplt.xlim(1, 20)\n\n# Check the original plot for the exact y-axis limits\nplt.ylim(0, 0.5)\n\n# Saving the figure\nplt.savefig('bli_performance.png', dpi=300)\n\n# Showing the plot for debugging\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10813/wiki_comp_configurable_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10813/news_configurable.png",
        "caption": "BLI performance in the different domain setting.",
        "source": "acl_20/2004.10813/acl2020_mine.tex",
        "arxiv_id": "acl_20/2004.10813",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data (x-axis)\nwindow_sizes = np.array([1, 2, 3, 4, 5, 7, 10, 15, 20])\n\n# Placeholder data for BLI performance (y-axis) for each language setting\nbli_fr = np.array([0.1, 0.15, 0.25, 0.35, 0.4, 0.4, 0.4, 0.4, 0.4])\nbli_de = np.array([0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.3, 0.3, 0.3])\nbli_ru = np.array([0.02, 0.06, 0.12, 0.18, 0.2, 0.22, 0.22, 0.22, 0.22])\nbli_ja = np.array([0.01, 0.03, 0.06, 0.09, 0.12, 0.14, 0.16, 0.16, 0.16])\n\n# Placeholder data for error bars\nerror = np.array([0.01] * 9)\n\n# Plotting the data\nplt.errorbar(window_sizes, bli_fr, yerr=error, label='fr', linestyle='-', marker='s', color='blue')\nplt.errorbar(window_sizes, bli_de, yerr=error, label='de', linestyle='--', marker='o', color='orange')\nplt.errorbar(window_sizes, bli_ru, yerr=error, label='ru', linestyle='-.', marker='^', color='green')\nplt.errorbar(window_sizes, bli_ja, yerr=error, label='ja', linestyle=':', marker='x', color='red')\n\n# Adding labels and title\nplt.xlabel('source/target window size')\nplt.ylabel('BLI (mrr)')\nplt.title('BLI performance in the different domain setting')\n\n# Adding the legend\nplt.legend()\n\n# Saving the plot as a .png file with 300 dpi\nplt.savefig('bli_performance.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10813/news_configurable_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10813/freq_analysis_same_domain.png",
        "caption": "BLI performance with the top 500 frequent and rare words in the comparable setting.",
        "source": "acl_20/2004.10813/acl2020_mine.tex",
        "arxiv_id": "acl_20/2004.10813",
        "llava_output": "The image is a line chart that presents the performance of BLI with the top 500 frequent and rare words in the comparable setting.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data\nwindow_sizes = np.array([1, 2, 3, 4, 5, 7, 10, 15, 20])\nfr_top500 = np.array([0.05, 0.15, 0.25, 0.35, 0.45, 0.52, 0.56, 0.59, 0.6])\nde_top500 = np.array([0.02, 0.08, 0.18, 0.28, 0.38, 0.47, 0.51, 0.54, 0.55])\nru_top500 = np.array([0.01, 0.05, 0.12, 0.22, 0.32, 0.42, 0.46, 0.48, 0.49])\nja_top500 = np.array([0.00, 0.03, 0.07, 0.15, 0.23, 0.28, 0.30, 0.31, 0.32])\n\nfr_bottom500 = np.array([0.05, 0.14, 0.2, 0.28, 0.35, 0.4, 0.41, 0.42, 0.42])\nde_bottom500 = np.array([0.02, 0.07, 0.12, 0.18, 0.24, 0.29, 0.31, 0.32, 0.32])\nru_bottom500 = np.array([0.01, 0.04, 0.09, 0.14, 0.19, 0.21, 0.22, 0.23, 0.23])\nja_bottom500 = np.array([0.00, 0.025, 0.05, 0.1, 0.13, 0.14, 0.15, 0.15, 0.15])\n\n# Error bars for the placeholder data\nerror = np.full(shape=window_sizes.shape, fill_value=0.02)\n\n# Creating the subplot figures\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plotting top500 data\nax1.errorbar(window_sizes, fr_top500, yerr=error, fmt='-o', label='fr', color='blue')\nax1.errorbar(window_sizes, de_top500, yerr=error, fmt='-s', label='de', color='orange')\nax1.errorbar(window_sizes, ru_top500, yerr=error, fmt='--d', label='ru', color='green')\nax1.errorbar(window_sizes, ja_top500, yerr=error, fmt=':p', label='ja', color='red')\nax1.set_title('top500')\n\n# Plotting bottom500 data\nax2.errorbar(window_sizes, fr_bottom500, yerr=error, fmt='-o', label='fr', color='blue')\nax2.errorbar(window_sizes, de_bottom500, yerr=error, fmt='-s', label='de', color='orange')\nax2.errorbar(window_sizes, ru_bottom500, yerr=error, fmt='--d', label='ru', color='green')\nax2.errorbar(window_sizes, ja_bottom500, yerr=error, fmt=':p', label='ja', color='red')\nax2.set_title('bottom500')\n\n# Setting the labels and legend\nfor ax in (ax1, ax2):\n    ax.set_xlabel('source/target window size')\n    ax.set_ylabel('BLI (mrr)')\n    ax.legend()\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('bli_performance.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10813/freq_analysis_same_domain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10813/freq_analysis_different_domain.png",
        "caption": "BLI performance on the top 500 frequent and rare words in the different domain setting.",
        "source": "acl_20/2004.10813/acl2020_mine.tex",
        "arxiv_id": "acl_20/2004.10813",
        "llava_output": "3 Line Charts</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming hypothetical data\nx = np.array([1, 2, 3, 4, 5, 7, 10, 15, 20])\n\n# Replace these with actual data\ny_values_top = {\n    'fr': np.random.rand(9),\n    'de': np.random.rand(9),\n    'ru': np.random.rand(9),\n    'ja': np.random.rand(9),\n}\n\ny_values_bottom = {\n    'fr': np.random.rand(9),\n    'de': np.random.rand(9),\n    'ru': np.random.rand(9),\n    'ja': np.random.rand(9),\n}\n\n# Define colors and markers\ncolors = {'fr': 'b', 'de': 'orange', 'ru': 'g', 'ja': 'r'}\nmarkers = {'fr': 's', 'de': 'o', 'ru': 'D', 'ja': '^'}\nlinestyles = {'fr': '-', 'de': '--', 'ru': '-.', 'ja': ':'}\n\n# Create the subplots\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n# Subplot for top 500\nfor lang, values in y_values_top.items():\n    ax[0].plot(x, values, label=lang, color=colors[lang], linestyle=linestyles[lang], marker=markers[lang])\nax[0].set_title('top500')\nax[0].set_xlabel('source/target window size')\nax[0].set_ylabel('BLI (mrr)')\nax[0].legend()\n\n# Subplot for bottom 500\nfor lang, values in y_values_bottom.items():\n    ax[1].plot(x, values, label=lang, color=colors[lang], linestyle=linestyles[lang], marker=markers[lang])\nax[1].set_title('bottom500')\nax[1].set_xlabel('source/target window size')\n# ax[1].set_ylabel('BLI (mrr)') # We can leave this out to reduce redundancy\nax[1].legend()\n\n# Adjust the layout\nplt.tight_layout()\n\n# Save the resulting figure as a .png file with 300 dpi\nplt.savefig('bli_performance.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10813/freq_analysis_different_domain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.10964/domains_intuition.png",
        "caption": "An illustration of data distributions. Task data is comprised of an observable task distribution, usually non-randomly sampled from a wider distribution (light grey ellipsis) within an even larger target domain, which is not necessarily one of the domains included in the original LM pretraining domain -- though overlap is possible. We explore the benefits of continued pretraining on data from the task distribution and the domain distribution.",
        "source": "acl_20/2004.10964/1_intro.tex",
        "arxiv_id": "acl_20/2004.10964",
        "llava_output": "0</s>",
        "is_data_visual": "Based on the provided justification and figure caption, it is difficult to determine with certainty whether the image represents a data visualization plot or not. However, based on the caption describing an exploration of benefits for pretraining on data from the task distribution and domain distribution, it is possible that the image may be related to data visualization. Without further information such as a specific data visualization chart or plot type, it is impossible to say for certain if the image is a data visualization.",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Polygon\nimport numpy as np\n\n# Create a new figure and a subplot with no axes\nfig, ax = plt.subplots()\nax.axis('off')\n\n# Draw triangles for target domain and original LM domain\ntarget_domain = Polygon([[0.05, 0.05], [0.95, 0.5], [0.05, 0.95]], color='blue', alpha=0.5)\nlm_domain = Polygon([[0.95, 0.05], [0.95, 0.95], [0.55, 0.5]], color='orange', alpha=0.5)\n\nax.add_patch(target_domain)\nax.add_patch(lm_domain)\n\n# Draw ellipses for task domain and wider distribution\nwider_distribution = patches.Ellipse((0.5, 0.5), 0.8, 0.5, color='grey', alpha=0.5)\ntask_distribution = patches.Ellipse((0.5, 0.5), 0.2, 0.1, color='black', alpha=0.5)\n\nax.add_patch(wider_distribution)\nax.add_patch(task_distribution)\n\n# Add label to the image\nax.text(0.05, 0.05, 'target domain', color='white', fontsize=12, va='bottom')\nax.text(0.95, 0.05, 'original LM domain', color='white', fontsize=12, ha='right', va='bottom')\n\n# Set aspect of the plot to be equal\nax.set_aspect('equal')\n\n# Set limits and save the figure\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.savefig('data_distribution.png', dpi=300, bbox_inches='tight', pad_inches=0)\n\n# Show the plot for verification\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.10964/domains_intuition_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.11054/dialog_diagram2_thick.png",
        "caption": "Illustration of reinforcement learning for dialog management. The agent (top right) interacts with the environment (left) by taking actions, and observing the resulting new state and reward. DQfD and RoFL RL agents are guided by an expert demonstrator (bottom right).",
        "source": "acl_20/2004.11054/acl.tex",
        "arxiv_id": "acl_20/2004.11054",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyArrowPatch, Circle\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.set_xlim(0, 10)\nax.set_ylim(0, 5)\nax.axis('off')\n\n# Drawing arrows\narrowprops = dict(arrowstyle=\"->\", color=\"blue\", lw=2)\nstate_reward_arrow = FancyArrowPatch((2, 3.5), (8, 3.5), **arrowprops)\naction_arrow = FancyArrowPatch((8, 2.5), (2, 2.5), **arrowprops)\n\nax.add_patch(state_reward_arrow)\nax.add_patch(action_arrow)\n\n# Circular arrows indicating feedback\narrowprops.update(connectionstyle=\"arc,angleA=-30,angleB=90,armA=30,armB=30,rad=10\")\nfeedback_arrow_l = FancyArrowPatch((1, 0), (1, 4), **arrowprops)\nfeedback_arrow_r = FancyArrowPatch((9, 4), (9, 0), **arrowprops)\n\nax.add_patch(feedback_arrow_l)\nax.add_patch(feedback_arrow_r)\n\n# Adding text\nax.text(5, 3.8, \"State, Reward\", va='center', ha='center', fontsize=12)\nax.text(5, 2.2, \"Action\", va='center', ha='center', fontsize=12)\n\n# Save the figure\nplt.savefig('reinforcement_learning_diagram.png', dpi=300)\n\nplt.show()\nfrom PIL import Image\n\n# Let's assume you have the following image paths\nhuman_figure = Image.open('path_to_human_figure_image.png')\nbrain_image = Image.open('path_to_brain_image.png')\ndatabase_image = Image.open('path_to_database_image.png')\n\n# You would then paste these images into the plot at the desired locations\n# ax.figimage() and/or PIL's paste method could be used here",
        "runnable": false,
        "output_figure_path": "chat-gpt-v/acl_20/2004.11054/dialog_diagram2_thick_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.11892/effect_of_data.png",
        "caption": "A comparison of the effect of the size of synthetic data on downstream QA performance.",
        "source": "acl_20/2004.11892/acl2020.tex",
        "arxiv_id": "acl_20/2004.11892",
        "llava_output": "The image is a Line Chart that shows the effect of the size of synthetic data on downstream QA performance.</s>",
        "is_data_visual": "Yes based on the justification and figure caption describing that the image is a Line Chart showing the effect of synthetic data size on downstream QA performance.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Replace these with the actual data points\nsynthetic_data_x = np.linspace(0, 400, 100)  # Sample sizes\nsynthetic_data_y = np.sin(synthetic_data_x/100) + 55 # Randomly generated scores\n\nlewis_data_x = np.linspace(0, 400, 100)  # Sample sizes\nlewis_data_y = np.cos(lewis_data_x/100) + 47  # Randomly generated scores to simulate a trend\n\n# Create the plot\nplt.figure(figsize=(10, 8))  # Size to match the given dimensions\n\n# Plot both series of data\nplt.plot(synthetic_data_x, synthetic_data_y, label=\"Synthetic template data\", color='black')\nplt.plot(lewis_data_x, lewis_data_y, label=\"Data from Lewis et al., 2019\", color='green')\n\n# Add labels and title\nplt.xlabel(\"# Samples (in thousands)\")\nplt.ylabel(\"F1 Score\")\nplt.title(\"A comparison of the effect of the size of synthetic data on downstream QA performance\")\n\n# Add the legend\nplt.legend()\n\n# Customize the tick marks (optional)\nplt.xticks(np.arange(0, 450, 50))\nplt.yticks(np.arange(44, 59, 2))\n\n# Save the plot with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.11892/effect_of_data_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.12239/loss.png",
        "caption": "Loss on development set on IMDB and Yahoo! Answer in each epoch while training with 200 labeled data and 5000 unlabeled data per class.",
        "source": "acl_20/2004.12239/acl2020.tex",
        "arxiv_id": "acl_20/2004.12239",
        "llava_output": "The image is a line chart that shows the loss on development set on IMDB and Yahoo! Answer in each epoch while training with 200 labeled data and 5000 unlabeled data per class.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Example data (replace these lists with your actual data points)\nepochs = list(range(30))  # 0-29 epochs\nbert_imdb = [0.4] * 30\nuda_imdb = [0.3] * 30\ntmix_imdb = [0.25] * 30\nmixtext_imdb = [0.2] * 30\n\nbert_yahoo = [1.0] * 30\nuda_yahoo = [1.2] * 30\ntmix_yahoo = [0.9] * 30\nmixtext_yahoo = [1.15] * 30\n\n# Create subplots with shared x-axis\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\n# Plotting for the IMDB dataset\nax1.plot(epochs, bert_imdb, label='Bert', color='green')\nax1.plot(epochs, uda_imdb, label='UDA', color='magenta')\nax1.plot(epochs, tmix_imdb, label='TMix', color='red')\nax1.plot(epochs, mixtext_imdb, label='MixText', color='blue')\nax1.set_title('IMDB')\nax1.set_ylabel('Loss')\n\n# Plotting for the Yahoo! Answer dataset\nax2.plot(epochs, bert_yahoo, label='Bert', color='green')\nax2.plot(epochs, uda_yahoo, label='UDA', color='magenta')\nax2.plot(epochs, tmix_yahoo, label='TMix', color='red')\nax2.plot(epochs, mixtext_yahoo, label='MixText', color='blue')\nax2.set_title('Yahoo! Answer')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Loss')\n\n# Adjust the plot to prevent overlapping of subplots\nplt.tight_layout()\n\n# Show legend\nax1.legend()\nax2.legend()\n\n# Save the figure\nplt.savefig('loss_plot.png', dpi=300)\n\n# Show the plot (optional)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.12239/loss_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.12239/loss.png",
        "caption": "Loss on development set on IMDB and Yahoo! Answer in each epoch while training with 200 labeled data and 5000 unlabeled data per class.",
        "source": "acl_20/2004.12239/MixText v2/acl2020.tex",
        "arxiv_id": "acl_20/2004.12239",
        "llava_output": "The image is a line chart showing the loss on development set on IMDB and Yahoo! Answer in each epoch while training with 200 labeled data and 5000 unlabeled data per class.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some sample data for demonstration purposes\nepochs = np.arange(1, 31)\nbert_imdb = np.random.uniform(0.2, 0.8, size=30)\nuda_imdb = np.random.uniform(0.1, 0.7, size=30)\ntmix_imdb = np.random.uniform(0.1, 0.6, size=30)\nmixtext_imdb = np.random.uniform(0.15, 0.65, size=30)\n\nbert_yahoo = np.random.uniform(1.0, 2.0, size=30)\nuda_yahoo = np.random.uniform(0.9, 1.9, size=30)\ntmix_yahoo = np.random.uniform(0.85, 1.85, size=30)\nmixtext_yahoo = np.random.uniform(0.95, 1.95, size=30)\n\n# Create the figure and subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7))\n\n# Plot the IMDB data\nax1.plot(epochs, bert_imdb, label='Bert', color='green')\nax1.plot(epochs, uda_imdb, label='UDA', color='magenta')\nax1.plot(epochs, tmix_imdb, label='TMix', color='red')\nax1.plot(epochs, mixtext_imdb, label='MixText', color='blue')\nax1.set_title('IMDB')\nax1.set_xlim([1, 30])\nax1.set_ylim([0, max(bert_imdb.max(), uda_imdb.max(), tmix_imdb.max(), mixtext_imdb.max())])\n\n# Plot the Yahoo! Answer data\nax2.plot(epochs, bert_yahoo, color='green')\nax2.plot(epochs, uda_yahoo, color='magenta')\nax2.plot(epochs, tmix_yahoo, color='red')\nax2.plot(epochs, mixtext_yahoo, color='blue')\nax2.set_title('Yahoo! Answer')\nax2.set_xlim([1, 30])\nax2.set_ylim([min(bert_yahoo.min(), uda_yahoo.min(), tmix_yahoo.min(), mixtext_yahoo.min()), max(bert_yahoo.max(), uda_yahoo.max(), tmix_yahoo.max(), mixtext_yahoo.max())])\n\n# Add the legend and save the figure\nplt.legend()\nplt.tight_layout()\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.12239/loss_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.13671/closure_runtimes.png",
        "caption": "Under each closure algorithm, the time to compute the closure after the next annotation is added, as \\# of existing annotations increases.",
        "source": "acl_20/2004.13671/supplementary_material.tex",
        "arxiv_id": "acl_20/2004.13671",
        "llava_output": "The image is a line chart showing the time to compute the closure after the next annotation is added, as the number of existing annotations increases.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Synthetic data to mimic the plot in the figure\nnp.random.seed(0)  # For reproducibility\nx = np.arange(0, 1600)\ny1 = 0.1 * x + np.random.normal(0, 5, size=x.size)  # Incremental closure\ny2 = 0.8 * x + np.random.normal(0, 150, size=x.size)  # Closure\n\n# Creating spikes in 'closure' data to simulate the spikes in the original plot\nspike_indices = np.random.choice(x[100::200], 7, replace=False)\ny2[spike_indices] += np.random.normal(1000, 500, size=spike_indices.size)\n\n# Plotting both datasets\nplt.plot(x, y1, label='incremental closure', color='blue')\nplt.plot(x, y2, label='closure', color='orange')\n\n# Adding plot decorations\nplt.xlabel('Number of existing annotations')\nplt.ylabel('Time (ms) to compute next closure')\nplt.title('Under each closure algorithm, the time to compute the closure after the next annotation is added,\\nas # of existing annotations increases.')\nplt.legend()\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.13671/closure_runtimes_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14096/postag_all_nodiff.png",
        "caption": "UAS accuracy for the average models (BERT 13, ELMo 3) on incoming dependencies of different part-of-speech categories.",
        "source": "acl_20/2004.14096/acl2020.tex",
        "arxiv_id": "acl_20/2004.14096",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming hypothetical data for the part of speech categories.\ncategories = [\n    'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART',\n    'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'\n]\nbert_ud = [45, 55, 67, 80, 54, 33, 44, 88, 70, 77, 65, 45, 85, 32, 43, 50, 55]  # Hypothetical values for BERT-UD\nbert_sud = [40, 50, 62, 75, 50, 30, 40, 83, 65, 72, 60, 40, 80, 27, 38, 45, 50] # Hypothetical values for BERT-SUD\nelmo_ud = [35, 60, 70, 85, 58, 37, 48, 90, 75, 82, 68, 49, 88, 39, 47, 55, 60]  # Hypothetical values for ELMO-UD\nelmo_sud = [30, 45, 65, 80, 53, 32, 43, 85, 70, 78, 63, 44, 84, 34, 42, 53, 58] # Hypothetical values for ELMO-SUD\n\n# The actual plotting\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Assigning colors to each model\ncolors = ['blue', 'darkblue', 'red', 'darkred']\n\n# Determining the bar position on y-axis\ny_positions = range(len(categories))\n\n# Plotting BERT-UD and BERT-SUD\nax.barh(y_positions, bert_ud, color=colors[0], edgecolor='black', height=0.4)\nax.barh(y_positions, bert_sud, color=colors[1], edgecolor='black', height=0.4, left=bert_ud)\n\n# Plotting ELMO-UD and ELMO-SUD\nax.barh([y+0.4 for y in y_positions], elmo_ud, color=colors[2], edgecolor='black', height=0.4)\nax.barh([y+0.4 for y in y_positions], elmo_sud, color=colors[3], edgecolor='black', height=0.4, left=elmo_ud)\n\n# Adding category labels\nax.set_yticks([i+0.2 for i in y_positions])\nax.set_yticklabels(categories)\n\n# Adding the legend\nax.legend(['BERT-UD', 'BERT-SUD', 'ELMO-UD', 'ELMO-SUD'], loc='lower right')\n\n# Adding labels and title if needed\nax.set_title('UAS accuracy for the average models (BERT 13, ELMo 3) on incoming dependencies of different part-of-speech categories')\nax.set_xlabel('UAS')\n\n# Reversing the order of categories on y-axis to match the image\nax.invert_yaxis()\n\n# Saving the figure\nplt.tight_layout()\nplt.savefig('uas_accuracy_plot.png', dpi=300)\n\n# Showing the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14096/postag_all_nodiff_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14096/graph_stats_combined.png",
        "caption": "UAS across sentence length bins (top); F1 across varying dependency lengths (middle); F1 across varying distances to root (bottom)",
        "source": "acl_20/2004.14096/acl2020.tex",
        "arxiv_id": "acl_20/2004.14096",
        "llava_output": "1. Line Chart: The image shows a line chart with three lines, each representing a different sentence length. The chart is used to display the UAS across sentence length bins.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data for the three plots\nsentence_lengths = ['1-10', '11-20', '21-30', '31-40', '41-50', '50+']\ndependency_lengths = ['root', '1', '2', '3', '4', '5', '6', '7', '8', '9', '>=10']\ndistance_to_root = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '>=10']\n\n# Random data for demonstration\nuas_data = {\n    'BERT-UD': np.random.randint(50, 100, size=len(sentence_lengths)),\n    'BERT-SUD': np.random.randint(50, 100, size=len(sentence_lengths)),\n    'ELMO-UD': np.random.randint(50, 100, size=len(sentence_lengths)),\n    'ELMO-SUD': np.random.randint(50, 100, size=len(sentence_lengths)),\n}\n\nf1_dependency_data = {\n    'BERT-UD': np.random.rand(len(dependency_lengths)),\n    'BERT-SUD': np.random.rand(len(dependency_lengths)),\n    'ELMO-UD': np.random.rand(len(dependency_lengths)),\n    'ELMO-SUD': np.random.rand(len(dependency_lengths)),\n}\n\nf1_distance_data = {\n    'BERT-UD': np.random.rand(len(distance_to_root)),\n    'BERT-SUD': np.random.rand(len(distance_to_root)),\n    'ELMO-UD': np.random.rand(len(distance_to_root)),\n    'ELMO-SUD': np.random.rand(len(distance_to_root)),\n}\n\n# Function to create each subplot\ndef create_subplot(ax, x, data, title, xlabel, ylabel):\n    for label, points in data.items():\n        if 'UD' in label:\n            linestyle = '-'\n        else:\n            linestyle = '--'\n        \n        ax.plot(x, points, linestyle=linestyle, label=label)\n        \n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.legend()\n\n# Creating the figure with subplots\nfig, axs = plt.subplots(3, 1, figsize=(8, 12))\n\ncreate_subplot(axs[0], sentence_lengths, uas_data, 'UAS', 'Sentence Length', 'UAS')\ncreate_subplot(axs[1], dependency_lengths, f1_dependency_data, 'F1', 'Dependency Length', 'F1')\ncreate_subplot(axs[2], distance_to_root, f1_distance_data, 'F1', 'Distance to Root', 'F1')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 dpi resolution\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14096/graph_stats_combined_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14096/results_min_bert_combined.png",
        "caption": "Probe results per framework, layer, and language, when trained on 3664 sentences. First row depicts UAS per layer and language for BERT, with average performance and error over UD/SUD in 3rd column. Bottom two row depicts the difference in UAS across UD ($+$) and SUD ($-$).",
        "source": "acl_20/2004.14096/acl2020.tex",
        "arxiv_id": "acl_20/2004.14096",
        "llava_output": "1. The image is a scatter plot showing the performance of BERT in different languages.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (replace with actual data)\nnp.random.seed(0)  # For reproducibility\nlayers = np.arange(1, 14)\nlanguages = ['arb', 'cmn', 'eng', 'eus', 'fin', 'heb', 'hin', 'ita', 'jpn', 'kor', 'rus', 'swe', 'tur']\nuas_data = {lang: np.random.randint(50, 90, size=len(layers)) for lang in languages}  # UAS per language for BERT-UD and BERT-SUD\naverage_performance = np.mean(list(uas_data.values()), axis=0)\nerror = np.random.rand(len(layers)) * 5  # Random error values\nuas_difference = {lang: np.random.uniform(-10, 10, size=len(layers)) for lang in languages}  # Difference in UAS between UD and SUD\n\n# Create a figure and a grid of subplots\nfig, axs = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n\n# Plot UAS for BERT-UD and BERT-SUD\nfor ax, schema in zip(axs[:2], ['BERT-UD', 'BERT-SUD']):\n    for lang, data in uas_data.items():\n        ax.plot(layers, data, label=lang)\n\n    ax.set_title(schema)\n    ax.set_ylim(0, 100)\n    ax.set_ylabel('UAS')\n\n# Plot the average performance and error over UD/SUD in the 3rd column\naxs[2].plot(layers, average_performance, label='ud', color='red')\naxs[2].fill_between(layers, average_performance-error, average_performance+error, color='red', alpha=0.3)\naxs[2].set_title('BERT')\naxs[2].set_ylim(0, 100)\naxs[2].set_ylabel('UAS')\n\n# Plot the difference in UAS across UD (+) and SUD (-)\nax_dif = axs[2].twinx()\nfor lang, data in uas_difference.items():\n    ax_dif.bar(layers - 0.4 + 0.1 * languages.index(lang), data, width=0.1, label=lang)\n\nax_dif.axhline(0, color='black', linewidth=0.8)\nax_dif.set_ylim(-15, 15)\nax_dif.set_ylabel('\u0394 UAS')\n\n# Add legend\nhandles, labels = [], []\nfor ax in axs:\n    for handle, label in zip(*ax.get_legend_handles_labels()):\n        if label not in labels:\n            handles.append(handle)\n            labels.append(label)\n\nfig.legend(handles, labels, loc='lower center', ncol=len(languages) // 2, bbox_to_anchor=(0.5, 0))\n\n# Adjust layout\nfig.tight_layout(rect=[0, 0.05, 1, 1])\n\n# Save as PNG file with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14096/results_min_bert_combined_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14243/final_importance_ranking_3_2.png",
        "caption": "Box plots of fraction of hidden representations removed for a decision flip. Dataset and models are mentioned at the top and bottom of figures. Blue and Yellow indicate the attention and random ranking.",
        "source": "acl_20/2004.14243/experiments_transparency.tex",
        "arxiv_id": "acl_20/2004.14243",
        "llava_output": "The image is a collection of box plots, each representing a different dataset and model. The box plots are arranged in a grid-like pattern, with each plot showing the fraction of hidden representations removed for a decision flip. The plots are labeled with the dataset and models used, and the y-axis shows the",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Random data to simulate the box plots\nnp.random.seed(0)\ndata = [np.random.rand(50) for _ in range(3)]\n\nlabels = ['Vanilla', 'Diversity', 'Ortho']\nmodels = ['Yelp', 'IMDB', 'Amazon', '20News', 'QQP', 'bAbI 1']\n\nfig, axs = plt.subplots(2, 3, figsize=(10, 8))  # nrows=2, ncols=3\n\nfor i, ax in enumerate(axs.flatten()):\n    # Generate random data for the sake of illustration\n    model_data = [np.random.rand(50) for _ in range(3)]\n    \n    # Plot boxplot with Attention as blue and Random as yellow\n    bpl = ax.boxplot(model_data, positions=np.array(range(len(data)))*2.0-0.4, widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"#1f77b4\"))\n    bpr = ax.boxplot(model_data, positions=np.array(range(len(data)))*2.0+0.4, widths=0.6, patch_artist=True, boxprops=dict(facecolor=\"#ffec8b\"))\n    \n    # Set model name as title\n    ax.set_title(models[i // 3 * 3 + i % 3])\n    ax.set_xticks(range(0, len(labels) * 2, 2))\n    ax.set_xticklabels(labels)\n    ax.set_ylim(0, 1)  # Assuming the fraction is between 0 and 1\n\n# Add legend\nfig.legend([bpl[\"boxes\"][0], bpr[\"boxes\"][0]], ['Attention', 'Random'], loc='upper right')\n\n# Tidy layout and save the figure\nplt.tight_layout()\nplt.savefig('boxplot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14243/final_importance_ranking_3_2_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14243/pos_3-2.png",
        "caption": "Distribution of cumulative attention given to different part-of-speech tags in the test dataset. Blue and Orange indicate the vanilla and Diversity LSTMs.",
        "source": "acl_20/2004.14243/experiments_transparency.tex",
        "arxiv_id": "acl_20/2004.14243",
        "llava_output": "The image is a bar chart showing the distribution of cumulative attention given to different part-of-speech tags in the test dataset. The bars are color-coded to indicate the performance of the vanilla and Diversity LSTMs.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# This is placeholder data. You'll need to replace it with your actual data.\ncategories = ['NOUN', 'ADJ', 'VERB', 'ADV', 'ADP', 'CONJ', 'DET', 'PRON', 'PRT', 'PUNCT', 'NUM', 'X']\nyelp_vanilla = np.random.rand(12) * 30\nyelp_diversity = np.random.rand(12) * 30\n# Repeat the above for Amazon, SST, 20News, Tweets, and QQP with appropriate values for each.\n\nfig, axes = plt.subplots(2, 3, figsize=(10, 6), sharey=True)\n\ndef draw_subplot(ax, category_names, vanilla_data, diversity_data, title):\n    y_pos = np.arange(len(category_names))\n    ax.barh(y_pos, vanilla_data, color='blue', label='Vanilla')\n    ax.barh(y_pos, diversity_data, color='orange', label='Diversity', left=vanilla_data)\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(category_names)\n    ax.set_title(title)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.grid(True)\n\n# You'll need to call `draw_subplot` for each subplot with the correct data and title.\ndraw_subplot(axes[0, 0], categories, yelp_vanilla, yelp_diversity, 'Yelp')\n# Repeat the above for each of your datasets to fill in the other subplots\n\nfig.suptitle('Distribution of cumulative attention given to different part-of-speech tags in the test dataset')\nfig.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rect to fit the main title\n\n# Add a single legend\nhandles, labels = axes[0, 0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', ncol=2)\n\nplt.savefig('part_of_speech_tags.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14243/pos_3-2_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14243/Ortho.png",
        "caption": "Orthogonal LSTM: Hidden state at a timestep is orthogonal to the mean of previous hidden states",
        "source": "acl_20/2004.14243/proposed_model.tex",
        "arxiv_id": "acl_20/2004.14243",
        "llava_output": "The image is a scatter plot showing the hidden state of a LSTM model over time.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Define vectors - replace with your actual vectors\nmean_vector = np.array([0, 1])\nnew_hidden_state = np.array([1, 0])\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Add vectors to the plot\nax.quiver(0, 0, mean_vector[0], mean_vector[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Mean')\nax.quiver(0, 0, new_hidden_state[0], new_hidden_state[1], angles='xy', scale_units='xy', scale=1, color='black', linestyle='dashed', label='New hidden state')\n\n# Fill the angle between vectors\nt = np.linspace(0, np.arctan2(new_hidden_state[1], new_hidden_state[0]), 100)\nr = np.linspace(0, 1, 100)\nr_grid, t_grid = np.meshgrid(r, t)\nx_grid, y_grid = r_grid * np.cos(t_grid), r_grid * np.sin(t_grid)\nax.fill_betweenx(y_grid[:,0], x_grid[:,0], color='yellow', alpha=0.3)\n\n# Set the aspect of the plot to be equal\nax.set_aspect('equal')\n\n# Place the labels\nplt.text(-0.5, 0.5, 'Mean', fontsize=12, ha='center')\nplt.text(1.5, -0.3, 'New hidden state', fontsize=12, ha='center')\n\n# Set the limits of the plot\nax.set_xlim(-1, 2)\nax.set_ylim(-1, 2)\n\n# Remove axes\nax.set_axis_off()\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14243/Ortho_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14302/boxplot.png",
        "caption": "Box plot of Spearman's rank correlation coefficients between the ground-truth ranking and the rankings by RANDOM. A dot in blue indicates the correlation coefficient of CHOSEN.",
        "source": "acl_20/2004.14302/acl2020_camera_ready.tex",
        "arxiv_id": "acl_20/2004.14302",
        "llava_output": "The image is a box plot, which is used to present the results of an experiment. It shows the distribution of Spearman's rank correlation coefficients between the ground-truth ranking and the rankings by RANDOM. The box plot includes a median line, which is the center of the box, and",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the random data for the box plot\n# The data should come from the actual research for accuracy\nnp.random.seed(0)\ndata = np.random.rand(100) * 0.5 + 0.25  # Random data centered around 0.5\nchosen_value = 0.65  # An example value for 'CHOSEN', the blue dot\n\n# Create the box plot\nplt.figure(figsize=(10, 4))  # Set an appropriate figure size\nplt.boxplot(data, vert=False, widths=0.6, patch_artist=True, showfliers=False)\n\n# Add the 'CHOSEN' dot in blue\nplt.plot(chosen_value, 1, 'o', color='blue')  # '1' represents the y-axis position\n\n# Set the title of the x-axis\nplt.xlabel(\"Spearman's rank correlation coefficient\")\n\n# Remove y-axis labels\nplt.yticks([])\n\n# Save the figure with 300 DPI\nplt.savefig('boxplot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14302/boxplot_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2004.14675/exampleGold.png",
        "caption": "Word alignment generated by a human annotator.",
        "source": "acl_20/2004.14675/acl2020.tex",
        "arxiv_id": "acl_20/2004.14675",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Define the sentence data\nsource_sentence = \"Das stimmt nicht !\".split()\ntarget_sentence = \"But this is not what happens .\".split()\n\n# Define the alignment matrix (manually set according to the image)\nalignment_matrix = [\n    [0, 1, 0, 0, 0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 1],\n    [1, 0, 1, 1, 1, 0, 1, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 1],\n]\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Draw the grid\nfor i, row in enumerate(alignment_matrix):\n    for j, cell in enumerate(row):\n        color = 'black' if cell == 1 else 'white'\n        rect = plt.Rectangle((j, -i), 1, 1, facecolor=color, edgecolor='black')\n        ax.add_patch(rect)\n\n# Set the ticks and labels\nax.set_xticks([i + 0.5 for i in range(len(target_sentence))])\nax.set_xticklabels(target_sentence, rotation=45, ha='right')\nax.set_yticks([-i - 0.5 for i in range(len(source_sentence))])\nax.set_yticklabels(source_sentence)\n\n# Set the limits and aspect\nax.set_xlim(0, len(target_sentence))\nax.set_ylim(-len(source_sentence), 0)\nax.set_aspect('equal')\n\n# Turn off the axes\nax.axis('off')\n\n# Save the figure\nplt.savefig('word_alignment.png', dpi=300)\n\n# Show the plot in case you want to see it in the python interpreter\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2004.14675/exampleGold_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00110/model_figure_v2.png",
        "caption": "Our model architecture, mixing terminology from the autoencoder and signaling game traditions.",
        "source": "acl_20/2005.00110/emergence_discrete.tex",
        "arxiv_id": "acl_20/2005.00110",
        "llava_output": "0</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyBboxPatch\n\n# Define a function to create a box\ndef create_box(ax, rect, text, edgecolor, facecolor):\n    box = FancyBboxPatch((rect[0], rect[1]), rect[2], rect[3],\n                         boxstyle=\"round,pad=0.1\", \n                         ec=edgecolor, fc=facecolor, \n                         mutation_aspect=0.5)\n    ax.add_patch(box)\n    rx, ry = box.get_x(), box.get_y()\n    cx, cy = rx + box.get_width()/2.0, ry + box.get_height()/2.0\n\n    ax.annotate(text, (cx, cy), color='black', weight='bold', \n                fontsize=10, ha='center', va='center')\n\n# Create the figure\nfig, ax = plt.subplots(figsize=(10, 5))\nax.set_xlim(0, 10)\nax.set_ylim(0, 5)\nax.axis('off')\n\n# Create boxes\ncreate_box(ax, [1, 2, 2, 1], 'Sender\\nencoder', 'black', '#D3D3D3')\ncreate_box(ax, [4, 3.2, 2, 0.5], 'Message', 'blue', '#ADD8E6')\ncreate_box(ax, [7, 2, 2, 1], 'Receiver\\ndecoder', 'black', '#D3D3D3')\n\n# Create arrows\nplt.arrow(2.1, 2.5, 1.8, 0, head_width=0.3, head_length=0.2, fc='k', ec='k')\nplt.arrow(7.1, 2.5, 1.8, 0, head_width=0.3, head_length=0.2, fc='k', ec='k')\n\n# Create labels\nplt.text(0.5, 4.75, 'Piece of information\\nfunction f', fontsize=9, ha='center', backgroundcolor='#90EE90')\nplt.text(10, 4.75, 'Action\\ngoal f(c)', fontsize=9, ha='center', backgroundcolor='#90EE90')\nplt.text(5, 1, 'Latent space', fontsize=9, ha='center')\n\n# Create context labels\nplt.text(1.5, 3.75, \"Sender's perspective\\ncontext c\", fontsize=9, ha='center', backgroundcolor='#FFD700')\nplt.text(8.5, 3.75, \"Receiver's perspective\\ncontext c'\", fontsize=9, ha='center', backgroundcolor='#FFD700')\n\n# Save the figure\nplt.savefig('model_architecture.png', dpi=300)",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00110/model_figure_v2_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00110/messages_o5_untrained.png",
        "caption": "Sampled messages for contexts of 10 objects of size 5 for (a)~an untrained and (b)~a trained network. Colors represent the $f_i \\in F$ input part of the Sender.",
        "source": "acl_20/2005.00110/emergence_discrete.tex",
        "arxiv_id": "acl_20/2005.00110",
        "llava_output": "1. Histogram: The image is a histogram showing the distribution of different colors for a trained and untrained network. The colors represent the $f_i \\in F$ input part of the Sender.</s>",
        "is_data_visual": "Yes. This image represents a data visualization histogram showing the distribution of different colors for a trained and untrained network, and the colors represent the $f_i \\in F$ input part of the Sender.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data generation (you should replace this with your actual dataset)\nnum_objects = 10\nsize_per_object = 5\nnum_samples = num_objects * size_per_object\n\n# Generate random sample data\nnp.random.seed(0)  # For reproducibility\ncoordinates_1 = np.random.uniform(0.04, 0.16, num_samples)\ncoordinates_2 = np.random.uniform(0, 0.08, num_samples)\n\n# Create an array with object categories (F0, F1, ..., F9) for each point\ncategories = np.repeat(np.arange(num_objects), size_per_object)\ncolors = plt.cm.tab10(categories)  # Generate colors from the 'tab10' colormap, which has 10 distinct colors\n\n# Plotting\nplt.figure(figsize=(8, 5))\nfor i in range(num_objects):\n    plt.scatter(\n        coordinates_1[categories == i],\n        coordinates_2[categories == i],\n        color=colors[i],\n        label=f'F{i}',\n        alpha=0.5, \n        edgecolors='none'\n    )\n\nplt.xlabel('Coordinate 1')\nplt.ylabel('Coordinate 2')\nplt.legend()\n\n# Save the figure with 300 dpi resolution\nplt.savefig('scatter_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00110/messages_o5_untrained_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00110/categorical_perception.png",
        "caption": "Categorical perception effect, demonstrated by accuracy of object recovery using messages shifted between two `meanings'.",
        "source": "acl_20/2005.00110/emergence_discrete.tex",
        "arxiv_id": "acl_20/2005.00110",
        "llava_output": "The image is a Line Chart that presents the experiment results.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical data (you would replace these with the actual data)\nx = np.linspace(-5, 5, 100)\ny1 = np.maximum(0, np.exp(-((x + 2)**2)) - 0.05) \ny2 = np.maximum(0, np.exp(-((x - 2)**2)) - 0.05) \n\n# Hypothetical error (you would replace these with the actual error values)\nerror1 = 0.1 * np.sqrt(y1)\nerror2 = 0.1 * np.sqrt(y2)\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Plot both sets of data with error bars\nax.errorbar(x, y1, yerr=error1, fmt='-o', color='green', label='F_-1')\nax.errorbar(x, y2, yerr=error2, fmt='-o', color='red', label='F_+1')\n\n# Add axis labels and title\nax.set_xlabel('Message shift (t)')\nax.set_ylabel('Object recovery accuracy')\nax.set_title('Categorical perception effect, demonstrated by accuracy of object recovery using messages shifted between two `meanings`.')\n\n# Add grid\nax.grid(True)\n\n# Add legend\nax.legend()\n\n# Save the figure with high resolution\nplt.savefig('categorical_perception_effect.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00110/categorical_perception_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00436/time.png",
        "caption": "The inference speed of our BiFlaG and compared models on GENIA test set. t/s indicates token per second.",
        "source": "acl_20/2005.00436/acl2020.tex",
        "arxiv_id": "acl_20/2005.00436",
        "llava_output": "The image is a line chart that presents the inference speed of the BiFlaG and compared models on the GENIA test set. The chart shows the inference speed in token per second (t/s) for each model.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Data for plotting\nmodels = ['BiFlaG (Ours)', 'Zheng et al., 2019', 'Sohrab and Miwa, 2018', 'Ju et al., 2018']\nspeeds = [6708, 4751, 2851, 3563]\ncolors = ['green', 'orange', 'blue', 'red']\n\n# Create bar chart\nplt.figure(figsize=(8, 5))\nbars = plt.bar(models, speeds, color=colors)\n\n# Add the data labels on the bars\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, yval, int(yval), va='bottom', ha='center')\n\n# Set labels and title\nplt.ylabel('Tokens per second (t/s)')\nplt.title('Inference Speed (t/s)')\n\n# Remove x-axis label\nplt.xlabel('')\n\n# Add legend\nplt.legend(models, loc='upper right')\n\n# Save the figure with 300 DPI\nplt.savefig('inference_speed.png', dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00436/time_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00456/mlm_score.png",
        "caption": "Visualization of the masked language modelling (MLM) metric. Context words are in grey; response words are in red. The red words are masked, and RoBERTa must predict the likelihood of their true value (shown in green).",
        "source": "acl_20/2005.00456/acl2020.tex",
        "arxiv_id": "acl_20/2005.00456",
        "llava_output": "This image is a visualization of the masked language modelling (MLM) metric. The context words are in grey, and the response words are in red. The red words are masked, and RoBERTa must predict the likelihood of their true value (shown in green). The image",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the sentences and color codes\nsentences = [\n    [\"how\", \"are\", \"you\", \"_eos\", \"i\", \"am\", \"ok\", \"_eos\"],\n    [\"how\", \"are\", \"you\", \"_eos\", \"MASK\", \"am\", \"ok\", \"_eos\", \"i\"],\n    [\"how\", \"are\", \"you\", \"_eos\", \"i\", \"MASK\", \"ok\", \"_eos\", \"am\"],\n    [\"how\", \"are\", \"you\", \"_eos\", \"i\", \"am\", \"MASK\", \"_eos\", \"ok\"],\n    [\"how\", \"are\", \"you\", \"_eos\", \"i\", \"am\", \"ok\", \"MASK\", \"_eos\"],\n]\n\ncontext_color = \"#D3D3D3\"\nmask_color = \"#FF6347\"\ntrue_value_color = \"#98FB98\"\ntext_color = \"black\"\nscore_text = \"MLM Score: -1.02\"\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(10, 4))\nax.axis('off')\n\n# Function to draw rectangles\ndef draw_rect(ax, xy, width, height, color):\n    rect = plt.Rectangle(xy, width, height, color=color, ec=\"black\")\n    ax.add_patch(rect)\n\n# Initialize y position (top down)\ny = 1\nrect_height = 0.2\nrect_width = 0.2\npadding = 0.05\n\n# Loop over sentences to create the boxes and text\nfor sentence in sentences:\n    x = 0  # Initialize x position (left to right)\n    for word in sentence:\n        box_color = true_value_color if word == \"MASK\" or word.startswith('_') else context_color\n        draw_rect(ax, (x, y), rect_width, rect_height, box_color)\n        ax.text(x + rect_width/2, y + rect_height/2, word, ha='center', va='center', color=text_color, fontsize=9)\n        x += rect_width + padding  # Update x position\n\n    y -= rect_height + padding  # Update y position\n\n# Add the MLM score text\nax.text(0, y - rect_height/2 - padding, score_text, ha='left', va='center', color=text_color, fontsize=9)\n\n# Tight layout and save the figure\nplt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\nplt.savefig(\"mlm_visualization.png\", dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00456/mlm_score_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00812/screenshot_call.png",
        "caption": "A speech sequence from our phone call dataset. Two audio segments are highlighted: a question (in blue) and a reported symptom (in yellow).",
        "source": "acl_20/2005.00812/acl2020.tex",
        "arxiv_id": "acl_20/2005.00812",
        "llava_output": "The image is a line chart that presents the speech sequence from a phone call dataset. Two audio segments are highlighted: a question (in blue) and a reported symptom (in yellow). The chart shows the audio waveform for both segments, with the question in blue and the symptom in yellow.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate a random audio signal for demonstration purposes\nnp.random.seed(0)\naudio_data = np.random.normal(0, 1, 44100)  # assuming a 1-second clip at a 44100Hz sample rate\n\n# Simulate a time vector for the audio data\ntime_vector = np.linspace(0, 1, len(audio_data), endpoint=False)\n\n# Plotting the audio signal\nplt.figure(figsize=(14, 3))\nplt.plot(time_vector, audio_data, color='grey')\n\n# Highlighting a question segment in blue\n# Assuming question is between 0.3s and 0.5s\nplt.axvspan(0.3, 0.5, color='blue', alpha=0.3)\nplt.text(0.4, 1, 'QUESTION 1', horizontalalignment='center', color='blue', weight='bold')\n\n# Highlighting a reported symptom segment in yellow\n# Assuming symptom is between 0.6s and 0.8s\nplt.axvspan(0.6, 0.8, color='yellow', alpha=0.5)\nplt.text(0.7, 1, 'SYMPTOM 1', horizontalalignment='center', color='darkorange', weight='bold')\n\n# Adjusting the y-axis to mimic the original plot's symmetry around the horizontal line\nplt.ylim(-3, 3)\n\n# Adding the time axis ticks and labels\nplt.xticks(np.arange(0, 1.1, 0.05), [f'{t:.3f}' for t in np.arange(0, 1.1, 0.05)])\nplt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x * 60:02.0f}:{(x * 60 % 1) * 1000:03.0f}'))\nplt.ylabel('Amplitude')\n\n# Adding grid lines for better readability\nplt.grid(True)\n\n# Removing top and right border lines\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Saving the plot as a PNG file with 300dpi resolution\nplt.savefig('audio_plot.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00812/screenshot_call_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00912/Fig2-citn-overall-ptype-cut.png",
        "caption": "Citation box plots for papers published 1965--2016: overall and by type.",
        "source": "acl_20/2005.00912/ACL2020-Citations_in_NLP.tex",
        "arxiv_id": "acl_20/2005.00912",
        "llava_output": "The image is a box plot, which is used to present the results of an experiment.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data, replace with your actual data\ndata_all = np.random.normal(loc=50, scale=10, size=200)\ndata_journal = np.random.normal(loc=40, scale=15, size=200)\ndata_top_tier = np.random.normal(loc=60, scale=20, size=200)\ndata_conference = np.random.normal(loc=30, scale=10, size=200)\ndata_workshop = np.random.normal(loc=20, scale=5, size=200)\ndata_shared_task = np.random.normal(loc=70, scale=25, size=200)\ndata_system = np.random.normal(loc=45, scale=15, size=200)\ndata_demonstration = np.random.normal(loc=25, scale=7, size=200)\ndata_tutorial = np.random.normal(loc=55, scale=10, size=200)\n\ndata = [data_all, data_journal, data_top_tier, data_conference, data_workshop,\n        data_shared_task, data_system, data_demonstration, data_tutorial]\nlabels = ['ALL', 'Journal', 'Top-tier', 'Conference', 'Workshop', \n          'Shared task', 'System', 'Demonstration', 'Tutorial']\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Create boxplots\nbp = ax.boxplot(data, labels=labels, patch_artist=True)\n\n# Customizing box colors\ncolors = ['#FFFF99', '#FFCC99', '#99CCFF', '#FF9999', '#CCCCCC',\n          '#99CC99', '#FFCCFF', '#CCFFFF', '#FFCC66']\n\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n\n# Adding the data points to the boxplots\nfor i in range(len(data)):\n    y = data[i]\n    x = np.random.normal(1+i, 0.04, size=len(y))\n    ax.plot(x, y, 'r.', alpha=0.2)\n\n# Saving the figure\nplt.savefig('boxplot_figure.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00912/Fig2-citn-overall-ptype-cut_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00912/Fig4-citn-time-cut.png",
        "caption": "Citation box plots for journal articles and top-tier conference papers from various time spans.",
        "source": "acl_20/2005.00912/ACL2020-Citations_in_NLP.tex",
        "arxiv_id": "acl_20/2005.00912",
        "llava_output": "The image is a box plot showing the number of citations for journal articles and top-tier conference papers from various time spans.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data for demonstration purposes\ndata_all = np.random.normal(50, 15, 200)\ndata_65_89 = np.random.normal(50, 15, 200)\ndata_90_99 = np.random.normal(60, 20, 200)\ndata_00_09 = np.random.normal(70, 25, 200)\ndata_10_16 = np.random.normal(80, 30, 200)\n\ndata = [data_all, data_65_89, data_90_99, data_00_09, data_10_16]\npositions = [1, 2, 3, 4, 5]\nlabels = ['ALL', '1965-89', '1990-99', '2000-09', '2010-16']\n\nfig, ax = plt.subplots()\nbplot = ax.boxplot(data, positions=positions, patch_artist=True, notch=True)\n\ncolors = ['#ffeead', '#ffcc5c', '#ff6f69', '#ffeead', '#96ceb4']\nfor patch, color in zip(bplot['boxes'], colors):\n    patch.set_facecolor(color)\n\n# Customizing the plot to resemble the provided image\nax.yaxis.grid(True)\nax.set_xticks(positions)\nax.set_xticklabels(labels)\nax.set_ylabel('#citations')\n\n# Adding the sample median values on top of the boxplots as text\nfor i in range(len(data)):\n    med_val = np.median(data[i])\n    plt.text(positions[i], med_val + 2, str(med_val), ha='center', va='bottom', color='green', fontsize=9, style='italic')\n\n# Save the figure with 300 DPI\nplt.savefig('boxplot_example.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00912/Fig4-citn-time-cut_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00912/Fig6-citn-long-short.png",
        "caption": "Citations box plots for long and short ACL papers published between 2003 and 2016.",
        "source": "acl_20/2005.00912/ACL2020-Citations_in_NLP.tex",
        "arxiv_id": "acl_20/2005.00912",
        "llava_output": "The image is a box plot showing the number of citations for long and short papers published between 2003 and 2016.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data to illustrate how you might create the plot\nnp.random.seed(10)\nlong_papers = np.random.normal(60, 20, 200)\nshort_papers = np.random.normal(20, 5, 200)\n\ndata = [long_papers, short_papers]\n\n# Creating a new figure and boxplot\nplt.figure(figsize=(10, 8))\nbox = plt.boxplot(data, patch_artist=True, medianprops=dict(color='green', linestyle='--'), meanline=True, showmeans=True, notch=False)\n\ncolors = ['#FFD580', '#FFA07A']  # Fill colors for the boxes\n\n# Coloring the boxes\nfor patch, color in zip(box['boxes'], colors):\n    patch.set_facecolor(color)\n\n# Adding annotations for median and mean values\nmedians = [np.median(long_papers), np.median(short_papers)]\nmeans = [np.mean(long_papers), np.mean(short_papers)]\n\nfor i, (median, mean) in enumerate(zip(medians, means)):\n    plt.text(i+1, median+0.5, f'{median}', horizontalalignment='center', color='black')\n    plt.text(i+1, mean-1.5, f'{mean:.2f}', horizontalalignment='center', color='black', style='italic')\n\n# Adding labels\nplt.xticks([1, 2], ['long papers', 'short papers'])\nplt.ylabel('#citations')\n\n# Removing top and right spines\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('boxplot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00912/Fig6-citn-long-short_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00912/Fig7-stream-cut.png",
        "caption": "Stream graph of \\#papers by \\#citations. The contribution of each venue and paper type is stacked one on top of another.",
        "source": "acl_20/2005.00912/ACL2020-Citations_in_NLP.tex",
        "arxiv_id": "acl_20/2005.00912",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes. Based on the Caption \"Stream graph of #papers by #citations\" and the Histogram image, it is a data visualization plot that presents experimental results.",
        "code": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Example data structure (you will have to replace this with your actual data)\n# Assuming 'citations' is a list of x-values\n# and 'venues' is a dictionary where keys are venues and values are lists of y-values (number of papers)\n\ncitations = np.linspace(0, 35, 100) # For example purposes, creating an array of citation counts\nvenues = {\n    'ACL': np.random.randint(300, 1200, size=100).cumsum()[::-1],\n    'CL': np.random.randint(200, 800, size=100).cumsum()[::-1],\n    'COLING': np.random.randint(100, 600, size=100).cumsum()[::-1],\n    # ... include all venues with appropriate data\n}\n\n# Replace the above code with your actual data loading and preprocessing\n\n# Reorder dictionary based on sum of papers to maintain layout similar to the provided chart\nvenues = dict(sorted(venues.items(), key=lambda x: -sum(x[1])))\n\n# Plotting the stream graph\nplt.stackplot(citations, venues.values(), labels=venues.keys(), alpha=0.8)\n\n# Customize the plot\nplt.title('Stream graph of #papers by #citations')\nplt.xlabel('#citations')\nplt.ylabel('#papers')\nplt.legend(loc='upper right', title='Venue')\nplt.margins(0, 0)  # You might want to adjust margins to match the style of the original plot\n\n# Save the plot with 300 DPI\nplt.savefig('stream_graph.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00912/Fig7-stream-cut_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2005.00912/Fig8.png",
        "caption": "The percentage of AA$'$ papers in various citation bins. In parenthesis: \\#papers.",
        "source": "acl_20/2005.00912/ACL2020-Citations_in_NLP.tex",
        "arxiv_id": "acl_20/2005.00912",
        "llava_output": "0.01</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the image\ncitation_bins = ['0', '1-9', '10-99', '100-999', '1000-9999']\npercentages = [6.4, 37.6, 47.5, 8.1, 0.4]\nnumber_of_papers = [1729, 10123, 12798, 2195, 104]\ncolors = ['#7f7f7f', '#8cd17d', '#59981a', '#ff9f43', '#d55e00']\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Bar plot\nbars = ax.bar(citation_bins, percentages, color=colors)\n\n# Add the data labels on the bars\nfor bar, percentage, number in zip(bars, percentages, number_of_papers):\n    height = bar.get_height()\n    label_x_pos = bar.get_x() + bar.get_width() / 2\n    label_y_pos = height\n    ax.text(label_x_pos, label_y_pos, f'{percentage}%', ha='center', va='bottom')\n    ax.text(label_x_pos, 0, f'({number:,})', ha='center', va='bottom', color='black', fontsize=8)\n\n# Add a star mark to the top left\nax.text(-0.5, ax.get_ylim()[1], '\u2605', ha='right', va='bottom', color='gray', fontsize=16)\n\n# Set the y-axis label\nax.set_ylabel('% of papers')\n\n# Set the y-axis range to 0-50% with ticks every 10%\nax.set_ylim(0, 50)\nax.set_yticks(range(0, 51, 10))\nax.set_yticklabels([f'{y}%' for y in range(0, 51, 10)])\n\n# Remove x-ticks\nax.tick_params(axis='x', which='both', length=0)\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure as a PNG file\nplt.savefig('citation_bins.png', dpi=300)\n\n# Show the plot for interactive view\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.00912/Fig8_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01096/n_segment.png",
        "caption": "\\small Average expected number of segments with varying hyperparameters. x-axis is the encoder/decoder hidden size and y-axis is the word embedding size. Upper two figures are without the granularity regularization and the bottom two are with regularization.",
        "source": "acl_20/2005.01096/acl2020.tex",
        "arxiv_id": "acl_20/2005.01096",
        "llava_output": "The image is a scatter plot showing the average expected number of segments with varying hyperparameters. The x-axis represents the encoder/decoder hidden size, and the y-axis represents the word embedding size. The plot is divided into two sections, with the upper two figures showing results without granularity regularization",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create data (Note: replace the values below with the appropriate data from the paper)\ne2e_no_reg = np.array([\n    [7.2, 6.8, 6.1, 5.5],\n    [7.3, 6.5, 6.8, 6.1],\n    [10.1, 9.9, 8.4, 7.0],\n    [10.8, 10.4, 8.9, 7.3]\n])\n\ne2e_with_reg = np.array([\n    [6.4, 6.4, 6.3, 6.2],\n    [6.3, 6.5, 6.4, 6.2],\n    [6.2, 6.3, 6.4, 6.3],\n    [6.6, 6.4, 6.2, 6.5]\n])\n\nwebnlg_no_reg = np.array([\n    [3.6, 3.5, 2.9, 2.8],\n    [3.8, 3.6, 3.3, 2.9],\n    [4.0, 3.7, 3.4, 3.1],\n    [4.4, 3.9, 3.3, 3.2]\n])\n\nwebnlg_with_reg = np.array([\n    [3.3, 3.3, 3.1, 3.1],\n    [3.4, 3.2, 3.2, 3.2],\n    [3.3, 3.3, 3.2, 3.1],\n    [3.4, 3.2, 3.1, 3.2]\n])\n\n# Set up the figure and axes\nfig, axs = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey='row')\n\n# Titles for the subplots\ntitles = ['E2E', 'WebNLG', 'E2E', 'WebNLG']\nfor ax, title in zip(axs.flat, titles):\n    ax.set_title(title)\n\n# Embedding sizes and hidden sizes labels\nembedding_sizes = ['64', '128', '256', '512']\nhidden_sizes = ['50', '100', '200', '300']\n\n# Plot each matrix\ncmaps = ['Greens', 'Greens']\ndata_matrices = [e2e_no_reg, webnlg_no_reg, e2e_with_reg, webnlg_with_reg]\n\nfor ax, data, cmap in zip(axs.flat, data_matrices, cmaps * 2):\n    cax = ax.matshow(data, cmap=cmap)\n    # Annotate each cell with the numeric value\n    for (i, j), val in np.ndenumerate(data):\n        ax.text(j, i, f'{val:.1f}', ha='center', va='center', color='black')\n\n# Adjustments to layout, labels and colorbar\nfor ax in axs[:, 0]:\n    ax.set_ylabel('Encoder/Decoder Hidden Size')\nfor ax in axs[1, :]:\n    ax.set_xlabel('Word Embedding Size')\n\n# Set tick labels\nfor ax in axs.flat:\n    ax.set_xticklabels([''] + embedding_sizes)\n    ax.set_yticklabels([''] + hidden_sizes)\n\n# Colorbar setup\nfig.colorbar(cax, ax=axs, orientation='vertical', fraction=.1)\n\n# Adjust layout and save as PNG file with 300dpi.\nplt.tight_layout()\nplt.savefig('data_visualization.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01096/n_segment_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01151/fonts.jpg",
        "caption": "Examples from our collected dataset visualized through fonts with the highest annotation agreements.",
        "source": "acl_20/2005.01151/acl2020.tex",
        "arxiv_id": "acl_20/2005.01151",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Define text content and font properties for each text\ntexts = [\n    (\"Keep Calm\\nand\\nVisit London\", {'fontsize': 14, 'fontweight': 'normal'}),\n    (\"25% OFF EVERYTHING ONLINE\\nAND IN-STORE\", {'fontsize': 14, 'fontweight': 'bold'}),\n    (\"Donut love\", {'fontsize': 24, 'fontweight': 'normal'}),\n    (\"Enjoy the Last Bit of\\nSummer!\", {'fontsize': 14, 'fontweight': 'normal'}),\n    (\"Money\\nManagement\", {'fontsize': 14, 'fontweight': 'normal'}),\n    (\"The Ultimate Carrot\\nCake\", {'fontsize': 14, 'fontweight': 'normal'}),\n    (\"EMPIRE STATE\\nOF MIND\", {'fontsize': 18, 'fontweight': 'bold'}),\n]\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.axis('off')\n\n# You must adjust these positions according to your layout.\npositions = [\n    (0.1, 0.8), (0.1, 0.6), (0.1, 0.3),\n    (0.7, 0.8), (0.7, 0.6), (0.7, 0.3),\n    (0.4, 0.1)\n]\n\nfor (text, props), (x, y) in zip(texts, positions):\n    ax.text(x, y, text, ha='center', va='center', **props)\n\n# Save the figure\nplt.savefig('text_visualization.png', dpi=300, bbox_inches='tight')\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01151/fonts_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01151/fontChart3.png",
        "caption": "Label distributions for three examples",
        "source": "acl_20/2005.01151/acl2020.tex",
        "arxiv_id": "acl_20/2005.01151",
        "llava_output": "1. Bar Chart: The image shows a bar chart with different colored bars, representing the number of followers for each blog. The chart is titled \"Small Business Blogs\" and displays the number of followers for each blog.\n2. Line Chart: The image features a line chart that shows the number",
        "is_data_visual": "Yes (Line Chart).",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data (replace with your actual data)\nlabels = ['27 Most Influential Small Business Blogs', 'My first surf day', 'Love you Mom']\nF0 = [0.2, 0.1, 0.2]\nF1 = [0.15, 0.05, 0]\nF2 = [0.1, 0.15, 0.05]\nF3 = [0.05, 0.1, 0.1]\nF4 = [0.1, 0.1, 0.05]\nF5 = [0.05, 0.05, 0.1]\nF6 = [0, 0.1, 0.15]\nF7 = [0, 0.05, 0.1]\nF8 = [0, 0, 0.15]\nF9 = [0, 0.05, 0.2]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.1  # the width of the bars\n\nfig, ax = plt.subplots()\n\n# Creating bars for each category\nax.bar(x - 4*width, F0, width, label='F0')\nax.bar(x - 3*width, F1, width, label='F1')\nax.bar(x - 2*width, F2, width, label='F2')\nax.bar(x - width, F3, width, label='F3')\nax.bar(x, F4, width, label='F4')\nax.bar(x + width, F5, width, label='F5')\nax.bar(x + 2*width, F6, width, label='F6')\nax.bar(x + 3*width, F7, width, label='F7')\nax.bar(x + 4*width, F8, width, label='F8')\nax.bar(x + 5*width, F9, width, label='F9')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Scores')\nax.set_title('Label distributions for three examples')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n# Rotate the tick labels and set their alignment\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Set the y-axis range if necessary to match your data; for example:\nax.set_ylim(0, 0.3)\n\n# Tight layout to prevent label cut-offs\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('label_distributions.png', dpi=300)\n\n# Optionally, display the figure\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01151/fontChart3_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01151/disChart3.png",
        "caption": "Average label distribution of the entire corpus",
        "source": "acl_20/2005.01151/acl2020.tex",
        "arxiv_id": "acl_20/2005.01151",
        "llava_output": "The image is a histogram showing the distribution of the frequency of the label \"F\" in the corpus.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Made-up values for demonstration purposes; replace these with the actual values.\nvalues = [0.18, 0.15, 0.10, 0.13, 0.11, 0.09, 0.12, 0.08, 0.17, 0.14]\nlabels = ['F0', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9']\n\n# Create a horizontal bar chart\nplt.barh(labels, values, color='blue')\n\n# Set the x-axis label\nplt.xlabel('Average label distribution')\n\n# Set the figure size if needed\nplt.figure(figsize=(10, 8))\n\n# Save the figure with 300 dpi\nplt.savefig('average_label_distribution.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01151/disChart3_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01306/economy_example.png",
        "caption": "Economy comparison: Recall vs number of patterns, for the different representations.",
        "source": "acl_20/2005.01306/pyBART.tex",
        "arxiv_id": "acl_20/2005.01306",
        "llava_output": "The image is a line chart showing the relationship between the number of patterns needed and the recall rate.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assume some sample data for illustration purposes\npatterns = np.linspace(0, 140, 100)  # Generate 100 points from 0 to 140\nrecall_basic = 1 - np.exp(-patterns / 50)\nrecall_enhanced = 1 - np.exp(-patterns / 60)\nrecall_our = 1 - np.exp(-patterns / 70)\n\n# Plot the continuous lines for the three series\nplt.plot(patterns, recall_basic, label='Basic Dependencies', color='blue')\nplt.plot(patterns, recall_enhanced, label='Enhanced Dependencies', color='green')\nplt.plot(patterns, recall_our, label='Our Representation', color='orange')\n\n# Add the horizontal dashed line for the economy recall\neconomy_recall = 0.307\nplt.axhline(y=economy_recall, color='red', linestyle='--', label='Economy (e.g., ~30.7% Recall)')\n\n# Annotating specific patterns for #52, #77, #112\nfor pattern_count, style in zip([52, 77, 112], ['-', '-', '-']):\n    recall_value = 1 - np.exp(-pattern_count / 70)\n    plt.annotate(f'#{pattern_count}', (pattern_count, recall_value),\n                 textcoords=\"offset points\", xytext=(0,10),\n                 ha='center', color='black')\n\n# Add other plot decorations\nplt.xlabel('# Of Patterns Needed')\nplt.ylabel('Recall')\nplt.title('Economy comparison: Recall vs number of patterns,\\nfor the different representations')\nplt.legend(loc='lower right')\n\n# Show the grid\nplt.grid(True)\n\n# Save the figure with 300dpi\nplt.savefig('economy_comparison_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01306/economy_example_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01348/attn.png",
        "caption": "Differences between attention weights for correct and incorrect referents; clusters are visible in the upper-right hand corner. Note the limited range expressed by the colourbar.",
        "source": "acl_20/2005.01348/attention.tex",
        "arxiv_id": "acl_20/2005.01348",
        "llava_output": "The image is a scatter plot showing the differences between attention weights for correct and incorrect referents. The x-axis represents the attention weights for incorrect referents, while the y-axis represents the attention weights for correct referents. The plot shows that the attention weights for correct referents are higher than those for incorrect refer",
        "is_data_visual": "Yes. The image is a scatter plot representing the experimental results of a sentiment analysis task, with the x-axis representing the attention weights for incorrect referents and the y-axis representing the attention weights for correct referents. The plot shows clusters visible in the upper-right hand corner, and the colorbar indicates the limited range of values represented by the attention weights.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Since I don't have the exact data from your image, I'll use placeholder random data.\n# You should replace this with your actual data.\ndata = [np.random.rand(10, 10) for _ in range(6)]\n\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(8, 6))\n\n# Labels for each subplot\nlabels = ['ORIG', 'VC', 'TEN', 'NUM', 'GEN', 'RC', 'SYN/NA', 'ADV']\nlabel_index = 0\n\nfor i, ax in enumerate(axs.flat):\n    im = ax.imshow(data[i], cmap='Greys', vmin=0, vmax=0.1)\n    ax.set_title(labels[label_index])\n    ax.axis('off')  # Hide the axis\n    label_index += 1\n\n# Add space for the colorbar\nfig.subplots_adjust(right=0.8)\n\n# Add the colorbar\ncbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nfig.colorbar(im, cax=cbar_ax)\n\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('data_visualization.png', dpi=300, bbox_inches='tight')\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01348/attn_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.01619/avg_kp_coverage.png",
        "caption": "Argument coverage per number of key points.",
        "source": "acl_20/2005.01619/ACL 2020 key points camera ready clean margin fix/analysis.tex",
        "arxiv_id": "acl_20/2005.01619",
        "llava_output": "1. Bar Chart</s>",
        "is_data_visual": "Based on the information provided, the image does appear to be a Bar Chart representing the argument coverage per number of key points.",
        "code": "import matplotlib.pyplot as plt\n\n# Data\nkey_points = [1, 2, 3, 4, 5, 6, 7]\ncoverage_percentages = [41.3, 56.3, 64.2, 69.3, 71.5, 72.3, 72.5]\n\n# Creating the figure and the bar chart\nplt.figure(figsize=(10, 5))\nbars = plt.bar(key_points, coverage_percentages, color='blue')\n\n# Adding the percentage labels above the bars\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, f\"{yval:.1f}%\", ha='center', va='bottom')\n\n# Setting the labels and title (if any)\nplt.xlabel('Number of Key Points')\nplt.ylabel('Argument Coverage (%)')\n# plt.title('Argument Coverage per Number of Key Points') # Uncomment this line if you want a title\n\n# Limit the y-axis as in the image\nplt.ylim(0, max(coverage_percentages) * 1.1)  # A bit more than the max value for aesthetic\n\n# Saving the figure in PNG format with 300 DPI\nplt.savefig('argument_coverage_per_key_points.png', dpi=300)\n\n# Showing the plot, if necessary (comment out or remove the next line when running as a script)\n# plt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.01619/avg_kp_coverage_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.03312/nakdan-makor-chopped.png",
        "caption": "Integrated Biblical quote marked with font.",
        "source": "acl_20/2005.03312/acl2020.tex",
        "arxiv_id": "acl_20/2005.03312",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Your data or text here; replace with your actual content\ntext = \"Your text goes here\"\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Hide the axes\nax.axis('off')\n\n# Set title or add text\nplt.text(0.5, 0.5, text, fontsize=12, ha='center', va='center')\n\n# Save the figure\nplt.savefig('plot.png', dpi=300, bbox_inches='tight', pad_inches=0)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.03312/nakdan-makor-chopped_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.03593/two_perplexities_f.png",
        "caption": "Relationship between log frequency bands used to replace words in synthetic Cookie Theft picture descriptions to simulate degrees of semantic dementia and perplexity of LSTM language models trained on picture descriptions by controls and dementia patients.",
        "source": "acl_20/2005.03593/acl2020.tex",
        "arxiv_id": "acl_20/2005.03593",
        "llava_output": "The image is a line chart that shows the relationship between log frequency bands used to replace words in synthetic cookie theft picture descriptions to simulate degrees of semantic dementia and perplexity of LSTM language models trained on picture descriptions by controls and dementia patients.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data (Replace these with your actual data)\nlog_frequency_bands = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\ncontrol_model_perplexity = [100, 120, 140, 160, 180, 200, 400]\ndementia_model_perplexity = [80, 100, 120, 150, 200, 250, 350]\n\n# Plotting the data\nplt.figure()\nplt.plot(log_frequency_bands, control_model_perplexity, 'o-', label='Control model')\nplt.plot(log_frequency_bands, dementia_model_perplexity, 'o--', label='Dementia model')\n\n# Adding title and labels\nplt.title('Relationship between log frequency bands and perplexity of LSTM language models')\nplt.xlabel('Log frequency bands')\nplt.ylabel('LSTM model perplexity')\n\n# Adding a legend\nplt.legend()\n\n# Saving the plot as a PNG file with 300dpi resolution\nplt.savefig('plot.png', dpi=300)\n\n# Showing the plot if you want to view it interactively (you can remove this line if not needed)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.03593/two_perplexities_f_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.03593/f_interpolation_bird_basic_bw_alpha.png",
        "caption": "Pretrained word embeddings. Elevation in perplexity over unperturbed transcript ($Po$) with the proportional contribution of a dementia model ($\\alpha$) to an interpolated model. Each point is the average of 268 data points, and error bars are not shown as they do not exceed the bounds of the markers.",
        "source": "acl_20/2005.03593/acl2020.tex",
        "arxiv_id": "acl_20/2005.03593",
        "llava_output": "The image is a line chart showing the elevation in perplexity over unperturbed transcript ($Po$) with the proportional contribution of a dementia model ($\\alpha$) to an interpolated model. Each point is the average of 268 data points, and error bars are not shown as they do",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Substitute these lists with the actual data from the academic paper.\n# You should have one x_values list and four y_values lists for the four different lines.\nx_values = [0.00, 0.25, 0.50, 0.75, 1.00]\ny_values_3 = [200, 150, 100, 80, 60]  # Example data for line 1\ny_values_2_5 = [150, 100, 80, 60, 50]  # Example data for line 2\ny_values_2 = [100, 70, 60, 50, 45]    # Example data for line 3\ny_values_1_5 = [80, 60, 45, 40, 35]   # Example data for line 4\ny_values_1 = [60, 40, 35, 30, 25]     # Example data for line 5\n\n# Plotting the lines with different markers\nplt.plot(x_values, y_values_3, marker='o', label='3')\nplt.plot(x_values, y_values_2_5, marker='^', label='2.5')\nplt.plot(x_values, y_values_2, marker='s', label='2')\nplt.plot(x_values, y_values_1_5, marker='p', label='1.5')\nplt.plot(x_values, y_values_1, marker='*', label='1')\n\n# Adding legend, which in the image is placed at the top right corner\nplt.legend(loc='upper right')\n\n# Setting the X and Y axis labels as per the caption provided\nplt.xlabel('\u03b1')\nplt.ylabel('Po - Pp')\n\n# Set y axis limit if needed (the provided example does not limit y axis)\n# plt.ylim(0, 250)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('pretrained_word_embeddings.png', dpi=300)\n\n# Show the plot for verification\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.03593/f_interpolation_bird_basic_bw_alpha_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.03624/modgen.png",
        "caption": "\\small Our model (best seen in color). The blue dotted line encompasses the classifier. The red dotted line encompasses the generator. The orange layer in the model helps combine the outputs from the variational model and the original classifier.",
        "source": "acl_20/2005.03624/acl2020.tex",
        "arxiv_id": "acl_20/2005.03624",
        "llava_output": "1. The image is a diagram showing the structure of a LSTM classifier.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a new figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Draw boxes representing different layers\nax.add_patch(patches.Rectangle((0.1, 0.7), 0.8, 0.1, facecolor='orange', label='Generator'))  # Generator box\nax.add_patch(patches.Rectangle((0.1, 0.85), 0.8, 0.1, facecolor='lightblue', label='Classifier'))  # Classifier box\n\n# Draw lines and arrows within the boxes\n# Simplified representation of layers and connections\nax.arrow(0.2, 0.75, 0.6, 0, head_width=0.02, head_length=0.02, fc='k', ec='k')\n\n# Add dashed lines for the LSTM Classifier\nax.plot([0.1, 0.9], [0.9, 0.9], 'b--')\nax.plot([0.1, 0.1], [0.7, 0.9], 'b--')\nax.plot([0.9, 0.9], [0.7, 0.9], 'b--')\n\n# Add dashed lines for the Variational Generator\nax.plot([0.1, 0.9], [0.7, 0.7], 'r--')\nax.plot([0.1, 0.1], [0.5, 0.7], 'r--')\nax.plot([0.9, 0.9], [0.5, 0.7], 'r--')\n\n# Add the names of the layers\nax.text(0.1, 0.96, 'LSTM Classifier', fontsize=12, color='blue')\nax.text(0.1, 0.66, 'Variational Generator', fontsize=12, color='red')\n\n# Add the connecting lines for an attention layer\narrows_x = [0.2 + 0.1*i for i in range(7)]\nfor arrow_x in arrows_x:\n    ax.arrow(arrow_x, 0.9, 0, -0.05, head_width=0.02, head_length=0.02, fc='k', ec='k')\n\n# Add labels for layers\nax.text(0.5, 0.8, 'Attention layer', fontsize=10, va='center', ha='center')\nax.text(0.5, 0.95, 'Classification layer', fontsize=10, va='center', ha='center')\n\n# Turn off axis\nax.axis('off')\n\n# Save the figure\nplt.savefig('model_visualization.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.03624/modgen_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.04511/multilingual-all.png",
        "caption": "t-SNE visualization of dependency head-dependent pairs projected into the cross-lingual syntactic subspace of Multilingual BERT\\@. Colors correspond to gold UD dependency type labels, which are unlabeled given that there are 43 in this visualization.",
        "source": "acl_20/2005.04511/main.tex",
        "arxiv_id": "acl_20/2005.04511",
        "llava_output": "The image is a scatter plot that shows the relationship between two variables. The scatter plot is colorful, with each point representing a different data point. The colors of the points correspond to the different types of dependency head-dependent pairs. The plot is visually appealing and provides a clear representation of the data.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n# This is a placeholder for the actual high-dimensional data you want to visualize\n# Replace `high_dimensional_data` with your own data.\nhigh_dimensional_data = np.random.rand(1000, 100)  # 1000 samples of 100-dimensions each\n\n# Replace `labels` with the actual labels for each data point.\n# There should be 43 unique labels if you have 43 different types.\nlabels = np.random.choice(43, 1000)\n\n# Perform t-SNE embedding\ntsne = TSNE(n_components=2, random_state=42)\ntransformed_data = tsne.fit_transform(high_dimensional_data)\n\n# Plotting the t-SNE data points\nplt.figure(figsize=(12, 8))\n\n# Usually we use different colors for different labels. We can generate a colormap for this.\n# Replace this with the colormap or list of colors actually corresponding to your labels\nunique_labels = np.unique(labels)\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n\nfor i, label in enumerate(unique_labels):\n    indices = labels == label\n    plt.scatter(transformed_data[indices, 0], transformed_data[indices, 1],\n                c=[colors[i]], label=str(label), alpha=0.5)\n\n# Hiding the axis labels since the t-SNE components don't represent anything physical\nplt.axis('off')\n\n# Saving the figure\nplt.savefig('tsne_visualization.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.04511/multilingual-all_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.04511/multilingual-all.png",
        "caption": "tSNE visualization of head-dependent pairs dependencies projected into the cross-lingual syntactic subspace of Multilingual BERT\\@. Colors correspond to gold UD dependency type labels. Although neither mBERT nor our probe is ever trained on UD dependency labels, the clusters in the learned representation substantially capture human analyses of dependency types.",
        "source": "acl_20/2005.04511/main_appendix.tex",
        "arxiv_id": "acl_20/2005.04511",
        "llava_output": "0</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\n# Hypothetical example dataset\n# Replace these values with your actual data\n# points would be a 2D numpy array of the t-SNE components\npoints = np.random.randn(1000, 2)\n# labels would be a 1D array of dependency type labels corresponding to each point\nlabels = np.random.choice(['type1', 'type2', 'type3', 'type4'], size=1000)\n\n# Create a color map for the labels\nunique_labels = np.unique(labels)\ncolors = plt.cm.get_cmap('tab20', len(unique_labels))\n\nlabel_to_color = {\n    label: colors(i) for i, label in enumerate(unique_labels)\n}\n\n# Plot the t-SNE points\nplt.figure(figsize=(12, 10))\nfor label in unique_labels:\n    indices = labels == label\n    plt.scatter(points[indices, 0], points[indices, 1], c=[label_to_color[label]], label=label, alpha=0.5)\n\n# Optional: uncomment the following line to include a legend\n# plt.legend(markerscale=2)\n\n# Remove the axis for a cleaner look\nplt.axis('off')\n\n# Save the plot as a PNG file\nplt.savefig('tsne_plot.png', dpi=300, bbox_inches='tight')\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.04511/multilingual-all_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.04511/pca.png",
        "caption": "Syntactic difference vectors visualized after dimensionality reduction with PCA, instead of tSNE, colored by UD dependency types. There are no significant trends evident.",
        "source": "acl_20/2005.04511/main_appendix.tex",
        "arxiv_id": "acl_20/2005.04511",
        "llava_output": "The image is a scatter plot with different colored dots representing different types of dependency.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Random seed for reproducibility\nnp.random.seed(0)\n\n# Simulating data: 1000 data points and 9 categories (as seen in the legend)\nnum_categories = 9\nnum_points = 1000\n\n# Generating random data points for PCA components\nx = np.random.normal(size=num_points)\ny = np.random.normal(size=num_points)\n\n# Assigning random categories to the data\ncategories = np.random.choice(num_categories, num_points)\n\n# Define some colors and labels for the plot\ncolors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple']\nlabels = ['advmod', 'amod', 'case', 'cc', 'conj', 'det', 'mark', 'nmod', 'nsubj', 'obj', 'obl']\n\n# Create a scatter plot\nfig, ax = plt.subplots()\nfor i in range(num_categories):\n    ix = np.where(categories == i)\n    ax.scatter(x[ix], y[ix], c=colors[i], label=labels[i], alpha=0.7, edgecolors='none')\n\n# Legend\nax.legend()\n\n# Remove the top and right spines\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\n# Increase the plot size\nfig.set_size_inches(10, 10)\n\n# Save the figure with 300 DPI\nplt.savefig('syntactic_differences_pca.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.04511/pca_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.04511/multilingual-all.png",
        "caption": "t-SNE visualization of dependency head-dependent pairs projected into the cross-lingual syntactic subspace of Multilingual BERT\\@. Colors correspond to gold UD dependency type labels, which are unlabeled given that there are 43 in this visualization.",
        "source": "acl_20/2005.04511/Multilingual Probing ACL Submission/main.tex",
        "arxiv_id": "acl_20/2005.04511",
        "llava_output": "The image is a scatter plot, which is a type of plot used to display the relationship between two variables. In this case, the scatter plot shows the relationship between two variables, with the x-axis representing one variable and the y-axis representing the other variable. The scatter plot is colorful, indicating that the",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Your data needs to be in a format like this, with 'x' and 'y' being the t-SNE components,\n# and 'label' being the dependency type (assuming it's categorical data).\n# data = pd.read_csv('path_to_your_data.csv')\n\n# Generating random data for the demonstration since actual data is unavailable.\nnp.random.seed(0)\nnum_points = 1000\nnum_categories = 43\n\n# Creating a random 2D dataset with 43 different categories (for the 43 colors)\nx = np.random.randn(num_points, 2)\nlabels = np.random.randint(0, num_categories, num_points)\n\n# Now you would plot each category one by one to mimic the colors.\n# Assuming you have a list of colors equal in length to the number of categories.\ncolors = plt.cm.jet(np.linspace(0, 1, num_categories))\n\nplt.figure(figsize=(12, 10))\n\nfor i in range(num_categories):\n    # Obtain the indices for data points with the current label\n    indices = labels == i\n    plt.scatter(x[indices, 0], x[indices, 1], label=f'Label {i}', color=colors[i], alpha=0.7)\n\nplt.title('t-SNE visualization of dependency head-dependent pairs')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.legend(markerscale=2)  # You might want to hide the legend if there are too many labels.\nplt.axis('off')  # Turn off the axis, as they are not present in the image you provided.\nplt.tight_layout()\nplt.savefig('tsne_visualization.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.04511/multilingual-all_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.04511/multilingual-all.png",
        "caption": "tSNE visualization of head-dependent pairs dependencies projected into the cross-lingual syntactic subspace of Multilingual BERT\\@. Colors correspond to gold UD dependency type labels. Although neither mBERT nor our probe is ever trained on UD dependency labels, the clusters in the learned representation substantially capture human analyses of dependency types.",
        "source": "acl_20/2005.04511/Multilingual Probing ACL Submission/main_appendix.tex",
        "arxiv_id": "acl_20/2005.04511",
        "llava_output": "The image is a scatter plot that shows the relationships between different colored dots.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n# Let's create some random data as an example (replace this with your data)\n# Let's assume there are 10 clusters (dependency types) and 1000 data points\nnp.random.seed(42)\nnum_clusters = 10\nnum_points = 1000\nX = np.random.rand(num_points, 100)  # Replace 100 with the number of features in your actual data\nlabels = np.random.choice(num_clusters, num_points)  # Replace with your actual labels\n\n# Now we run t-SNE on the data\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\nX_tsne = tsne.fit_transform(X)\n\n# Plotting the result of t-SNE\nplt.figure(figsize=(12, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='hsv', alpha=0.5)\n\n# Let's create a legend with the gold UD dependency type labels\n# You will need a list of the actual dependency type labels instead of 'Type 1', 'Type 2', etc.\ndependency_types = ['Type ' + str(i+1) for i in range(num_clusters)]\nplt.legend(handles=scatter.legend_elements()[0], labels=dependency_types)\n\n# Remove axis\nplt.axis('off')\n\n# Save the plot as a PNG\nplt.savefig('tsne_plot.png', dpi=300, bbox_inches='tight')\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.04511/multilingual-all_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.04511/pca.png",
        "caption": "Syntactic difference vectors visualized after dimensionality reduction with PCA, instead of tSNE, colored by UD dependency types. There are no significant trends evident.",
        "source": "acl_20/2005.04511/Multilingual Probing ACL Submission/main_appendix.tex",
        "arxiv_id": "acl_20/2005.04511",
        "llava_output": "The image is a scatter plot, showing the syntactic difference vectors visualized after dimensionality reduction with PCA. The plot is colored by UD dependency types, and there are no significant trends evident.</s>",
        "is_data_visual": "Yes, which means the image represents a data visualization plot that is a scatter plot.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Sample data: Replace with your actual PCA-reduced data and labels.\nnp.random.seed(0)\nn_samples = 1000\nn_dependency_types = 8  # I see 8 different labels in the legend\n\n# Random 2D points to mimic PCA output\nX = np.random.normal(size=(n_samples, 2))\n\n# Random labels to mimic UD dependency types\nlabels = np.random.choice(range(n_dependency_types), n_samples)\n\n# Color map for the dependency types\ncolors = plt.cm.tab10(np.linspace(0, 1, n_dependency_types))\n\n# Create a DataFrame for easy plotting with seaborn\ndf = pd.DataFrame({'PCA1': X[:, 0], 'PCA2': X[:, 1], 'DependencyType': labels})\n\n# Start plotting\nplt.figure(figsize=(10, 10))\n\n# Plot each dependency type separately to control colors and labels\nfor i in range(n_dependency_types):\n    plt.scatter(df['PCA1'][df['DependencyType'] == i], \n                df['PCA2'][df['DependencyType'] == i], \n                label=f'Type {i}', \n                alpha=0.7,\n                color=colors[i])\n\n# Create legends with custom labels (Replace with actual dependency type names)\ndependency_types = ['advmod', 'amod', 'case', 'cc', 'conj', 'det', 'mark', 'nmod', 'nsubj', 'obj', 'obl']\nhandles, _ = plt.gca().get_legend_handles_labels()\nplt.legend(handles, dependency_types[:n_dependency_types], title=\"UD dependency types\", loc='upper right')\n\nplt.axis('equal')  # To keep the aspect ratio square\nplt.title('Syntactic difference vectors\\nvisualized after dimensionality reduction with PCA')\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\n\n# Save the figure with 300 dpi\nplt.savefig('pca_plot.png', dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.04511/pca_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.05189/statistic.png",
        "caption": "Evolution of evidence predictions on the development set of CoQA. From the inside to the outside, the four rings correspond to \\BERTHAtt (iteration 0) and \\BERTSelfCTM (iteration 1, 2, 3), respectively.",
        "source": "acl_20/2005.05189/acl2020.tex",
        "arxiv_id": "acl_20/2005.05189",
        "llava_output": "The image is a Bar Chart that presents the evolution of evidence predictions on the development set of CoQA. The chart shows the four rings, each corresponding to a different iteration of the model: \\BERTHAtt (iteration 0) and \\BERTSelfCTM (iteration 1, 2",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# The following data is dummy data for illustration purposes only\n# Replace this with your actual data when creating your plot\ndata = {\n    'ring0': [1, 2, 3, 4],\n    'ring1': [1, 2, 3, 4],\n    'ring2': [1, 2, 3, 4],\n    'ring3': [1, 2, 3, 4]\n}\n\n# Define the number of different rings you want to plot\nnum_rings = len(data)\n\ntheta = np.linspace(0, 2 * np.pi, len(data['ring0']) + 1)\nwidth = (2 * np.pi) / len(data['ring0'])\n\n# Create a subplot with polar projection\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n\n# Plot each of the rings\nfor i, ring_key in enumerate(data.keys()):\n    radii = np.array(data[ring_key])\n    bars = ax.bar(theta[:-1], radii, width=width, bottom=10 * i, edgecolor='white')\n\n# Set the aspect ratio to be equal, so the plot is circular\nax.set_aspect(\"equal\")\n\n# Custom decorations, like labels and ticks, can be added here.\n\n# Save the plot to a file\nplt.savefig('multi_ring_plot.png', dpi=300)\n\n# Show the plot if desired\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.05189/statistic_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.05189/appendix-case.png",
        "caption": "Weight distribution of the two cases from the sentence-level attention.",
        "source": "acl_20/2005.05189/acl2020.tex",
        "arxiv_id": "acl_20/2005.05189",
        "llava_output": "1. The image is a line chart showing the weight distribution of the two cases from the sentence-level attention.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data (Replace with actual data)\n# Two 2D arrays representing the weight distributions for each case\nweights_case_1 = np.array([\n    [0.1, 0.2, 0.5, 0.1, 0.1],\n    [0.1, 0.3, 0.4, 0.1, 0.1],\n    [0.2, 0.2, 0.3, 0.2, 0.1],\n    [0.1, 0.4, 0.3, 0.1, 0.1],\n])\n\nweights_case_2 = np.array([\n    [0.1, 0.2, 0.4, 0.2, 0.1],\n    [0.1, 0.5, 0.2, 0.1, 0.1],\n    [0.1, 0.3, 0.3, 0.2, 0.1],\n    [0.2, 0.4, 0.1, 0.1, 0.2],\n])\n\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 6))\n\n# Create the heatmaps\nsns.heatmap(weights_case_1, ax=axs[0], cmap=\"Greens\", cbar=False)\nsns.heatmap(weights_case_2, ax=axs[1], cmap=\"Greens\", cbar=False)\n\n# Set the y labels to be iteration numbers\naxs[0].set_yticklabels(range(weights_case_1.shape[0])[::-1], rotation=0, va=\"center\")\naxs[1].set_yticklabels(range(weights_case_2.shape[0])[::-1], rotation=0, va=\"center\")\n\n# Set the y labels to show \"iteration\"\naxs[0].set_ylabel('iteration')\naxs[1].set_ylabel('iteration')\n\n# Hide the x-axis labels\naxs[0].set_xticklabels([])\naxs[1].set_xticklabels([])\n\n# Add a gap between the heatmaps\nplt.subplots_adjust(hspace=0.5)\n\n# Save the figure\nplt.savefig('weight_distribution.png', dpi=300)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.05189/appendix-case_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2005.05763/winowhy_demo.png",
        "caption": "One example from the WinoWhy dataset. Plausible and implausible reasons are indicated with the tick and the crosses respectively. Resources of different reasons are shown in brackets. `Human Reverse' means the human reason for the reverse question.",
        "source": "acl_20/2005.05763/WinoWhy-with-appendix.tex",
        "arxiv_id": "acl_20/2005.05763",
        "llava_output": "The image is a table with various statements and their corresponding reasons. It is not a chart or a graph, but rather a text-based representation of information. The table contains statements such as \"The fish ate the worm,\" \"It was hungry,\" and \"It refers to fish because,\" along",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib import lines\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Title text\nax.text(0.02, 0.95, 'The fish ate the worm. It was hungry.\\nIt refers to fish because:', fontsize=15, color='black', fontweight='bold', ha='left', va='center')\n\n# Option texts (A, B, C) with resources and ticks or crosses\noptions = [\n    (\"A. Hungry staff tend to eat. (Human)\", True),\n    (\"B. Worm is the one being eaten. (Human Reverse)\", False),\n    (\"C. the worm is a common name for a variety of fish. (Generation Model)\", False)\n]\n\nfor i, (option_text, is_correct) in enumerate(options):\n    ypos = 0.6 - i * 0.2\n    ax.text(0.05, ypos, option_text, fontsize=13, color='black', ha='left', va='center')\n\n    if is_correct:\n        plt.text(0.95, ypos, '\u2713', fontsize=20, color='green', ha='right', va='center')\n    else:\n        plt.text(0.95, ypos, '\u2718', fontsize=20, color='red', ha='right', va='center')\n\n# Drawing the dashed lines\ndashed_line_y_positions = [0.75, 0.55, 0.35, 0.15]\nfor ypos in dashed_line_y_positions:\n    line = lines.Line2D([0.05, 0.95], [ypos, ypos], lw=1, color='blue', linestyle='--', alpha=0.8, transform=ax.transAxes, clip_on=False)\n    ax.add_line(line)\n\n# Hide the axes, ticks, etc.\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\n\n# Save the figure\nplt.savefig('plot.png', dpi=300, bbox_inches='tight')\n\n# Display the figure\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.05763/winowhy_demo_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.05763/reason-category.png",
        "caption": "Distribution of different knowledge types.",
        "source": "acl_20/2005.05763/WinoWhy-with-appendix.tex",
        "arxiv_id": "acl_20/2005.05763",
        "llava_output": "The image is a bar chart showing the distribution of different knowledge types.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Data\ncategories = ['Object', 'Eventuality', 'Spatial', 'Others', 'Property', 'Quantity']\nvalues = [998, 963, 869, 839, 657, 634]\n\n# Create bar chart\nplt.bar(categories, values, color='blue')\n\n# Annotate each bar with the value\nfor i, value in enumerate(values):\n    plt.text(i, value + 10, str(value), ha='center', va='bottom')\n\n# Set labels and title\nplt.xlabel('Knowledge Types')\nplt.ylabel('Number of Infrequent Phrases')\nplt.title('Distribution of different knowledge types')\n\n# Save the figure with 300 DPI\nplt.savefig('distribution_of_knowledge_types.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.05763/reason-category_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2005.05763/reason-quality.png",
        "caption": "Distribution of reason plausibility score. The positive, acceptable, and negative reasons are denoted with the tick, confusing emoji, and cross respectively.",
        "source": "acl_20/2005.05763/WinoWhy-with-appendix.tex",
        "arxiv_id": "acl_20/2005.05763",
        "llava_output": "The image is a bar chart showing the distribution of reason plausibility score. The positive, acceptable, and negative reasons are denoted with the tick, confusing emoji, and cross respectively.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (replace with your actual data)\nplausibility_categories = ['0.0', '0.2', '0.4', '0.6', '0.8', '1.0']\nhuman_reasons = [120, 450, 300, 150, 50, 10]\nhuman_reverse_reasons = [100, 400, 250, 120, 40, 5]\ngeneration_model_reasons = [80, 320, 200, 100, 30, 3]\n\nbar_width = 0.2\n\n# Set position of bar on X axis\nr1 = np.arange(len(human_reasons))\nr2 = [x + bar_width for x in r1]\nr3 = [x + bar_width for x in r2]\n\n# Create the figure and the axes\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Plot the data\nax.bar(r1, human_reasons, color='skyblue', width=bar_width, edgecolor='grey', label='Human')\nax.bar(r2, human_reverse_reasons, color='orange', width=bar_width, edgecolor='grey', label='Human Reverse')\nax.bar(r3, generation_model_reasons, color='green', width=bar_width, edgecolor='grey', label='Generation Model')\n\n# Add the annotations for the tick, confused emoji, and cross\nax.text(-0.5, max(human_reasons) + 10, '\u274c', fontsize=30, ha='center')\nax.text(2.5, max(human_reasons) + 10, '\ud83d\ude15', fontsize=30, ha='center')\nax.text(5.5, max(human_reasons) + 10, '\u2705', fontsize=30, ha='center')\n\n# Add labels, title, and legend\nax.set_xlabel('Plausibility', fontweight='bold', fontsize=15)\nax.set_ylabel('Number of Winnowy Reasons', fontweight='bold', fontsize=15)\nax.set_title('Distribution of Reason Plausibility Scores')\nax.set_xticks([r + bar_width for r in range(len(human_reasons))])\nax.set_xticklabels(plausibility_categories)\n\nax.legend()\n\n# Save the plot as a PNG file with 300 DPI\nplt.tight_layout()\nplt.savefig('distribution_of_reason_plausibility.png', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.05763/reason-quality_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06114/stacked_length_comp.png",
        "caption": "Test scores compared against the number of dialog turns given as context prior to generating samples for \\decModel(355M) and \\norefModel(355M).",
        "source": "acl_20/2005.06114/acl2020.tex",
        "arxiv_id": "acl_20/2005.06114",
        "llava_output": "4 Bar Charts</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Mock data based on the trends observed in the provided image\n# Replace these with your actual data\ncontext_lengths = [1, 3, 7]\nscores_wo_refs_realistic = [0.38, 0.34, 0.32]\nscores_w_refs_realistic = [0.33, 0.35, 0.36]\n\nscores_wo_refs_reference = [0.40, 0.35, 0.30]\nscores_w_refs_reference = [0.39, 0.36, 0.33]\n\nscores_wo_refs_quality = [0.35, 0.33, 0.31]\nscores_w_refs_quality = [0.39, 0.37, 0.34]\n\nscores_wo_refs_coherency = [0.36, 0.34, 0.31]\nscores_w_refs_coherency = [0.35, 0.33, 0.32]\n\n# Create a 2x2 plot grid\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\n# Plot the first subplot for Realistic scores\naxs[0, 0].plot(context_lengths, scores_wo_refs_realistic, marker='o', label='355M (w/o refs)', color='blue')\naxs[0, 0].plot(context_lengths, scores_w_refs_realistic, marker='o', label='355M (w/ refs)', color='orange')\naxs[0, 0].set_title('Realistic')\naxs[0, 0].set_xlabel('Context Length')\naxs[0, 0].set_ylabel('Score')\n\n# Plot the second subplot for Reference scores\naxs[0, 1].plot(context_lengths, scores_wo_refs_reference, marker='o', label='355M (w/o refs)', color='blue')\naxs[0, 1].plot(context_lengths, scores_w_refs_reference, marker='o', label='355M (w/ refs)', color='orange')\naxs[0, 1].set_title('Reference')\naxs[0, 1].set_xlabel('Context Length')\naxs[0, 1].set_ylabel('Score')\n\n# Plot the third subplot for Quality scores\naxs[1, 0].plot(context_lengths, scores_wo_refs_quality, marker='o', label='355M (w/o refs)', color='blue')\naxs[1, 0].plot(context_lengths, scores_w_refs_quality, marker='o', label='355M (w/ refs)', color='orange')\naxs[1, 0].set_title('Quality')\naxs[1, 0].set_xlabel('Context Length')\naxs[1, 0].set_ylabel('Score')\n\n# Plot the fourth subplot for Coherency scores\naxs[1, 1].plot(context_lengths, scores_wo_refs_coherency, marker='o', label='355M (w/o refs)', color='blue')\naxs[1, 1].plot(context_lengths, scores_w_refs_coherency, marker='o', label='355M (w/ refs)', color='orange')\naxs[1, 1].set_title('Coherency')\naxs[1, 1].set_xlabel('Context Length')\naxs[1, 1].set_ylabel('Score')\n\n# Add a legend to the top left subplot\naxs[0, 0].legend(title='Model Sizes')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Save the figure as a png file with 300 dpi\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06114/stacked_length_comp_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06117/high_freq.png",
        "caption": "Number of annotations by frequent annotators",
        "source": "acl_20/2005.06117/main.tex",
        "arxiv_id": "acl_20/2005.06117",
        "llava_output": "1. The image is a bar chart showing the number of annotations by frequent annotators.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data based on the image\ncategories = ['Top 5', 'Top 10', 'Top 20', 'Top 30', 'Top 40', 'Top 50']\npercentages = [25, 40, 60, 75, 85, 100]  # Replace with actual percentages\n\n# Create the bar chart\nplt.figure(figsize=(10, 6)) # Adjust figure size if needed\nplt.bar(categories, percentages, color='blue')\n\n# Adding the title and labels\nplt.title('Number of annotations by frequent annotators')\nplt.xlabel('Frequent Annotators')\nplt.ylabel('Annotation Percentage (%)')\n\n# Adding the percentage text on top of each bar\nfor i, value in enumerate(percentages):\n    plt.text(i, value + 1, f\"{value}%\", ha='center', va='bottom')\n\n# Saving the figure with 300 DPI\nplt.savefig('annotations_by_frequent_annotators.png', dpi=300)\n\n# Also display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06117/high_freq_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06606/seg.jpg",
        "caption": "\\small An illustration of marginalizing subword segmentations of the word `unconscious'",
        "source": "acl_20/2005.06606/sec3_seg_lv.tex",
        "arxiv_id": "acl_20/2005.06606",
        "llava_output": "This is a line chart that illustrates the marginalizing subword segmentations of the word \"unconscious\". The chart shows the different subword segments of the word, with each segment represented by a line. The segments are labeled with their corresponding subword names, such as \"un\", \"con",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyBboxPatch, ConnectionPatch\n\n# Function to draw a box with rounded corners\ndef draw_box(ax, text, xy, width, height):\n    box = FancyBboxPatch((xy[0] - width / 2, xy[1] - height / 2),\n                         width,\n                         height,\n                         boxstyle=\"round,pad=0.1\",\n                         ec=\"black\",\n                         fc=\"white\",\n                         lw=2)\n    ax.add_patch(box)\n    ax.text(xy[0], xy[1], text, ha='center', va='center', fontsize=12)\n\n# Initialise the figure and axis\nfig, ax = plt.subplots(figsize=(8, 3))\n\n# Set axis limits and hide the axes\nplt.xlim(-2, 11)\nplt.ylim(-2, 3)\nplt.axis('off')\n\n# Define the positions of the nodes\nnodes = \"unconscious\"\nnode_positions = [(i, 0) for i in range(1, len(nodes) + 1)]\n\n# Draw the circles for each character in the word\nfor i, (x, y) in enumerate(node_positions, start=1):\n    circ = plt.Circle((x, y), 0.5, edgecolor='black', facecolor='lightblue', lw=2)\n    ax.add_patch(circ)\n    ax.text(x, y, nodes[i - 1], ha='center', va='center', fontsize=12)\n\n# Define the segments and positions\nsegments = [('uncon', (1.5, 2)), ('un', (-0.5, 2)), ('con', (3.5, -2)), ('scious', (8.5, 1)), ('conscious', (7.5, -2))]\nwidths = [2.5, 1.5, 1.5, 2.5, 3.0]\nheights = [1, 1, 1, 1, 1]\n\n# Draw the segments\nfor (text, xy), width, height in zip(segments, widths, heights):\n    draw_box(ax, text, xy, width, height)\n\n# Add connecting paths\ncon_paths = [([1, 0], [1.5, 1]), ([2, 0], [1.5, 1]), ([3, 0], [3.5, -1]), ([4, 0], [3.5, -1]), ([5, 0], [8.5, 0])]\ncon_patches = [ConnectionPatch(xyA=p1, xyB=p2, coordsA=\"data\", coordsB=\"data\",\n                               arrowstyle=\"-\", shrinkA=5, shrinkB=5,\n                               mutation_scale=20, fc=\"w\", linewidth=2)\n               for p1, p2 in con_paths]\n\nfor patch in con_patches:\n    ax.add_patch(patch)\n\n# Adding dots before the 'u' character\nax.text(0, 0, '....', ha='center', va='center', fontsize=12)\n\n# Save the figure\nplt.savefig('visualization.png', dpi=300, bbox_inches='tight')\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06606/seg_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06606/model.png",
        "caption": "An illustration of the mixed character-subword Transformer. The input is a list of characters, whereas the output is a sequence of subwords.",
        "source": "acl_20/2005.06606/sec4_trans.tex",
        "arxiv_id": "acl_20/2005.06606",
        "llava_output": "The image is a flow chart that shows the process of the mixed character-subword Transformer. It is not a Bar Chart, Line Chart, Pie Chart, Histogram, Scatter Plot, or Box Plot.</s>",
        "is_data_visual": "Based on the justification and figure caption, the image is a data visualization plot. The process of the mixed character-subword Transformer can be represented using a flow chart, which is a type of data visualization. The input is a list of characters, whereas the output is a sequence of subwords, which can be presented visually using a bar chart, line chart, pie chart, histogram, scatter plot, or box plot. However, since the image is a flow chart specifically, it can be considered a data visualization plot. Therefore, the answer is (Yes).",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Hide the axes\nax.axis('off')\n\n# Define the blocks' properties\nblocks = {\n    'Embedding': {'color': 'orange',   'xy': (0.2, 0.2), 'width': 0.6, 'height': 0.1},\n    'Transformer Decoder': {'color': 'blue',    'xy': (0.2, 0.4), 'width': 0.6, 'height': 0.1},\n    'Linear': {'color': 'purple',     'xy': (0.2, 0.6), 'width': 0.6, 'height': 0.05},\n    'Softmax': {'color': 'grey',      'xy': (0.2, 0.67), 'width': 0.6, 'height': 0.05},\n}\n\n# Add blocks to the figure\nfor text, block in blocks.items():\n    rect = patches.Rectangle(**block, edgecolor='black', linewidth=1)\n    ax.add_patch(rect)\n    plt.text(block['xy'][0] + block['width']/2, block['xy'][1] + block['height']/2, text,\n             horizontalalignment='center', verticalalignment='center')\n\n# Input sequence\ninput_seq = ['c', 'a', 't', '|', 's', 'a', 't']\nfor i, char in enumerate(input_seq):\n    plt.text(0.2 + i * 0.1, 0.1, char, ha='center', va='center')\n\n# Save the figure\nplt.savefig('transformer_visualization.png', dpi=300, bbox_inches='tight')\n\n# Show the plot (not required if you only want to save the file)\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06606/model_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06606/cond_disagreement.png",
        "caption": "Disagreement of DPE segments between Et-En and Ro-En over English vocabulary",
        "source": "acl_20/2005.06606/sec5_expr.tex",
        "arxiv_id": "acl_20/2005.06606",
        "llava_output": "The image is a line chart showing the disagreement of DPE segments between Et-En and Ro-En over English vocabulary.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Data\nfrequencies = ['5', '10', '25', '50', '>50']\npercentages = [0.35, 0.22, 0.16, 0.10, 0.01]\ncolors = ['#1f77b4', '#ff7f0e', '#7f7f7f', '#ffbb78', '#aec7e8']\n\n# Plotting the bar chart\nplt.bar(frequencies, percentages, color=colors)\n\n# Adding the percentage text on top of each bar\nfor i, (freq, perc) in enumerate(zip(frequencies, percentages)):\n    plt.text(freq, perc, str(perc), ha='center', va='bottom', fontsize=10, color='black')\n\n# Setting the title and labels\nplt.title('Disagreement between Et-En an Ro-En over En side')\nplt.xlabel('Word Frequency')\nplt.ylabel('Percentage')\n\n# Save the figure\nplt.savefig('bar_chart.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06606/cond_disagreement_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06606/disagreement.png",
        "caption": "Disagreement of segments between BPE and DPE over Estonian vocabulary.",
        "source": "acl_20/2005.06606/sec5_expr.tex",
        "arxiv_id": "acl_20/2005.06606",
        "llava_output": "The image is a line chart that shows the disagreement of segments between BPE and DPE over Estonian vocabulary.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the bar chart\nfrequencies = ['5', '10', '50', '100', '200', '>200']\nvalues = [0.64, 0.52, 0.47, 0.40, 0.37, 0.16]\ncolors = ['blue', 'orange', 'grey', 'yellow', 'skyblue', 'green']\n\n# Create the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(frequencies, values, color=colors)\n\n# Add the data values on top of the bars\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 2), ha='center', va='bottom')\n\n# Set the title and labels\nplt.title('Disagreement between BPE and DPE')\nplt.xlabel('Word Frequency')\nplt.ylabel('Percentage')\n\n# Save the figure with 300dpi\nplt.savefig('disagreement_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06606/disagreement_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2005.06606/bleu_enet.png",
        "caption": "BLEU scores of BPE vs DPE by the lengths of sentences for En$\\rightarrow$Et.",
        "source": "acl_20/2005.06606/sec5_expr.tex",
        "arxiv_id": "acl_20/2005.06606",
        "llava_output": "The image is a line chart that shows the BLEU scores of BPE and DPE by the lengths of sentences for En$\\rightarrow$Et.</s>",
        "is_data_visual": "Yes, the justification and caption both clearly state that the image is a line chart of BLEU scores for BPE and DPE with respect to the lengths of En$\\rightarrow$Et sentences. Therefore, it is a data visualization plot that presents experimental results.",
        "code": "import matplotlib.pyplot as plt\n\n# Example data based on the visible bars in the chart\nsentence_lengths = ['10', '20', '30', '40', '50', '>50']\nBPE_scores = [16.0, 18.0, 17.5, 17.0, 16.0, 14.0]  # Replace these with actual values\nDPE_scores = [17.0, 19.0, 18.5, 17.5, 16.5, 15.0]  # Replace these with actual values\n\nbar_width = 0.35  # bar width\nindex = range(len(sentence_lengths))\n\n# Plotting BPE scores\nplt.bar(index, BPE_scores, bar_width, label='BPE', color='blue')\n\n# Plotting DPE scores\nplt.bar([i + bar_width for i in index], DPE_scores, bar_width, label='DPE', color='red')\n\nplt.xlabel('Sentence Length')\nplt.ylabel('BLEU')\nplt.title('BLEU scores by sentence length (En-Et)')\nplt.xticks([i + bar_width / 2 for i in index], sentence_lengths)\nplt.legend()\n\n# Save the figure with 300 dpi\nplt.savefig('bleu_scores.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.06606/bleu_enet_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.07105/OpenIE_InVOutDomain.png",
        "caption": "For OpenIE, using the full SWDE set (except the test site), including in-vertical training data (i.e. Level II knowledge), allows for 5-10 point gains in precision at equivalent recall compared to using only out-of-vertical training data (Level I).",
        "source": "acl_20/2005.07105/acl2020.tex",
        "arxiv_id": "acl_20/2005.07105",
        "llava_output": "The image is a line chart showing the precision and recall of an experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Replace these lists with your actual data points\nrecall_unseen_website = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nprecision_unseen_website = [1, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5]\n\nrecall_unseen_vertical = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nprecision_unseen_vertical = [0.95, 0.9, 0.88, 0.86, 0.84, 0.82, 0.8, 0.78, 0.76, 0.74, 0.72]\n\n# Setting up the plot\nplt.figure()\nplt.plot(recall_unseen_website, precision_unseen_website, label='Unseen Website', color='blue')\nplt.plot(recall_unseen_vertical, precision_unseen_vertical, label='Unseen Vertical', color='orange')\n\n# Add labels and a legend\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('precision_recall_curve.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.07105/OpenIE_InVOutDomain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.07105/varying_number_of_training_sites.png",
        "caption": "Performance on the ClosedIE Movie vertical increases significantly as more sites are added to the training data.",
        "source": "acl_20/2005.07105/acl2020.tex",
        "arxiv_id": "acl_20/2005.07105",
        "llava_output": "The image is a line chart that shows the performance on the ClosedIE Movie vertical as more training sites are added.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - replace these with the actual data.\nrecall_levels = np.linspace(0, 1, 100)\nprecision_data = {\n    1: 1 - 0.9 * recall_levels**0.5,\n    2: 1 - 0.8 * recall_levels**0.5,\n    3: 1 - 0.75 * recall_levels**0.5,\n    4: 1 - 0.70 * recall_levels**0.5,\n    5: 1 - 0.65 * recall_levels**0.5,\n    6: 1 - 0.60 * recall_levels**0.5,\n    7: 1 - 0.55 * recall_levels**0.5,\n}\n\n# Color map (so the colors will not match exactly without the exact RGB values)\ncolors = plt.cm.viridis(np.linspace(0, 1, 7))\n\n# Create the plot\nplt.figure()\n\nfor i, color in zip(precision_data.keys(), colors):\n    plt.plot(recall_levels, precision_data[i], label=f'{i}', color=color)\n\n# Add the legend\nplt.legend(title=\"# training sites\", loc='upper right')\n\n# Label the axes\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\n# Set the plot limits\nplt.xlim([0, 1])\nplt.ylim([0, 1])\n\n# Save the plot with 300 dpi\nplt.savefig('precision_recall_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.07105/varying_number_of_training_sites_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.08178/imojie.png",
        "caption": "Precision-Recall curve of OpenIE Systems.",
        "source": "acl_20/2005.08178/acl2020.tex",
        "arxiv_id": "acl_20/2005.08178",
        "llava_output": "The image is a line chart displaying the precision-recall curve of OpenIE systems.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Fictitious data for the example\nrecall_points = np.linspace(0, 0.5, 50)\nprecision_openie4 = np.exp(-recall_points) * (1 - recall_points) + 0.2\nprecision_rnn = np.exp(-recall_points*1.2) * (1 - (recall_points*1.1)) + 0.15\nprecision_copyattn = np.exp(-recall_points*1.5) * (1 - (recall_points*1.3)) + 0.1\nprecision_imojie = np.exp(-recall_points*1.7) * (1 - (recall_points*1.5)) + 0.05\n\n# Create the plot\nplt.figure()\nplt.plot(recall_points, precision_openie4, label='OpenIE-4', color='blue')\nplt.plot(recall_points, precision_rnn, label='RNN-OIE', color='red')\nplt.plot(recall_points, precision_copyattn, label='CopyAttn(+BERT)', color='green')\nplt.plot(recall_points, precision_imojie, label='IMOJIE(Agg)', color='orange')\n\n# Setting the title and labels\nplt.title('Precision-Recall curve of OpenIE Systems')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\n# Adding a legend\nplt.legend()\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('precision_recall_curve.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.08178/imojie_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.08178/Fig1.png",
        "caption": "Measuring performance with varying input sentence lengths",
        "source": "acl_20/2005.08178/acl2020.tex",
        "arxiv_id": "acl_20/2005.08178",
        "llava_output": "The image is a bar chart that shows the performance of two different algorithms, Bert and Moje, with varying input sentence lengths. The chart has a horizontal axis that represents the sentence length in words and a vertical axis that shows the performance score. There are multiple bars in the chart, each representing a different sentence length,",
        "is_data_visual": "Yes, the image is a data visualization plot representing the performance of two different algorithms (Bert and Moje) with varying input sentence lengths. It shows the performance score of each algorithm for different input sentence lengths.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data: Replace with your actual data\ncategories = ['9-17', '17-25', '25-33', '33-41', '41-49', '49-62']\ncopy_attention_bert = [65, 62, 55, 49, 45, 43]  # Replace with actual data\nimojie = [60, 59, 50, 48, 44, 40]                # Replace with actual data\n\n# Define the bar width and positions\nbar_width = 0.35\nindex = np.arange(len(categories))\n\n# Create the bars\nfig, ax = plt.subplots()\nbar1 = ax.bar(index, copy_attention_bert, bar_width, label='CopyAttention+BERT', color='blue')\nbar2 = ax.bar(index + bar_width, imojie, bar_width, label='IMOJIE (OpenIE-4)', color='red')\n\n# Add the labels and title\nax.set_xlabel('Sentence length (in words)')\nax.set_ylabel('Optimal F1 Score')\nax.set_title('Measuring performance with varying input sentence lengths')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(categories)\nax.legend()\n\n# Save the figure with high resolution\nplt.savefig('performance_varying_input_lengths.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.08178/Fig1_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "acl_20_figures/2005.08178/Fig2.png",
        "caption": "Measuring performance of CopyAttention with BERT model upon changing the beam size",
        "source": "acl_20/2005.08178/acl2020.tex",
        "arxiv_id": "acl_20/2005.08178",
        "llava_output": "The image is a line chart that presents the performance of CopyAttention with BERT model upon changing the beam size. The chart shows the results of the experiment, with the beam size varying from 1 to 11. The line chart is divided into three sections, each representing a different beam size. The",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data points extracted from the image\nbeam_sizes = [1, 3, 5, 7, 11]\noptimal_f1 = [38.8, 51.7, 50.7, 50.2, 49.7]\nauc = [20, 32.1, 34.2, 38.3, 35.4]\nlast_f1 = [38.8, 49.2, 43.8, 34.8, 30.6]\n\n# Plotting the data\nplt.plot(beam_sizes, optimal_f1, marker='o', color='blue', label='Optimal F1')\nplt.plot(beam_sizes, auc, marker='o', color='red', label='AUC')\nplt.plot(beam_sizes, last_f1, marker='o', color='gold', label='Last F-1')\n\n# Annotating the data points with their value\nfor i, txt in enumerate(optimal_f1):\n    plt.annotate(txt, (beam_sizes[i], optimal_f1[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\nfor i, txt in enumerate(auc):\n    plt.annotate(txt, (beam_sizes[i], auc[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\nfor i, txt in enumerate(last_f1):\n    plt.annotate(txt, (beam_sizes[i], last_f1[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Labeling the axes and the title\nplt.xlabel('Beam size')\nplt.ylabel('Performance metric value')\nplt.title('Measuring performance of CopyAttention with BERT model upon changing the beam size')\n\n# Adding a legend\nplt.legend()\n\n# Saving the figure with 300 DPI\nplt.savefig('performance_plot.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.08178/Fig2_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "acl_20_figures/2005.10070/size_effect.png",
        "caption": "ROUGE-1 F1-scores for different numbers of supplementary articles from Common Crawl.",
        "source": "acl_20/2005.10070/main.tex",
        "arxiv_id": "acl_20/2005.10070",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - replace these with your actual data values\nsupplementary_articles = np.array([0, 10, 20, 50, 100, 200])\noracle_scores = np.array([0.44, 0.48, 0.50, 0.52, 0.54, 0.56])\ncentroid_scores = np.array([0.30, 0.32, 0.34, 0.36, 0.38, 0.40])\ntsr_scores = np.array([0.26, 0.28, 0.30, 0.32, 0.34, 0.36])\n\n# Error values as standard deviation, assumming some random standard deviation\noracle_std = np.array([0.01] * len(supplementary_articles))\ncentroid_std = np.array([0.015] * len(supplementary_articles))\ntsr_std = np.array([0.02] * len(supplementary_articles))\n\n# Plotting the data\nplt.errorbar(supplementary_articles, oracle_scores, yerr=oracle_std, fmt='-o', color='red', label='Oracle')\nplt.errorbar(supplementary_articles, centroid_scores, yerr=centroid_std, fmt='-s', color='blue', label='Centroid')\nplt.errorbar(supplementary_articles, tsr_scores, yerr=tsr_std, fmt='-^', color='green', label='TSR')\n\n# Additional plot parameters\nplt.xlabel('Number of Supplementary Articles')\nplt.ylabel('ROUGE-1 F1-score')\nplt.title('ROUGE-1 F1-scores for different numbers of supplementary articles')\nplt.legend()\nplt.grid(True)\n\n# Save the figure\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show plot for interactive use\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.10070/size_effect_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2005.10107/frequencies.png",
        "caption": "Counts of published articles and textual mentions across dates in an article collection about Enron.",
        "source": "acl_20/2005.10107/main.tex",
        "arxiv_id": "acl_20/2005.10107",
        "llava_output": "The image is a line chart showing the counts of published articles and textual mentions across dates in an article collection about Enron.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport numpy as np\n\n# Sample data (replace with your actual data)\npublication_dates = [datetime(2001, 1, 1), datetime(2002, 5, 15), datetime(2003, 8, 20)]  # Dates of publication\npub_counts = [10, 5, 15]  # Counts of published articles\n\nmention_dates = [datetime(2001, 3, 10), datetime(2002, 7, 25), datetime(2003, 11, 30)]  # Dates of textual mentions\nment_counts = [20, 10, 25]  # Counts of textual mentions\n\nground_truth_dates = [datetime(2001, 6, 1), datetime(2002, 8, 15), datetime(2003, 10, 20)]  # Ground truth dates\n\n# Create the plot\nplt.figure(figsize=(10, 6))\n\n# Create two subplots (stacked)\nax1 = plt.subplot(211)\nax2 = plt.subplot(212, sharex=ax1)\n\n# Plot publication count\nax1.bar(publication_dates, pub_counts, color='red', label='pub-count', width=20)\n\n# Plot mention count\nax2.bar(mention_dates, ment_counts, color='orange', label='ment-count', width=20)\n\n# Plot ground truth as lines on both subplots\nfor ax in [ax1, ax2]:\n    for date in ground_truth_dates:\n        ax.axvline(x=date, color='purple', linestyle='-', linewidth=2)\n\n# Set a log scale for y-axis of mentions to match the plot scale in your image\nax2.set_yscale('log')\n\n# Add legend to both subplots\nax1.legend(loc='upper left')\nax2.legend(loc='upper left')\n\n# Format the x-axis to show the year only\nax2.xaxis.set_major_locator(mdates.YearLocator())\nax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n\n# Tighten the layout\nplt.tight_layout()\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('enron_article_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.10107/frequencies_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.10716/shapleyRemoveSparse.png",
        "caption": "Removing training data with low Shapley value improves the performance of the KNN regressor.",
        "source": "acl_20/2005.10716/acl2020.tex",
        "arxiv_id": "acl_20/2005.10716",
        "llava_output": "The image is a line chart showing the performance of a KNN regressor with different amounts of training data removed.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - Replace these with actual values from your study\nx_values = np.arange(0, 31, 5)  # Fraction of train data removed\ny_shapley_1 = np.random.uniform(70, 90, len(x_values))\ny_shapley_5 = np.random.uniform(70, 90, len(x_values))\ny_shapley_25 = np.random.uniform(70, 90, len(x_values))\ny_shapley_50 = np.random.uniform(70, 90, len(x_values))\ny_shapley_100 = np.random.uniform(70, 90, len(x_values))\ny_random_50 = np.random.uniform(60, 80, len(x_values))\n\n# Creating the plot\nplt.figure(figsize=(10, 5))\nplt.plot(x_values, y_shapley_1, label='Shapley (K_test = 1)', color='black', linestyle='-')\nplt.plot(x_values, y_shapley_5, label='Shapley (K_test = 5)', color='red', linestyle='-')\nplt.plot(x_values, y_shapley_25, label='Shapley (K_test = 25)', color='green', linestyle='-')\nplt.plot(x_values, y_shapley_50, label='Shapley (K_test = 50)', color='blue', linestyle='-')\nplt.plot(x_values, y_shapley_100, label='Shapley (K_test = 100)', color='magenta', linestyle='-')\nplt.plot(x_values, y_random_50, label='Random (K_test = 50)', color='red', linestyle='--')\n\nplt.title('Prediction accuracy (%)')\nplt.xlabel('Fraction of train data removed (%)')\nplt.ylabel('Prediction accuracy (%)')\nplt.legend()\n\n# Saving the plot with 300dpi resolution\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.10716/shapleyRemoveSparse_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.10865/abstracts.png",
        "caption": "Detailed view of selected abstracts that contribute to the evidence map. These are automatically annotated with all extracted information.",
        "source": "acl_20/2005.10865/main.tex",
        "arxiv_id": "acl_20/2005.10865",
        "llava_output": "0</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data for the bar chart\ncategories = ['Category A', 'Category B', 'Category C']\nvalues = [10, 20, 30]\n\n# Creating a bar chart\nplt.bar(categories, values)\n\n# Adding a title\nplt.title('Sample Bar Chart')\n\n# Labeling axes\nplt.xlabel('Categories')\nplt.ylabel('Values')\n\n# Saving the figure as a PNG file with 300 DPI\nplt.savefig('bar_chart.png', dpi=300)\n\n# Optional: Display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.10865/abstracts_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2005.11055/pie_chart.png",
        "caption": "Relative frequencies of each tag in the dataset.",
        "source": "acl_20/2005.11055/ticketseg_03_zdata.tex",
        "arxiv_id": "acl_20/2005.11055",
        "llava_output": "The image is a pie chart showing the relative frequencies of each tag in the dataset.</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data: relative frequencies (percentages) for each tag\n# Replace with your actual data\nsizes = [10, 15, 20, 25, 10, 5, 15]\nlabels = ['Other', 'Command / Code', 'Command Output / Info Message', \n          'Error Message / Stack Trace', 'File Content (Not Code)', 'Path / URL', \n          'Semi-Structured Information']\n\n# Colors for each section\ncolors = ['#f0b3d6', '#5293d2', '#bbd792', '#faa74b', \n          '#c7c7c7', '#fad989', '#b2df8a']\n\n# Explode the 3rd slice (e.g. 'Command Output / Info Message')\nexplode = (0, 0, 0.1, 0, 0, 0, 0)  # only \"explode\" the 3rd slice (if desired)\n\nplt.figure(figsize=(8, 4))\n\n# Plot a donut chart\nplt.pie(sizes, labels=None, colors=colors, autopct=None, startangle=140, explode=explode, \n        wedgeprops=dict(width=0.3))\n\n# Draw a circle at the center of the pie to make it a donut\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')\n\n# Remove the label for the legend\nplt.legend(labels, loc='center right', bbox_to_anchor=(1.5, 0.5), fontsize=12)\n\n# Save the plot as a PNG file\nplt.savefig('donut_plot.png', bbox_inches='tight', dpi=300)\n\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.11055/pie_chart_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "acl_20_figures/2005.13344/plot_dm.png",
        "caption": "Number of predicted transitions relative to the length of the sentence, for the three SDP formalisms on the development set from SemEval 2015 Task 18.",
        "source": "acl_20/2005.13344/acl2020.tex",
        "arxiv_id": "acl_20/2005.13344",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Based on the provided justification and figure caption, the image is a histogram representing the number of predicted transitions relative to the length of the sentence, for the three SDP formalisms on the development set from SemEval 2015 Task 18. Therefore, the answer is: Yes.",
        "code": "import matplotlib.pyplot as plt\n\n# This represents the data; you will need to obtain the actual data from your academic paper\nsentence_lengths = [1, 2, 3, 4]  # Replace this with actual sentence length data\npredicted_transitions = [1.5, 2.5, 3.5, 4.5]  # Replace this with actual transition data\n\n# Create the scatter plot\nplt.scatter(sentence_lengths, predicted_transitions, color='red')  # Assuming the points are red\n\n# Optionally add titles and labels if they were used in the original plot\nplt.title(\"Number of predicted transitions relative to the length of the sentence\")\nplt.xlabel(\"Sentence Length\")\nplt.ylabel(\"Predicted Transitions\")\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig(\"plot.png\", dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.13344/plot_dm_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "acl_20_figures/2005.13962/extraction-process-diagram-v10.png",
        "caption": "The extraction process for the measurements released in \\corpusname.",
        "source": "acl_20/2005.13962/arxiv.tex",
        "arxiv_id": "acl_20/2005.13962",
        "llava_output": "3</s>",
        "is_data_visual": "Yes.",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data\ntime = np.linspace(0, 1, 100)\naudio_signal = np.random.normal(0, 1, size=time.shape)\n\n# Create subplots\nfig, axs = plt.subplots(3, figsize=(10, 8), gridspec_kw={'height_ratios': [1, 2, 2]})\n\n# Audio waveform plot\naxs[0].plot(time, audio_signal)\naxs[0].set_title('Audio Waveform Example')\naxs[0].set_xlim([time[0], time[-1]])\n\n# Phoneme alignment plot (placeholder)\nphoneme_example = ['k', 'a', 'm', ' ', 'n']\nfor i, phoneme in enumerate(phoneme_example):\n    axs[1].text(i, 0, phoneme, size=12, ha='center')\naxs[1].set_title('Phoneme Alignment Example')\naxs[1].axis('off')\n\n# Formant and sibilant plot (placeholder)\nfor i in range(3):\n    axs[2].plot(time, np.random.normal(0, 1, size=time.shape))\naxs[2].set_title('Formant and Sibilant Measures Example')\naxs[2].set_xlim([time[0], time[-1]])\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('example_figure.png', dpi=300)\n\n# Show the figure\nplt.show()",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2005.13962/extraction-process-diagram-v10_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "acl_20_figures/2006.09639/simplification.jpg",
        "caption": "An example of three edit operations on a given sentence. Note that dropping clauses or phrases is common in text simplification datasets.",
        "source": "acl_20/2006.09639/acl2020.tex",
        "arxiv_id": "acl_20/2006.09639",
        "llava_output": "1. In 2016, American developers had spent 12 billion dollars on constructing theme parks.\n2. According to a Seattle-based reporter, American developers had spent 12 billion dollars on constructing theme parks.\n3. Reducing the sentence to a simpler",
        "is_data_visual": "Yes, the image must represent a data visualization since it portrays the amount of money spent by American developers on constructing theme parks in 2016.",
        "code": "import matplotlib.pyplot as plt\n\n# Prepare the sentences and edit labels\nsentences = [\n    \"In 2016 alone, American developers had spent 12 billion dollars on constructing theme parks, according to a Seattle based reporter.\",\n    \"In 2016 alone, American developers had spent 12 billion dollars on constructing theme parks.\",\n    \"American developers had spent 12 billion dollars in 2016 alone on constructing theme parks.\",\n    \"American developers had spent 12 billion dollars in 2016 alone on building theme parks.\"\n]\noperations = [\"Deletion\", \"Reordering\", \"Lexical Simplification\"]\n\n# Initialize the plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.axis('off')  # Hide the axes\n\n# Add the sentences and arrows\nfor i, sentence in enumerate(sentences):\n    ax.text(0.1, 1-i*0.25, sentence, fontsize=12, verticalalignment='top')\n    if i < len(operations):\n        ax.annotate('', xy=(0.5, 0.87-i*0.25), xytext=(0.5, 1-i*0.25),\n                    arrowprops=dict(arrowstyle=\"->\", lw=1.5))\n        ax.text(0.52, 1-(i+0.5)*0.25, operations[i], fontsize=12, verticalalignment='top')\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('text_simplification_visualization.png', dpi=300)",
        "runnable": true,
        "output_figure_path": "chat-gpt-v/acl_20/2006.09639/simplification_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2110.04869/latency.png",
        "caption": "Estimated latency from the lookup table vs. evaluated latency on V100 GPU with batch size 256. Reduction ratio computed with respect to the latency of the full model.",
        "source": "cvpr_3/2110.04869/5_appendix.tex",
        "arxiv_id": "cvpr_3/2110.04869",
        "llava_output": "The image is a line chart showing the estimated latency from a lookup table and the evaluated latency on a V100 GPU with a batch size of 256. The reduction ratio is computed with respect to the latency of the full model.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate dummy data for the plot\nx = np.linspace(0, 80, 100)\ny = x  # Since the plot seems to be a y=x line, but you can adjust it as needed\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Scatter plot with green circles and specified size\nax.scatter(x, y, s=50, facecolors='none', edgecolors='green', linewidth=1.5)\n\n# Plotting the y=x line with green dotted line style\nax.plot(x, y, 'g--')\n\n# Setting x and y axis limits\nax.set_xlim(0, 80)\nax.set_ylim(0, 80)\n\n# Setting x and y labels\nax.set_xlabel('Estimated latency reduction (%)')\nax.set_ylabel('Evaluated latency reduction (%)')\n\n# Adding the R^2 value\nax.text(50, 10, r'$R^2 = 0.99864$', fontsize=12, color='green')\n\n# Save the figure as a png file with 300 dpi\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data (you would replace this with your actual data)\nnp.random.seed(0)\nx = np.random.rand(100) * 80  # Estimated latency reduction\ny = x + (np.random.rand(100) - 0.5) * 10  # Evaluated latency reduction, with some noise\n\n# Fit a linear model to the data to get the R squared value (for illustration)\ncoefficients = np.polyfit(x, y, 1)\npoly = np.poly1d(coefficients)\ny_fit = poly(x)\nss_res = np.sum((y - y_fit) ** 2)\nss_tot = np.sum((y - np.mean(y)) ** 2)\nr_squared = 1 - (ss_res / ss_tot)\n\n# Create the scatter plot\nplt.scatter(x, y, facecolors='none', edgecolors='g')\n\n# Annotate the R squared value on the plot\nplt.text(5, 75, 'R$^2$ = {:.5f}'.format(r_squared))\n\n# Label the axes\nplt.xlabel('Estimated latency reduction (%)')\nplt.ylabel('Evaluated latency reduction (%)')\n\n# Save the figure as a png file with 300 DPI\nplt.savefig('scatter_plot.png', dpi=300)\n\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2110.04869/latency_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2110.04869/tradeoff.png",
        "caption": "Comparing the parameter reduction-accuracy tradeoff and latency reduction-accuracy tradeoff of different pruning schemes. Latency estimated on RTX 2080 GPU. Model size compression rate and latency reduction rate are computed based on that of the DeiT-Base model respectively.",
        "source": "cvpr_3/2110.04869/5_appendix.tex",
        "arxiv_id": "cvpr_3/2110.04869",
        "llava_output": "This image is a scatter plot that compares the parameter reduction-accuracy tradeoff and latency reduction-accuracy tradeoff of different pruning schemes. The x-axis represents the parameter reduction rate, while the y-axis represents the accuracy. The scatter plot shows the relationship between the two variables",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data points\nx = [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]\ny = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Plot the data\nax.scatter(x, y, color='green')\n\n# Set the title and axis labels\nax.set_xlabel('Hessian sensitivity')\nax.set_ylabel('Squared loss difference')\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('plot.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\nplt.style.use('seaborn-darkgrid')  # Set the plot style to match closely with the image provided\nplt.rcParams.update({'font.size': 14})  # Adjust to match font size, this may need tuning\n# Example data points\nlatency_reduction = [10, 20, 30, 40, 50]  # This should be the x-axis data\naccuracy_no_head_align = [83.0, 82.8, 82.6, 82.4, 82.2]  # y-axis data for 'No explicit head alignment'\naccuracy_head_align = [83.5, 83.3, 83.1, 82.9, 82.7]  # y-axis data for 'Explicit head alignment (Ours)'\n\n# Plot the lines\nplt.plot(latency_reduction, accuracy_no_head_align, 'o-', color='gray', label='No explicit head alignment')\nplt.plot(latency_reduction, accuracy_head_align, 'o-', color='green', label='Explicit head alignment (Ours)')\nplt.axhline(y=83, color='blue', linestyle='--', label='DEIT')  # Assuming the DEIT baseline is at y=83%\n\n# Note: You will have to adjust these values to match the data from your paper.\nplt.xlabel('Latency reduction (%)')\nplt.ylabel('Top-1 accuracy (%)')\nplt.title('Comparison of Pruning Schemes')  # Adjust title to match your figure caption if needed\n\n# Add the legend outside the plot on the bottom\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=1)\nplt.xlim(0, 60)  # Adjust this to the range of your actual latency reduction data\nplt.ylim(81, 84)  # Adjust this to the range of your actual accuracy data\n\n# Save figure with high resolution\nplt.savefig('comparison_plot.png', dpi=300, bbox_inches='tight')\nplt.show()\nplt.close()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2110.04869/tradeoff_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2110.04869/sensitivity.png",
        "caption": "Hessian importance score vs. squared loss difference.",
        "source": "cvpr_3/2110.04869/5_appendix.tex",
        "arxiv_id": "cvpr_3/2110.04869",
        "llava_output": "The image is a scatter plot showing the Hessian importance score on the y-axis and the squared loss difference on the x-axis.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data points (you may need to adjust these to match the exact data from the plot)\nx_data = [0, 0.5, 0.6, 0.8, 0.9, 1, 1.2, 1.5, 2, 2.5, 3, 3.3, 3.7, 4, 4.5, 5]\ny_data = [0.05, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.15, 0.2, 0.25, 0.3, 0.32, 0.35, 0.38, 0.4, 0.42]\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Plot the data\nax.scatter(x_data, y_data, color='green')\n\n# Set the title and axis labels\nax.set_xlabel('Hessian sensitivity')\nax.set_ylabel('Squared loss difference')\nax.set_xlim(0, 5)\nax.set_ylim(0, 0.4)\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('plot.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate mock data\nnp.random.seed(0)  # For reproducibility\nhessian_sensitivity = np.random.uniform(0, 5, 50)\nsquared_loss_difference = hessian_sensitivity * np.random.uniform(0, 0.08, 50)\nsquared_loss_difference += np.random.normal(0, 0.02, 50)\n\n# Scatter plot\nplt.scatter(hessian_sensitivity, squared_loss_difference, color='green')\n\n# Label axes\nplt.xlabel('Hessian sensitivity')\nplt.ylabel('Squared loss difference')\n\n# Save the plot\nplt.savefig('scatter_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2110.04869/sensitivity_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2202.06312/PR_PBE.png",
        "caption": "The progress of clean-image identification with respect to the increase of iterations.",
        "source": "cvpr_3/2202.06312/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2202.06312",
        "llava_output": "The image is a scatter plot, showing the progress of clean-image identification with respect to the increase of iterations.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating dummy data\nx = np.linspace(0, 1, 100)\niterations = {\n    'iter1': np.sqrt(x),\n    'iter2': np.sqrt(x) - 0.05,\n    'iter3': np.sqrt(x) - 0.1,\n    'iter4': np.sqrt(x) - 0.15,\n    'iter5': np.sqrt(x) - 0.2,\n    'iter6': np.sqrt(x) - 0.25,\n    'iter7': np.sqrt(x) - 0.3,\n    'iter8': np.sqrt(x) - 0.35\n}\n\ncolors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray']\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot the data\nfor i, (label, y_data) in enumerate(iterations.items()):\n    ax.plot(x, y_data, label=label, color=colors[i])\n\n# Set the title, axis labels, and legend\nax.set_title('Precision/Recall Curve')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.legend(loc='lower right')\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('precision_recall_curve.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# This is example data, replace these arrays with your actual data points\nrecall = np.linspace(0, 1, 100)\nprecisions = {\n    'iter1': np.sqrt(recall),\n    'iter2': np.square(recall),\n    'iter3': np.sqrt(recall) * 0.9,\n    'iter4': np.square(recall) * 0.9,\n    'iter5': np.sqrt(recall) * 0.8,\n    'iter6': np.square(recall) * 0.8,\n    'iter7': np.sqrt(recall) * 0.7,\n    'iter8': np.square(recall) * 0.7\n}\n\n# Set up the plot\nplt.figure(figsize=(10, 6))\n\n# Plot each precision-recall curve\nfor label, precision in precisions.items():\n    plt.plot(recall, precision, label=label)\n\n# Customize the plot\nplt.title('Precision/Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(title='Legend')\nplt.grid(True)\n\n# Save the plot to a PNG file\nplt.savefig('precision_recall_curve.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2202.06312/PR_PBE_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "cvpr_figures_2/2205.04437/Pretrain.png",
        "caption": "Quantitative comparison on PSNR(dB) of four different networks without and with the same-task pre-training on $\\times$4 SR.",
        "source": "cvpr_3/2205.04437/ArxivVersion.tex",
        "arxiv_id": "cvpr_3/2205.04437",
        "llava_output": "The image is a line chart that shows the PSNR (dB) values of four different networks. The chart compares the performance of the networks without and with the same-task pre-training on $\\times$4 SR. The line chart is black and white, and the PSNR values are represented by the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data based on visual representation\nnetworks = ['Network1', 'Network2', 'Network3', 'Network4']\nwithout_pretraining = [2.5, 5, 3.5, 4.5]\nwith_pretraining = [3, 7, 5, 6]\n\nx = np.arange(len(networks))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nrects1 = ax.bar(x - width/2, without_pretraining, width, label='Without Pre-training', color='blue')\nrects2 = ax.bar(x + width/2, with_pretraining, width, label='With Pre-training', color='orange')\n\n# Add some text for labels, title, and custom x-axis tick labels, etc.\nax.set_ylabel('PSNR(dB)')\nax.set_title('Quantitative comparison on PSNR(dB) of four different networks without and with the same-task pre-training on \u00d74 SR')\nax.set_xticks(x)\nax.set_xticklabels(networks)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.savefig(\"comparison_plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data - replace with the data from the paper\nnetworks = ['Net1', 'Net2', 'Net3', 'Net4']\nvalues_without_pretraining = [20, 25, 30, 35]  # Replace with actual PSNR values\nvalues_with_pretraining = [22, 27, 33, 37]  # Replace with actual PSNR values\nerror = [1, 2, 1.5, 2]  # Replace with actual error values\n\n# Setting the positions and width for the bars\npos = np.arange(len(networks))\nwidth = 0.4\n\n# Plotting the bars\nfig, ax = plt.subplots()\n\nbars1 = ax.bar(pos - width/2, values_without_pretraining, width, \n               yerr=error, label='Without Pretraining', color='blue', capsize=10)\nbars2 = ax.bar(pos + width/2, values_with_pretraining, width, \n               yerr=error, label='With Pretraining', color='orange', capsize=10)\n\n# Adding labels and title (update accordingly)\nax.set_ylabel('PSNR(dB)')\nax.set_title('Quantitative Comparison on PSNR(dB)')\nax.set_xticks(pos)\nax.set_xticklabels(networks)\n\n# Adding the legend and configuring it to show in the best location\nax.legend(loc='best')\n\n# Save the figure with 300 DPI\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2205.04437/Pretrain_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2110.04869/latency.png",
        "caption": "Estimated latency from the lookup table vs. evaluated latency on V100 GPU with batch size 256. Reduction ratio computed with respect to the latency of the full model.",
        "source": "cvpr_3/2110.04869/5_appendix.tex",
        "arxiv_id": "cvpr_3/2110.04869",
        "llava_output": "The image is a line chart that shows the estimated latency from a lookup table compared to the evaluated latency on a V100 GPU with a batch size of 256. The reduction ratio is computed with respect to the latency of the full model.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate dummy data for the plot\nx = np.linspace(0, 80, 100)\ny = x  # Since the plot seems to be a y=x line, but you can adjust it as needed\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Scatter plot with green circles and specified size\nax.scatter(x, y, s=50, facecolors='none', edgecolors='green', linewidth=1.5)\n\n# Plotting the y=x line with green dotted line style\nax.plot(x, y, 'g--')\n\n# Setting x and y axis limits\nax.set_xlim(0, 80)\nax.set_ylim(0, 80)\n\n# Setting x and y labels\nax.set_xlabel('Estimated latency reduction (%)')\nax.set_ylabel('Evaluated latency reduction (%)')\n\n# Adding the R^2 value\nax.text(50, 10, r'$R^2 = 0.99864$', fontsize=12, color='green')\n\n# Save the figure as a png file with 300 dpi\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data - Replace these with the actual data from the paper\nx_data = np.linspace(0, 80, 100)  # Estimated latency reduction\ny_data = x_data * 0.99 + np.random.rand(100) * 4  # Sample evaluated latency reduction\n\n# Create the plot\nplt.figure(figsize=(6, 5))\nplt.scatter(x_data, y_data, facecolors='none', edgecolors='g')  # Scatter plot with green circles\n\n# Fit a linear model to the data to get the R^2 value as shown in the figure\ncoefficients = np.polyfit(x_data, y_data, 1)\npolynomial = np.poly1d(coefficients)\nr_squared = np.corrcoef(x_data, y_data)[0, 1]**2\n\n# Plot the line of best fit\nplt.plot(x_data, polynomial(x_data), 'g--')\n\n# Add the text box with the R^2 value\nplt.text(60, 10, f'$R^2$ = {r_squared:.5f}', fontsize=12)\n\n# Label the axes\nplt.xlabel('Estimated latency reduction (%)')\nplt.ylabel('Evaluated latency reduction (%)')\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('latency_reduction_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2110.04869/latency_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2110.04869/tradeoff.png",
        "caption": "Comparing the parameter reduction-accuracy tradeoff and latency reduction-accuracy tradeoff of different pruning schemes. Latency estimated on RTX 2080 GPU. Model size compression rate and latency reduction rate are computed based on that of the DeiT-Base model respectively.",
        "source": "cvpr_3/2110.04869/5_appendix.tex",
        "arxiv_id": "cvpr_3/2110.04869",
        "llava_output": "The image is a Line Chart that presents the experiment results of different pruning schemes. It shows the parameter reduction-accuracy tradeoff and latency reduction-accuracy tradeoff for each pruning scheme. The chart helps to compare the performance of the pruning schemes and their impact on the model size,",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data points based on the graph provided\n# Assuming these are the correct points, you may need to adjust them to match the exact values\ndeit_x = [0, 10, 20, 30, 40, 50, 60]\ndeit_y = [83, 82.75, 82.5, 82.25, 82, 81.75, 81.5]\n\nours_x = [0, 10, 20, 30, 40, 50, 60]\nours_y = [83.5, 83.25, 83, 82.75, 82.5, 82.25, 82]\n\n# Plotting DEIT line\nplt.plot(deit_x, deit_y, 'o-', label='No explicit head alignment', color='grey', linestyle='-', markerfacecolor='white')\n\n# Plotting 'Ours' line with explicit head alignment\nplt.plot(ours_x, ours_y, 'o-', label='Explicit head alignment (Ours)', color='green')\n\n# DEIT baseline (dashed line)\nplt.plot(deit_x, deit_y, 'b--', label='DEIT', linewidth=1)\n\n# Labels and Title\nplt.xlabel('Latency reduction (%)')\nplt.ylabel('Top-1 accuracy (%)')\nplt.title('Parameter reduction-accuracy tradeoff and latency reduction-accuracy tradeoff')\n\n# Legend\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('tradeoff_plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some dummy data\nlatency_reduction = np.linspace(0, 60, 5)\naccuracy_no_head_alignment = np.linspace(83.3, 81.7, 5)  # Dummy data for 'No explicit head alignment'\naccuracy_explicit_head_alignment = np.linspace(83.1, 81.5, 5)  # Dummy data for 'Explicit head alignment (Ours)'\n\n# Create the plot\nfig, ax = plt.subplots()\n\n# Plot the 'No explicit head alignment' line\nax.plot(latency_reduction, accuracy_no_head_alignment, label='No explicit head alignment', color='gray', marker='o', linestyle='-', linewidth=2)\n\n# Plot the 'Explicit head alignment (Ours)' line\nax.plot(latency_reduction, accuracy_explicit_head_alignment, label='Explicit head alignment (Ours)', color='green', marker='o', linestyle='-', linewidth=2)\n\n# Plot the 'DEIT' reference line\nax.axhline(y=83, color='blue', linestyle='--', linewidth=2, label='DEIT')\n\n# Set the axes labels\nax.set_xlabel('Latency reduction (%)')\nax.set_ylabel('Top-1 accuracy (%)')\n\n# Set the axes limits\nax.set_xlim(0, 60)\nax.set_ylim(81, 83.5)\n\n# Add a legend with a shadow\nlegend = ax.legend(loc='lower left', shadow=True)\n\n# Set the grid\nax.grid(True)\n\n# Save the figure\nplt.savefig('parameter_latency_tradeoff.png', dpi=300)\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2110.04869/tradeoff_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2110.04869/sensitivity.png",
        "caption": "Hessian importance score vs. squared loss difference.",
        "source": "cvpr_3/2110.04869/5_appendix.tex",
        "arxiv_id": "cvpr_3/2110.04869",
        "llava_output": "The image is a scatter plot showing the Hessian importance score on the y-axis and the squared loss difference on the x-axis.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data generation\nnp.random.seed(0)  # For reproducibility\nhessian_sensitivity = np.random.gamma(2., 0.5, 100)  # Example distribution\nsquared_loss_difference = hessian_sensitivity * np.random.normal(loc=0.1, scale=0.05, size=hessian_sensitivity.size)\n\n# Scatter plot\nplt.scatter(hessian_sensitivity, squared_loss_difference)\n\n# Adding title and labels\nplt.xlabel('Hessian sensitivity')\nplt.ylabel('Squared loss difference')\n\n# Set the same limits as the provided plot\nplt.xlim(0, 5)\nplt.ylim(0, 0.4)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('scatter_plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data (replace this with your actual data)\nnp.random.seed(0)\nx = np.random.rand(50) * 5\ny = x**2 * np.random.rand(50) * 0.1\n\n# Create the scatter plot\nplt.scatter(x, y, alpha=0.6, edgecolors=\"w\", s=100)\n\n# Set the labels for the axes\nplt.xlabel('Hessian sensitivity')\nplt.ylabel('Squared loss difference')\n\n# Remove the top and right spines (borders)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('hessian_importance_vs_squared_loss_difference.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2110.04869/sensitivity_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2202.06312/PR_PBE.png",
        "caption": "The progress of clean-image identification with respect to the increase of iterations.",
        "source": "cvpr_3/2202.06312/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2202.06312",
        "llava_output": "The provided image is a scatter plot that shows the progress of clean-image identification with respect to the increase of iterations.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample precision and recall data\n# Replace these with your actual data\nprecision = np.array([\n    np.linspace(0.9, 0.1, 100),\n    np.linspace(0.92, 0.2, 100),\n    np.linspace(0.94, 0.3, 100),\n    np.linspace(0.96, 0.4, 100),\n    np.linspace(0.98, 0.5, 100),\n    np.linspace(1.0, 0.6, 100),\n    np.linspace(0.95, 0.7, 100),\n    np.linspace(0.93, 0.8, 100)\n])\n\nrecall = np.linspace(0, 1, 100)\n\n# Plotting each iteration\nfor i, p in enumerate(precision, start=1):\n    plt.plot(recall, p, label=f'iter{i}')\n\n# Adding title and labels\nplt.title('Precision/Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\n# Adding legend\nplt.legend(loc='best')\n\n# Saving the figure with 300 DPI\nplt.savefig('precision_recall_curve.png', dpi=300)\n\n# Showing the plot (if necessary)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# This is placeholder data. Replace this with your actual data points.\nrecall = np.linspace(0, 1, 100)  # An array of recall values from 0 to 1\nprecisions = [np.random.rand(100) for _ in range(8)]  # Placeholder for precision values for each iteration\n\n# Ensure the precisions are sorted in descending order for visualization purpose\nfor precision in precisions:\n    precision[::-1].sort()\n\n# Plot setup\nplt.figure(figsize=(10, 6))\ncolors = ['blue', 'orange', 'green', 'red', 'purple', 'brown', 'pink', 'gray']\nlabels = ['iter1', 'iter2', 'iter3', 'iter4', 'iter5', 'iter6', 'iter7', 'iter8']\n\n# Plot each precision-recall curve\nfor precision, color, label in zip(precisions, colors, labels):\n    plt.plot(recall, precision, label=label, color=color)\n\n# Decorate the plot\nplt.title('Precision/Recall Curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()  # Show legend\nplt.grid(True)  # Optional: turn on the grid\n\n# Save the figure as a PNG file\nplt.savefig('precision_recall_curve.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2202.06312/PR_PBE_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2205.04437/Pretrain.png",
        "caption": "Quantitative comparison on PSNR(dB) of four different networks without and with the same-task pre-training on $\\times$4 SR.",
        "source": "cvpr_3/2205.04437/ArxivVersion.tex",
        "arxiv_id": "cvpr_3/2205.04437",
        "llava_output": "The image is a line chart that displays the PSNR (dB) values of four different networks. The chart shows the performance of the networks without pre-training and with the same-task pre-training on $\\times$4 SR. The line chart is used to quantitatively compare the results of the experiments",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Replace 'network1', 'network2', etc., with the actual names of the networks.\nnetworks = ['Network1', 'Network2', 'Network3', 'Network4']\n\n# These are just example values. Replace them with the actual PSNR values from the paper.\nvalues_without_pretraining = [26, 27, 28, 29]  # PSNR without pre-training\nvalues_with_pretraining = [30, 31, 32, 33]  # PSNR with pre-training\n\nx = np.arange(len(networks))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 5))\nrects1 = ax.bar(x - width/2, values_without_pretraining, width, label='Without Pre-training')\nrects2 = ax.bar(x + width/2, values_with_pretraining, width, label='With Pre-training')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel('Networks')\nax.set_ylabel('PSNR(dB)')\nax.set_title('Quantitative comparison on PSNR(dB) of four different networks\\nwithout and with the same-task pre-training on x4 SR')\nax.set_xticks(x)\nax.set_xticklabels(networks)\nax.legend()\n\n# Adding the error bars which appear as small horizontal lines on the top of the bars.\n# Assuming some example error values, replace them with actual error margins.\nerrors = [0.5, 0.4, 0.6, 0.3]  # example error margins\nfor i in range(len(x)):\n    ax.errorbar(x[i] - width/2, values_without_pretraining[i], yerr=errors[i], fmt='none', color='black', capsize=5)\n    ax.errorbar(x[i] + width/2, values_with_pretraining[i], yerr=errors[i], fmt='none', color='black', capsize=5)\n\n# Function to attach a text label above each bar, showing its height.\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nfig.tight_layout()\n\n# Save the figure. The dpi=300 makes it high resolution.\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot.\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n# Placeholder data for PSNR(dB) values\nnetworks = ['Net A', 'Net B', 'Net C', 'Net D']\nvalues_without_pretraining = [20, 25, 23, 28]  # Example data\nvalues_with_pretraining = [22, 27, 25, 30]     # Example data\n\n# Placeholder data for error bar lengths\nerrors = [1, 1.5, 1, 2]  # Example data\n# Specify the width of the bars and locations on the x-axis\nbar_width = 0.35\nindex = np.arange(len(networks))\n\n# Create the bars for \"without pretraining\"\nplt.bar(index, values_without_pretraining, bar_width, label='Without Pretraining', color='blue', yerr=errors, capsize=5)\n\n# Create the bars for \"with pretraining\"\nplt.bar(index + bar_width, values_with_pretraining, bar_width, label='With Pretraining', color='orange', yerr=errors, capsize=5)\n# Add x-ticks in the middle of the group bars\nplt.xticks(index + bar_width / 2, networks)\n\n# Remove the top and right spines\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add a legend, labels, and title as needed (placeholders used here)\nplt.xlabel('Networks')\nplt.ylabel('PSNR(dB)')\nplt.legend()\n# Save the figure with a high resolution\nplt.savefig('bar_chart.png', dpi=300)\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2205.04437/Pretrain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2207.01463/learning_efficiency.jpg",
        "caption": "AUROC vs epoch curve of cable category on the MVTecAD dataset.",
        "source": "cvpr_3/2207.01463/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2207.01463",
        "llava_output": "1. The image is a Line Chart that shows the AUROC-epoch curve of cable category on the MVTecAD dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example epoch data\nepochs = np.arange(0, 97, 1)\n\n# Example AUROC data - Replace these with the actual data points\nimage_auroc_wo_fas = np.random.uniform(low=0.5, high=1.0, size=len(epochs))  # Random data for illustration\npixel_auroc_wo_fas = np.random.uniform(low=0.5, high=1.0, size=len(epochs))  # Random data for illustration\nimage_auroc_w_fas = np.random.uniform(low=0.5, high=1.0, size=len(epochs))  # Random data for illustration\npixel_auroc_w_fas = np.random.uniform(low=0.5, high=1.0, size=len(epochs))  # Random data for illustration\n\n# Ensure the 'with FAS' data is generally higher than 'without FAS' for demonstration purposes\nimage_auroc_w_fas = np.clip(image_auroc_w_fas, a_min=image_auroc_wo_fas, a_max=None)\npixel_auroc_w_fas = np.clip(pixel_auroc_w_fas, a_min=pixel_auroc_wo_fas, a_max=None)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, image_auroc_wo_fas, label='Image AUROC w/o FAS', color='blue')\nplt.plot(epochs, pixel_auroc_wo_fas, label='Pixel AUROC w/o FAS', linestyle='--', color='red')\nplt.plot(epochs, image_auroc_w_fas, label='Image AUROC w FAS', color='blue', linestyle='--')\nplt.plot(epochs, pixel_auroc_w_fas, label='Pixel AUROC w FAS', linestyle='-', color='red')\n\n# Add labels and title\nplt.xlabel('Epoch')\nplt.ylabel('AUROC')\nplt.title('AUROC-Epoch curve of cable category on the MVTecAD dataset')\nplt.legend(loc='lower right')\n\n# Save the plot as a high-resolution PNG file\nplt.savefig('auroc_epoch_curve.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming that data is represented as numpy arrays, here's some dummy data.\nepochs = np.arange(0, 100, 5)  # Sample epochs from 0 to 95\nimage_auroc_wo_fas = np.clip(np.log(epochs+1), 0, 1)  # Dummy representation of an increasing trend\npixel_auroc_wo_fas = np.clip(np.log(epochs+1) - 0.1, 0, 1)\nimage_auroc_w_fas = np.clip(np.log(epochs+1) - 0.2, 0, 1)\npixel_auroc_w_fas = np.clip(np.log(epochs+1) - 0.3, 0, 1)\n\n# Now, let's replicate the plot from the image.\nplt.figure(figsize=(10, 8))  # Set figure size\n\n# Plot each line with the appropriate style\nplt.plot(epochs, image_auroc_wo_fas, 'b-')  # Solid blue line\nplt.plot(epochs, pixel_auroc_wo_fas, 'r-')  # Solid red line\nplt.plot(epochs, image_auroc_w_fas, 'b--')  # Dashed blue line\nplt.plot(epochs, pixel_auroc_w_fas, 'r--')  # Dashed red line\n\n# Add labels, title, and legend\nplt.xlabel('Epoch')\nplt.ylabel('AUROC')\nplt.title('AUROC-Epoch curve')\nplt.legend(['Image AUROC w/o FAS', 'Pixel AUROC w/o FAS', 'Image AUROC w FAS', 'Pixel AUROC w FAS'])\n\n# Set limit for x and y-axis\nplt.xlim(0, 96)\nplt.ylim(0.5, 1.0)\n\n# Use grid\nplt.grid(True)\n\n# Finally, save the plot as a PNG file with 300 DPI.\nplt.savefig('auroc_vs_epoch_curve.png', dpi=300)\nplt.show()  # Display the plot",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2207.01463/learning_efficiency_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2207.01463/other_dataset.jpg",
        "caption": "AUROC results on the AITEX, ELPV, BrainMRI, and HeadCT datasets.",
        "source": "cvpr_3/2207.01463/CVPR2023_ BGAD-Camera-Ready/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2207.01463",
        "llava_output": "The image is a line chart showing the AUROC results on the AITEX, ELPV, BrainMRI, and HeadCT datasets.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Names of the datasets\ndatasets = ['AITEX', 'ELPV', 'BrainMRI', 'HeadCT']\n\n# Example data for each method - replace with actual values\nKNN = [88.1, 85.4, 73.2, 82.7]\nDTree = [91.4, 86.4, 82.1, 89.1]\nSVM = [85.9, 87.5, 80.3, 83.9]\nNB = [93.0, 92.4, 81.1, 89.9]\nLR = [92.4, 91.3, 83.1, 86.3]\nMLP = [90.1, 89.2, 80.2, 87.1]\nDA = [92.3, 89.5, 78.9, 85.5]\nBDA_Onn = [94.2, 91.7, 83.3, 88.9]\n\n# Set position of bar on X axis\nr1 = np.arange(len(KNN))\nr2 = [x + 0.125 for x in r1]\nr3 = [x + 0.125 for x in r2]\nr4 = [x + 0.125 for x in r3]\nr5 = [x + 0.125 for x in r4]\nr6 = [x + 0.125 for x in r5]\nr7 = [x + 0.125 for x in r6]\nr8 = [x + 0.125 for x in r7]\n\n# Create the figure and the bars\nfig, ax = plt.subplots(figsize=(10, 5))\nbarWidth = 0.125\n\nbars1 = plt.bar(r1, KNN, color='blue', width=barWidth, edgecolor='grey', label='KNN')\nbars2 = plt.bar(r2, DTree, color='orange', width=barWidth, edgecolor='grey', label='DTree')\nbars3 = plt.bar(r3, SVM, color='green', width=barWidth, edgecolor='grey', label='SVM')\nbars4 = plt.bar(r4, NB, color='red', width=barWidth, edgecolor='grey', label='NB')\nbars5 = plt.bar(r5, LR, color='purple', width=barWidth, edgecolor='grey', label='LR')\nbars6 = plt.bar(r6, MLP, color='brown', width=barWidth, edgecolor='grey', label='MLP')\nbars7 = plt.bar(r7, DA, color='pink', width=barWidth, edgecolor='grey', label='DA')\nbars8 = plt.bar(r8, BDA_Onn, color='grey', width=barWidth, edgecolor='grey', label='BDA_Onn')\n\n# Add the data values on top of the bars\nfor bars in [bars1, bars2, bars3, bars4, bars5, bars6, bars7, bars8]:\n    for bar in bars:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval,1), va='bottom', ha='center', fontsize=8)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nplt.xlabel('Datasets')\nplt.ylabel('AUROC')\nplt.title('AUROC results on the AITEX, ELPV, BrainMRI, and HeadCT datasets')\nplt.xticks([r + 0.35 for r in range(len(KNN))], datasets)\nplt.legend()\n\n# Save the plot as a high-resolution PNG file\nplt.savefig('auroc_results.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data arrays with the AUROC values for each method and dataset.\ndata = {\n    'KDAD': [57.6, 86.4, 74.4, 79.6],\n    'DevNet': [59.8, 83.2, 73.3, 74.2],\n    'FLOS': [53.8, 51.4, 69.4, 59.8],\n    'SAOE': [56.4, 45.7, 69.3, 59.7],\n    'MLEP': [67.5, 63.5, 63.2, 69.8],\n    'DRA': [69.2, 57.8, 53.1, 69.8],\n    'NFAD': [69.2, 74.4, 69.3, 75.8],\n    'BGAD': [86.4, 90.3, 86.4, 80.7], # Ours\n}\n\ndatasets = ['AITEX', 'ELPV', 'BrainMRI', 'HeadCT']\nmethods = list(data.keys())\n\n# Colors for the bars\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']  # Placeholder, adjust accordingly\n\n# Number of bars (methods) per group (dataset)\nn_bars = len(methods)\n\n# The x position of bars\nbar_width = 0.1  # Adjust as needed to fit the figure\nx_indexes = np.arange(len(datasets))\n\n# Setup figure and axis\nfig, ax = plt.subplots(figsize=(20, 8))  # Adjust the figure size as needed\n\n# Create bars for each method\nfor i, method in enumerate(methods):\n    auroc_scores = data[method]\n    xpos = x_indexes + i * bar_width\n    bars = ax.bar(xpos, auroc_scores, width=bar_width, label=method, color=colors[i])\n\n    # Add the data labels on the bars\n    for bar in bars:\n        yval = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2.0, yval, round(yval, 1), va='bottom')  # Adjust positioning if needed\n\n# Set the x-axis tick labels to the dataset names, adjusted to the middle of the group of bars\nax.set_xticks(x_indexes + bar_width * (n_bars / 2 - 0.5))\nax.set_xticklabels(datasets)\n\n# Add labels, title and a legend\nax.set_ylabel('AUROC %')\nax.set_title('AUROC results on the AITEX, ELPV, BrainMRI, and HeadCT datasets')\nax.legend()\n\n# Save the figure with 300dpi\nplt.savefig('bar_chart.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2207.01463/other_dataset_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2207.01463/learning_efficiency.jpg",
        "caption": "AUROC vs epoch curve of cable category on the MVTecAD dataset.",
        "source": "cvpr_3/2207.01463/CVPR2023_ BGAD-Camera-Ready/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2207.01463",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data for epochs\nepochs = np.linspace(0, 96, 100)  # Simulate epochs from 0 to 96\n\n# Placeholder data for AUROC values\n# Replace these with your actual data\nauroc_image_wo_fas = np.clip(np.tanh(0.1 * (epochs - 20)), 0.5, 1)\nauroc_pixel_wo_fas = np.clip(np.tanh(0.08 * (epochs - 20)) + 0.1, 0.5, 1)\nauroc_image_w_fas = np.clip(auroc_image_wo_fas + 0.05, 0.5, 1)\nauroc_pixel_w_fas = np.clip(auroc_pixel_wo_fas + 0.05, 0.5, 1)\n\n# Start plotting\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, auroc_image_wo_fas, label='Image AUROC w/o FAS', color='blue', linewidth=2)\nplt.plot(epochs, auroc_pixel_wo_fas, label='Pixel AUROC w/o FAS', color='red', linestyle='--', linewidth=2)\nplt.plot(epochs, auroc_image_w_fas, label='Image AUROC w FAS', color='blue', linestyle='--', linewidth=2)\nplt.plot(epochs, auroc_pixel_w_fas, label='Pixel AUROC w FAS', color='red', linewidth=2)\n\n# Add title and labels\nplt.title('AUROC-Epoch curve of cable category on the MVTecAD dataset')\nplt.xlabel('Epoch')\nplt.ylabel('AUROC')\n\n# Add grid, legend, and set the limits\nplt.grid(True)\nplt.legend()\nplt.ylim(0.5, 1.0)\n\n# Save the plot as a high-resolution PNG file\nplt.savefig('auroc_epoch_curve.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Mock data based on the general shape seen in the image\nepochs = np.arange(0, 100, 1)\nimage_auroc_wo_fas = np.clip(1 - np.exp(-epochs / 10), 0, 1)\npixel_auroc_wo_fas = np.clip(1 - np.exp(-epochs / 15), 0, 1)\nimage_auroc_w_fas = np.clip(0.5 - 0.5 * np.exp(-epochs / 20), 0, 1)\npixel_auroc_w_fas = np.clip(0.5 - 0.5 * np.exp(-epochs / 25), 0, 1)\n\n# Plotting the lines corresponding to each series\nplt.figure(figsize=(10, 7))\nplt.plot(epochs, image_auroc_wo_fas, color='blue', label='Image AUROC w/o FAS')\nplt.plot(epochs, pixel_auroc_wo_fas, color='red', label='Pixel AUROC w/o FAS')\nplt.plot(epochs, image_auroc_w_fas, 'b--', label='Image AUROC w FAS')  # Dashed line\nplt.plot(epochs, pixel_auroc_w_fas, 'r--', label='Pixel AUROC w FAS')  # Dashed line\n\n# Adding title and labels\nplt.title('AUROC-Epoch curve')\nplt.xlabel('Epoch')\nplt.ylabel('AUROC')\nplt.legend(loc='lower right')\n\n# Setting limits for clarity and grid for visual guidance\nplt.xlim(0, 96)\nplt.ylim(0.5, 1.0)\nplt.grid(True)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('auroc_epoch_curve.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2207.01463/learning_efficiency_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2209.03102/Err_Recall.png",
        "caption": "Mean error and recall of the virtual points computed with different numbers (K) of retrieved depths per seed.",
        "source": "cvpr_3/2209.03102/MSMD.tex",
        "arxiv_id": "cvpr_3/2209.03102",
        "llava_output": "The image is a line chart showing the mean error and recall of the virtual points computed with different numbers (K) of retrieved depths per seed.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the image\nk_values = [1, 2, 4, 6, 10]  # Number of retrieved depths (K)\nrecall = [49.93, 66.27, 75.96, 90.14, 94.26]  # Recall (%)\nmean_error = [2.46, 2.53, 2.52, 2.45, 2.74]  # Mean error (m)\n\n# Create the figure and the line plot for recall\nfig, ax1 = plt.subplots()\n\ncolor = 'tab:blue'\nax1.set_xlabel('Number of retrieved depths (K)')\nax1.set_ylabel('Recall (%)', color=color)\nax1.plot(k_values, recall, color=color, marker='o', linestyle='-', linewidth=2, markersize=8)\nax1.tick_params(axis='y', labelcolor=color)\nax1.set_ylim(45, 100)  # Adjust the y-axis limits to match the original plot\n\n# Create a twin Axes sharing the same x-axis for the mean error\nax2 = ax1.twinx()\n\ncolor = 'tab:red'\nax2.set_ylabel('Mean error (m)', color=color)\nax2.plot(k_values, mean_error, color=color, marker='^', linestyle='--', linewidth=2, markersize=8)\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim(2.3, 3.0)  # Adjust the y-axis limits to match the original plot\n\n# Annotate the data points\nfor i, txt in enumerate(recall):\n    ax1.annotate(f'{txt}', (k_values[i], recall[i] + 1), textcoords=\"offset points\", xytext=(0,5), ha='center', color=color)\n\nfor i, txt in enumerate(mean_error):\n    ax2.annotate(f'{txt}', (k_values[i], mean_error[i] - 0.05), textcoords=\"offset points\", xytext=(0,-15), ha='center', color=color)\n\n# Title and grid\nplt.title('Mean error and recall of the virtual points computed with different numbers (K) of retrieved depths per seed')\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Data from the image as described in the caption\nK_values = [1, 2, 4, 6, 10]  # Number of retrieved depths\nrecall = [49.93, 66.27, 75.96, 90.14, 94.26]  # Recall percentages\nmean_error = [2.46, 2.53, 2.52, 2.74, 2.45]  # Mean errors\n\n# Create a figure and a set of subplots\nfig, ax1 = plt.subplots()\n\n# Plot the Recall values\ncolor = 'tab:blue'\nax1.set_xlabel('Number of retrieved depths (K)')\nax1.set_ylabel('Recall (%)', color=color)\nline1 = ax1.plot(K_values, recall, color=color, marker='o', label='Recall (%)')\nax1.tick_params(axis='y', labelcolor=color)\nax1.set_ylim([50, 100])  # Assuming from the image provided\nax1.set_xscale('log')  # The x-axis is on a log scale\n\n# Annotating Recall values on the plot\nfor i, txt in enumerate(recall):\n    ax1.annotate(f'{txt}', (K_values[i], recall[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Creating the secondary y-axis for the Mean Error values \nax2 = ax1.twinx() \ncolor = 'tab:red'\nax2.set_ylabel('Mean error (m)', color=color)  \nline2 = ax2.plot(K_values, mean_error, color=color, linestyle='--', marker='^', label='Mean error (m)')\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylim([2.3, 3])  # Assuming from the image provided\n\n# Annotating Mean Error values on the plot\nfor i, txt in enumerate(mean_error):\n    ax2.annotate(f'{txt}', (K_values[i], mean_error[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Legend (assuming it's not needed here as per the image provided, but code is here if required)\n# lines = line1 + line2\n# labs = [l.get_label() for l in lines]\n# ax1.legend(lines, labs, loc=0)\n\n# Fine-tune figure; make subplots close to each other and hide x labels for all but bottom plot.\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2209.03102/Err_Recall_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2209.12152/CKA-Concat-Concat-Block.png",
        "caption": "CKA analysis on hidden representations of networks that employ three ways to combine long skip branches. We analyze the similarity between hidden representations after each transformer block in the same network.",
        "source": "cvpr_3/2209.12152/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2209.12152",
        "llava_output": "The image is a scatter plot that presents the experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assume we do not have the actual data, so let's create a random 13x13 matrix\ndata = np.random.rand(13, 13)\n\n# Create the heatmap using imshow\nplt.figure(figsize=(6, 4.5))  # Adjust the figure size as needed\nplt.imshow(data, cmap='coolwarm', aspect='auto')  # 'coolwarm' is a similar colormap\n\n# Add a colorbar to the side\ncbar = plt.colorbar()\ncbar.set_label('Value')\n\n# Set the range of the colorbar to match the image\ncbar.set_ticks(np.arange(0.3, 1.1, 0.1))\n\n# Set the aspect of the plot to 'auto' so that the squares become rectangles, matching the original figure\nplt.gca().set_aspect('auto')\n\n# Turn off the grid lines\nplt.grid(False)\n\n# Set the tick labels\nplt.xticks(ticks=np.arange(0, 13), labels=np.arange(0, 13))\nplt.yticks(ticks=np.arange(0, 13), labels=np.arange(0, 13))\n\n# Save the figure\nplt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Generate random data to simulate the heatmap matrix\n# In your case, you should use the actual data from the CKA analysis\ndata = np.random.rand(13, 13)\n\n# Step 2: Create the heatmap using matplotlib\nplt.figure(figsize=(8,6))\nheatmap = plt.imshow(data, cmap='viridis', aspect='equal')\nplt.colorbar(heatmap, fraction=0.046, pad=0.04)\n\n# Optionally, customize ticks and labels\nplt.xticks(range(data.shape[1]))\nplt.yticks(range(data.shape[0]))\n\n# Step 3: Save the heatmap as a PNG file with 300 DPI\nplt.savefig('cka_analysis_heatmap.png', dpi=300)\nplt.show()",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2209.12152/CKA-Concat-Concat-Block_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2210.01115/eurosat_dist.png",
        "caption": "Eurosat; Ours ($0.516$) vs CoOp ($0.491$)",
        "source": "cvpr_3/2210.01115/main.tex",
        "arxiv_id": "cvpr_3/2210.01115",
        "llava_output": "1. The image is a scatter plot.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data to simulate the heatmaps\n# Assuming the heatmap is 10x10\ndata1 = np.random.rand(10, 10)\ndata2 = np.random.rand(10, 10)\n\n# Set up the figure and axes\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw={'wspace': 0.5})\n\n# Heatmap for \"Ours\"\nim1 = ax1.imshow(data1, cmap='coolwarm', aspect='equal')\nax1.set_title('Ours')\nax1.grid(False)  # Turn off the grid\n# Hide the ticks\nax1.set_xticks([])\nax1.set_yticks([])\n\n# Heatmap for \"CoOp\"\nim2 = ax2.imshow(data2, cmap='coolwarm', aspect='equal')\nax2.set_title('CoOp')\nax2.grid(False)  # Turn off the grid\n# Hide the ticks\nax2.set_xticks([])\nax2.set_yticks([])\n\n# Save the figure\nplt.savefig('heatmap_comparison.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming that the values are between -1 and 1, where negative is red, and positive is blue.\n# Generate some dummy data: two 10x10 arrays with values between -1 and 1\ndata_ours = np.random.rand(10, 10) * 2 - 1\ndata_coop = np.random.rand(10, 10) * 2 - 1\n\n# Function to create the plots\ndef create_subplot(ax, data, title):\n    ax.matshow(data, cmap='RdBu', vmin=-1, vmax=1)\n    ax.set_title(title, pad=20, fontsize=16)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.grid(which='both', color='lightgrey', linestyle='-', linewidth=1)\n\n# Create a figure with subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Create the plots\ncreate_subplot(ax1, data_ours, 'Ours ($0.516$)')\ncreate_subplot(ax2, data_coop, 'CoOp ($0.491$)')\n\n# Adjust layout\nplt.subplots_adjust(wspace=0.1)\n\n# Save the figure\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot if desired\nplt.show()",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2210.01115/eurosat_dist_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2211.06891/Teaser.png",
        "caption": "\\small Comparison of PSNR-Parameters with previous HSI reconstruction methods. The PSNR (in dB) is plotted on the vertical axis, while memory cost parameters are represented on the horizontal axis. Our proposed Residual Degradation Learning Unfolding Framework with Mixing priors across Spatial and Spectral (RDLUF-Mix$S^2$) Transformers outperforms previous methods while requiring fewer parameters.",
        "source": "cvpr_3/2211.06891/01_intro.tex",
        "arxiv_id": "cvpr_3/2211.06891",
        "llava_output": "1. The image is a line chart showing the comparison of PSNR-Parameters with previous HSI reconstruction methods.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Example data points (you should use actual data here)\nmethods = ['Ours 9stage', 'Ours 7stage', 'Ours 5stage', 'Ours 3stage', 'CST-M', 'CST-L', 'HDNet', 'DGSMP', 'DNU']\nx_values = [2, 2.5, 3, 3.5, 1.5, 2.2, 4, 4.5, 1]  # Number of parameters in millions\ny_values = [39, 37, 36, 35, 33, 32, 34, 31, 30]  # PSNR in dB\nsizes = [200, 150, 150, 150, 100, 100, 100, 100, 100]  # Size of markers\ncolors = ['red', 'red', 'red', 'red', 'green', 'green', 'purple', 'orange', 'blue']  # Colors of the markers\nshapes = ['*', 'o', 'o', 'o', 's', '^', 'D', 'p', 'h']  # Shapes of the markers\n\n# Plotting the figure\nplt.figure(figsize=(10, 8))\nfor method, x, y, size, color, shape in zip(methods, x_values, y_values, sizes, colors, shapes):\n    plt.scatter(x, y, s=size, c=color, label=method, marker=shape)\n\n# Annotating each point with the method name\nfor i, method in enumerate(methods):\n    plt.annotate(method, (x_values[i], y_values[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Set the axes labels\nplt.xlabel('Numbers of parameters (Millions)')\nplt.ylabel('PSNR (dB)')\n\n# Set title and grid\nplt.title('Comparison of PSNR-Parameters with previous HSI reconstruction methods')\nplt.grid(True)\n\n# Save the figure\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot\nplt.legend()\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Example data (replace with your actual data points)\nmethods = ['DNU', 'MST-S', 'MST-M', 'MST-L', 'CST-S', 'CST-M', 'CST-L', 'CST-L*', 'HDNet', 'DGSMP', 'DAHST', 'Ours 3stage', 'Ours 5stage', 'Ours 7stage', 'Ours 9stage']\npsnr_values = [31, 34, 35, 36, 34.5, 35.5, 36.5, 37, 37, 33, 38, 37, 38, 38.5, 39]  # PSNR values\nmemory_params = [1, 2, 2.5, 3, 2.2, 2.7, 3.5, 4, 2.7, 3.8, 4.5, 2, 2.5, 3, 3.5]  # Memory cost in millions\n\n# Example style data (colors and markers should be replaced with corresponding ones from your plot)\ncolors = ['blue', 'green', 'green', 'green', 'red', 'red', 'red', 'pink', 'purple', 'orange', 'brown', 'red', 'red', 'red', 'red']\nmarkers = ['hexagon', 'triangle', 'square', 'diamond', 'pin', 'clover', 'thin_diamond', 'star', 'circle', 'pentagon', 'hexagon2', 'star', 'star', 'star', 'star']\n\n# Create the plot\nplt.figure(figsize=(10, 8))\n\n# Plot each method\nfor method, psnr, mem, color, marker in zip(methods, psnr_values, memory_params, colors, markers):\n    plt.scatter(mem, psnr, label=method, color=color, marker=marker, s=100)\n\nplt.xlabel('Numbers of parameters(Millions)')\nplt.ylabel('PSNR (dB)')\nplt.title('Comparison of PSNR-Parameters with previous HSI reconstruction methods')\n\n# Add legend with custom handles because there will be multiple of the same marker/color\n# Normally, you would define your own legend. Here's an example with one entry:\ncustom_handles = [mpatches.Patch(color='red', label='Our Method')]\nplt.legend(handles=custom_handles)\n\nplt.xlim(0, 6)\nplt.ylim(30, 40)\nplt.grid(True)\n\n# Save the plot as PNG file with 300 DPI\nplt.savefig('comparison_plot.png', dpi=300)\n\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2211.06891/Teaser_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2211.10772/train_efficiency.jpg",
        "caption": "Comparison with open-source Transformer-based methods using only Total-Text training set.",
        "source": "cvpr_3/2211.10772/main.tex",
        "arxiv_id": "cvpr_3/2211.10772",
        "llava_output": "The image is a line chart that shows the end-to-end training time for different methods.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example Step (K) values\nsteps = np.array([0, 20, 40, 60, 80, 100, 120])\n\n# Example End-to-End None (%) values for three different methods\nspts_values = np.array([0, 10, 20, 30, 40, 50, 60])  # Replace with actual data\nswin_values = np.array([0, 15, 25, 35, 45, 55, 60])  # Replace with actual data\ntestr_values = np.array([0, 5, 15, 25, 35, 45, 55])  # Replace with actual data\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(steps, spts_values, marker='o', color='orange', linestyle='-', label='SPTS | Max Mem.: 11GB')\nplt.plot(steps, swin_values, marker='s', color='blue', linestyle='-', label='SwinTextSpotter | Max Mem.: 24GB (2 imgs / GPU)')\nplt.plot(steps, testr_values, marker='d', color='red', linestyle='-', label='TESTR | Max Mem.: 18GB (2 imgs / GPU)')\n\n# Highlight the last point for \"Ours\" method with different style\nours_last_point = (120, 40)  # Replace with actual last data point\nplt.scatter(ours_last_point[0], ours_last_point[1], color='yellow', s=100, label='Ours | Max Mem.: 17GB (2 imgs / GPU)', zorder=5)\n\n# Add grid, title, and legend\nplt.grid(True)\nplt.title('Comparison with open-source Transformer-based methods using only Total-Text training set')\nplt.legend()\n\n# Set the labels for the axes\nplt.xlabel('Step (K)')\nplt.ylabel('End-to-End None (%)')\n\n# Set the range for the axes\nplt.xlim(0, 120)\nplt.ylim(0, 60)\n\n# Save the figure\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Sample data\nsteps = [0, 20, 40, 60, 80, 100, 120]  # X-axis: Steps in thousands (K)\nour_model = [0, 15, 30, 55, 60, 65, 65]  # Y-axis: End-to-End (%)\ntestr = [0, 10, 25, 40, 45, 50, 50]\nswintextspotter = [0, 5, 20, 35, 35, 35, 35]\nspts = [0, 20, 50, 60, 60, 60, 60]\n\n# Plotting the lines\nplt.plot(steps, our_model, 'o-', label='Ours | Max Mem.: 17GB (2 imgs / GPU)', color='purple')\nplt.plot(steps, testr, 's-', label='TESTR | Max Mem.: 18GB (2 imgs / GPU)', color='red')\nplt.plot(steps, swintextspotter, 'D-', label='SwinTextSpotter | Max Mem.: 24GB (2 imgs / GPU)', color='cyan')\nplt.plot(steps, spts, '^-', label='SPTS | Max Mem.: 11GB', color='orange')\n\n# Add a legend\nplt.legend(loc='lower right')\n\n# Label x and y axes\nplt.xlabel('Step (K)')\nplt.ylabel('End-to-End (None) (%)')\n\n# Title\nplt.title('Comparison with open-source Transformer-based methods using only Total-Text training set')\n\n# Saving the plot as a .png file with 300 DPI\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2211.10772/train_efficiency_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2211.10772/noise_influence.jpg",
        "caption": "Analysis of the sensitivity to different line locations.",
        "source": "cvpr_3/2211.10772/main.tex",
        "arxiv_id": "cvpr_3/2211.10772",
        "llava_output": "2 Line Charts are provided to present the experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming these are the step values and corresponding end-to-end results for both plots.\n# Replace these with the actual values from your dataset.\nsteps = np.array([2, 3, 4, 5, 6])  # Replace with actual steps (K)\nend_to_end_none_left = {\n    'Normal': np.random.uniform(81, 82, len(steps)),\n    'Shift: 0-25%': np.random.uniform(80, 81, len(steps)),\n    'Shift: 25-50%': np.random.uniform(79, 80, len(steps)),\n    'Shift: 50-75%': np.random.uniform(78, 79, len(steps)),\n}\n\nend_to_end_none_right = {\n    'Normal': np.random.uniform(81, 82, len(steps)),\n    'Shrink: 0-10%': np.random.uniform(80, 81, len(steps)),\n    'Shrink: 10-20%': np.random.uniform(79, 80, len(steps)),\n    'Shrink: 20-30%': np.random.uniform(78, 79, len(steps)),\n}\n\n# Create figure and axes\nfig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n\n# Plot the left chart\nfor label, data in end_to_end_none_left.items():\n    axs[0].plot(steps, data, marker='o', label=label)\n    axs[0].fill_between(steps, data - np.random.rand(len(steps)), data + np.random.rand(len(steps)), alpha=0.2)\n\n# Plot the right chart\nfor label, data in end_to_end_none_right.items():\n    axs[1].plot(steps, data, marker='o', label=label)\n    axs[1].fill_between(steps, data - np.random.rand(len(steps)), data + np.random.rand(len(steps)), alpha=0.2)\n\n# Add legend, grid, labels, title etc.\naxs[0].set_title('Sensitivity to Line Shift')\naxs[1].set_title('Sensitivity to Line Shrink')\nfor ax in axs:\n    ax.set_xlabel('Step (K)')\n    ax.set_ylabel('End-to-End (None) (%)')\n    ax.set_xticks(steps)\n    ax.grid(True)\n    ax.legend()\n\nplt.tight_layout()\n\n# Save the plot as a PNG file\nplt.savefig('sensitivity_analysis.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n# Example data points\nx = np.array([2, 4, 6])\n# Example means for lines\ny = np.array([[80, 82, 83],\n              [81, 83, 84],\n              [79, 81, 82],\n              [78, 80, 81]])\n# Example confidence intervals around each line\nyerr = np.array([[1.5, 1.2, 1],\n                 [1, 0.8, 0.7],\n                 [1.8, 1.5, 1.2],\n                 [2, 1.7, 1.3]])\n# Create a figure with specified size\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n# Define colors and markers\ncolors = ['cyan', 'red', 'orange', 'purple']\nmarkers = ['s', 'D', 'o', 'v']\n\nfor i, ax in enumerate(axes):\n    for j in range(y.shape[0]):\n        ax.plot(x, y[j], marker=markers[j], color=colors[j])\n        ax.fill_between(x, y[j] - yerr[j], y[j] + yerr[j], color=colors[j], alpha=0.2)\n\n    # Set the labels and titles\n    ax.set_xlabel('Step (K)')\n    ax.set_ylabel('End-to-End (%)')\n    ax.set_title('Analysis of the sensitivity to different line locations')\n\n    # Configure legend\n    ax.legend(['Normal', 'Shrink: 0-25%', 'Shrink: 25-50%', 'Shrink: 50-75%'], loc='lower right')\n\n# Adjust layout\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2211.10772/noise_influence_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2211.16056/noisy_improve_distribution-qkv.png",
        "caption": "Output histogram in selected transformer layers before and after activation quantization. NoisyQuant significantly reduces the output quantization error over EasyQuant baselines.",
        "source": "cvpr_3/2211.16056/4_ablation.tex",
        "arxiv_id": "cvpr_3/2211.16056",
        "llava_output": "The image is a histogram showing the output of a transformer layer before and after activation quantization.</s>",
        "is_data_visual": null,
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some example data\nnp.random.seed(0)  # For reproducibility\nfp_data = np.random.normal(0, 1, 1000)\neasyquant_data = np.random.normal(0, 1, 1000) * 0.9  # Just as an example\nnoisyquant_data = np.random.normal(0, 1, 1000) * 1.1  # Just as an example\n\n# Create figure and axes\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the left chart (EasyQuant)\naxs[0].hist(fp_data, bins=30, alpha=0.7, label='FP')\naxs[0].hist(easyquant_data, bins=30, alpha=0.7, label='EasyQuant')\naxs[0].set_title('QE=6.5e-02')\naxs[0].text(0.5, 0.9, '-26% QE', transform=axs[0].transAxes, fontsize=14, color='red', ha='center')\n\n# Plot the right chart (NoisyQuant)\naxs[1].hist(fp_data, bins=30, alpha=0.7, label='FP')\naxs[1].hist(noisyquant_data, bins=30, alpha=0.7, label='NoisyQuant')\naxs[1].set_title('QE=4.8e-02')\n\n# Set common properties and layout\nfor ax in axs:\n    ax.set_yscale('log')\n    ax.set_xlabel('Layer Output')\n    ax.set_ylabel('Histogram')\n    ax.legend()\n\nplt.tight_layout()\n\n# Save the plot as a PNG file\nplt.savefig('output_histogram.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2211.16056/noisy_improve_distribution-qkv_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.06200/complexity.png",
        "caption": "Distribution of complexity scores among reviewed clips. The majority of the transformations in the wild are not challenging but there is still a sufficient number of clips in the target 4-5 range.",
        "source": "cvpr_3/2212.06200/supplementary.tex",
        "arxiv_id": "cvpr_3/2212.06200",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data for the complexity scores\ncomplexity_scores = [1, 2, 3, 4, 5]\nnumber_of_clips = [1500, 1750, 1250, 1000, 500]\n\n# Create the bar plot\nplt.figure(figsize=(8, 6))  # Size of the figure\nplt.bar(complexity_scores, number_of_clips, color=['blue', 'orange', 'green', 'red', 'purple'])\n\n# Add title and labels\nplt.title('Distribution of complexity scores among reviewed clips')\nplt.xlabel('Complexity score')\nplt.ylabel('# of clips')\n\n# Set the limit for the y-axis\nplt.ylim(0, 1800)\n\n# Optionally, you can format the x-axis to show only integers\nplt.xticks(complexity_scores)\n\n# Save the figure with 300 dpi\nplt.savefig('complexity_distribution.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Complexity scores and the corresponding number of videos\n# Note: You will need to fill in these values based on the actual data from the figure\ncomplexity_scores = [1, 2, 3, 4, 5]\nnumber_of_videos = [1000, 1500, 1250, 750, 250]  # Replace these with the actual counts from the image\n\n# Create the bar chart\nplt.bar(complexity_scores, number_of_videos, color=['blue', 'orange', 'green', 'red', 'purple'])\n\n# Set the labels and title\nplt.xlabel('Complexity score')\nplt.ylabel('# of Videos')\nplt.title('Distribution of complexity scores among reviewed clips')\n\n# Save the figure with 300 DPI\nplt.savefig('complexity_distribution.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.06200/complexity_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.06200/lens.png",
        "caption": "Distribution of video lengths in VOST. The vast majority of the samples fall into the challenging 10-30 seconds range, and a significant number of the videos are even longer than that.",
        "source": "cvpr_3/2212.06200/supplementary.tex",
        "arxiv_id": "cvpr_3/2212.06200",
        "llava_output": "The image is a histogram showing the distribution of video lengths in VOST.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data\ncategories = ['< 10 sec.', '10-20 sec.', '20-30 sec.', '> 30 sec.']\nvalues = [50, 300, 200, 100]  # Replace these values with the actual counts from your data\n\n# Create the bar chart\nplt.figure(figsize=(10, 8))\nplt.bar(categories, values, color=['blue', 'orange', 'green', 'red'])\n\n# Title and labels\nplt.title('Distribution of video length')\nplt.xlabel('Video length')\nplt.ylabel('Number of videos')\n\n# Saving the figure with 300 DPI\nplt.savefig('distribution_of_video_length.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Data for video lengths as per the categories in the figure.\ncategories = ['< 10 sec.', '10-20 sec.', '20-30 sec.', '> 30 sec.']\nvideo_counts = [50, 300, 200, 100]\n\n# Colors for the bars as per the figure.\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n\n# Create bar plot\nplt.figure(figsize=(10, 8))  # adjust the size to fit your needs\nplt.bar(categories, video_counts, color=colors)\n\n# Add title and labels as per the figure.\nplt.title('Distribution of video length')\nplt.xlabel('Video length')\nplt.ylabel('# of videos')\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('video_length_distribution.png', dpi=300)\n\n# Optionally show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.06200/lens_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.06200/sizes.png",
        "caption": "Distribution of object sizes in VOST. Most of the objects are small due to the nature of first-person videos, but there is a significant long tail of larger objects, such as cars.",
        "source": "cvpr_3/2212.06200/supplementary.tex",
        "arxiv_id": "cvpr_3/2212.06200",
        "llava_output": "The image is a histogram, which shows the distribution of object sizes in VOST. The histogram consists of multiple bars, each representing a different size of objects. The bars are arranged in a way that shows the frequency of objects in each size category. The histogram is a useful visual representation of the distribution of",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data (example data, replace with your actual data)\nsizes = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # X-axis: Mask size relative to the size of the image\ncounts = [1e7, 1e6, 1e5, 4e4, 2e4, 1e4, 5e3, 2e3, 1e3, 500]  # Y-axis: Number of instances (log scale)\n\n# Create the bar chart\nplt.figure(figsize=(10, 8))\nplt.bar(sizes, counts, color=plt.cm.tab20c.colors)\n\n# Set the y-axis to logarithmic scale\nplt.yscale('log')\n\n# Title and labels\nplt.title('Distribution of object sizes')\nplt.xlabel('Mask size relative to the size of the image')\nplt.ylabel('Number of instances (log scale)')\n\n# Tick marks\nplt.xticks(sizes)\nplt.yticks(10**np.arange(0, 8))  # Adjust this depending on your actual y-axis data range\n\n# Saving the figure with 300 DPI\nplt.savefig('distribution_of_object_sizes.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 2: Generate data (This is an example and will not match the exact values from the image)\nx = np.arange(10, 110, 10)  # Mask sizes from 10 to 100\ny = np.array([1e5, 5e4, 2e4, 1e4, 5e3, 2e3, 1e3, 700, 500, 300])  # Estimated object count, logarithmic scale\n\n# Step 3: Create the bar plot\nplt.figure(figsize=(10, 6))\nbars = plt.bar(x, y, color=plt.cm.tab20(np.arange(len(x))))\n\n# Step 4: Set labels and title\nplt.xlabel('Mask size relative to the size of the image')\nplt.ylabel('# of masks (log scale)')\nplt.title('Distribution of object sizes')\n\n# Step 5: Set scales and ticks\nplt.yscale('log')\nplt.xticks(x)\nplt.yticks([10**i for i in range(2, 6)])  # Change depending on the actual scale of your dataset\n\n# Saving the plot with 300 DPI\nplt.savefig('distribution_of_object_sizes.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.06200/sizes_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.06200/speeds.png",
        "caption": "Distribution of object motion normalized by the object area in VOST. Most videos are smooth but there is a significant amount of challenging sequences with fast motion.",
        "source": "cvpr_3/2212.06200/supplementary.tex",
        "arxiv_id": "cvpr_3/2212.06200",
        "llava_output": "The image is a histogram that shows the distribution of object motion normalized by the object area in VOST.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (replace with your actual data)\nmotion_normalized_by_area = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0]\ninstances_log_scale = [1e7, 5e6, 1e6, 5e5, 1e5, 5e4, 2e4, 1e4, 5e3, 2e3, 1e3, 500, 300, 200, 100, 50, 30, 20, 10, 5]\n\n# Create the bar chart\nplt.figure(figsize=(10, 8))\nplt.bar(motion_normalized_by_area, instances_log_scale, color=plt.cm.Pastel1.colors, width=0.1)\n\n# Set the y-axis to logarithmic scale\nplt.yscale('log')\n\n# Set the scale of the x-axis to 0.2 increments\nplt.xticks(np.arange(0, max(motion_normalized_by_area) + 0.2, 0.2))\n\n# Title and labels\nplt.title('Distribution of object speed')\nplt.xlabel('Motion normalized by area')\nplt.ylabel('Number of instances (log scale)')\n\n# Saving the figure with 300 DPI\nplt.savefig('distribution_of_object_speed.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data\nmotion_normalized_by_area = np.arange(0.2, 4.2, 0.2)\nnumber_of_videos = np.random.rand(len(motion_normalized_by_area)) * 100  # Replace with actual data\n\n# Sort the data by motion to simulate the decreasing pattern seen in the plot\nnumber_of_videos_sorted = np.sort(number_of_videos)[::-1]\n\n# Create a bar chart\nplt.bar(motion_normalized_by_area, number_of_videos_sorted, color='skyblue', width=0.1)\n\n# Set the caption\nplt.title('Distribution of object motion normalized by the object area in VOST')\n\n# Set the scale of the y-axis to be logarithmic\nplt.yscale('log')\n\n# Set log-scale formatting\nplt.gca().yaxis.set_major_formatter(plt.FormatStrFormatter('%.0f'))\n\n# Set labels for the axes\nplt.xlabel('Motion normalized by area')\nplt.ylabel('# of videos (log scale)')\n\n# Save the plot as a PNG file with 300 dpi resolution\nplt.savefig('distribution_plot.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.06200/speeds_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.06344/segtime.png",
        "caption": "Our method produces segmentations in roughly linear time with respect to the \\# edges of the model.",
        "source": "cvpr_3/2212.06344/07_supplemental.tex",
        "arxiv_id": "cvpr_3/2212.06344",
        "llava_output": "The image is a line chart showing the time it takes to segment a model in linear time with respect to the number of edges.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming we have the x and y data\n# Since the exact data points are not provided, we'll generate similar looking data\nx = np.logspace(4, 6, num=50)  # Generates 50 points between 10^4 and 10^6\ny = x * 0.000001  # This is a placeholder for the actual relationship, adjusted to look linear on a log-log plot\n\nplt.figure()\nplt.plot(x, y)\n\n# Set the scale of both axes to logarithmic\nplt.xscale('log')\nplt.yscale('log')\n\n# Set the title and labels\nplt.title('DA Wand Segmentation Time')\nplt.xlabel('# Edges')\nplt.ylabel('Inference Time (s)')\n\n# Set the limits of the axes\nplt.xlim([10**4, 10**6])\nplt.ylim([10**-1, 10**1])\n\n# Save the plot with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot for verification\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical data\nedges = np.logspace(4, 6, 100)\ntime = edges * 1e-6  # This constant is arbitrary, just to create a linear relationship on a log scale\n\n# Create the plot\nplt.figure()\nplt.plot(edges, time)\n\n# Set the x and y axes to logarithmic scale\nplt.xscale('log')\nplt.yscale('log')\n\n# Labeling the axes and the title\nplt.xlabel('# Edges')\nplt.ylabel('Inference Time (s)')\nplt.title('DA Wand Segmentation Time')\n\n# Saving the figure to a file with 300 DPI\nplt.savefig('segmentation_time.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.06344/segtime_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.07242/speed.png",
        "caption": "Inference speed in frames per second for our final model and two baseline architectures. Each point corresponds to one of the training garments. The speed was measured using NVIDIA GeForce RTX 3060 GPU",
        "source": "cvpr_3/2212.07242/101_experiments_suppmat.tex",
        "arxiv_id": "cvpr_3/2212.07242",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data to represent the points on the plot.\n# You should replace these lists with the actual data points.\nnodes = [2000, 4000, 6000, 8000, 10000, 12000]\nfps_fine15 = [10, 9, 8, 7, 6, 5]  # Replace with actual FPS values for Fine15\nfps_ours = [12, 11, 9, 8, 7, 6]  # Replace with actual FPS values for Ours\nfps_fine48 = [20, 18, 15, 13, 10, 9]  # Replace with actual FPS values for Fine48\n\n# Create a scatter plot\nplt.figure()\nplt.scatter(nodes, fps_fine15, color='blue', label='Fine15')\nplt.scatter(nodes, fps_ours, color='orange', label='Ours')\nplt.scatter(nodes, fps_fine48, color='green', label='Fine48')\n\n# Add a legend\nplt.legend()\n\n# Add title and labels\nplt.title('Inference Speed in Frames per Second')\nplt.xlabel('Number of the Garment Nodes')\nplt.ylabel('Frames per Second')\n\n# Save the plot as a PNG file with a resolution of 300 DPI\nplt.savefig('inference_speed.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Sample data - replace these with your actual data points\ndata_fine15 = [(3000, 10), (5000, 15), (7000, 20), (9000, 25)]\ndata_ours = [(3000, 5), (5000, 12), (7000, 18), (9000, 19)]\ndata_fine48 = [(3000, 2), (5000, 7), (7000, 12), (9000, 14)]\n\n# Unzipping the data into x and y values for each model\nx_fine15, y_fine15 = zip(*data_fine15)\nx_ours, y_ours = zip(*data_ours)\nx_fine48, y_fine48 = zip(*data_fine48)\n\n# Create the plot\nplt.figure()\nplt.scatter(x_fine15, y_fine15, color='blue', label='Fine15')\nplt.scatter(x_ours, y_ours, color='orange', label='Ours')\nplt.scatter(x_fine48, y_fine48, color='green', label='Fine48')\n\n# Labeling the axes\nplt.xlabel('number of the garment nodes')\nplt.ylabel('frames per second')\n\n# Add a legend\nplt.legend()\n\n# Save the plot as a PNG file\nplt.savefig('inference_speed.png', dpi=300)\n\n# Show the plot for verification\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.07242/speed_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.09069/new_synthetic.png",
        "caption": "Rate-distortion curves on the NeRF synthetic dataset. The numbers inside parenthesis denote the axis resolution of grids.",
        "source": "cvpr_3/2212.09069/true-arxiv.tex",
        "arxiv_id": "cvpr_3/2212.09069",
        "llava_output": "3. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data points (these should be replaced with the actual data)\nmethods = ['VM192 (300) + Ours', 'VM384 (500) + Ours', 'TensorRF (8bit)', 'NeRF (8bit)', 'PREF (8bit)', 'cNeRF (8bit)', 'ccNeRF (32bit)']\nsizes = [\n    [18],  # VM192\n    [12, 10, 8, 6],  # VM384\n    [7],  # TensorRF\n    [4],  # NeRF\n    [3],  # PREF\n    [2.5],  # cNeRF\n    [1]  # ccNeRF\n]\npsnrs = [\n    [33],  # VM192\n    [32.5, 32.2, 32, 31.8],  # VM384\n    [32],  # TensorRF\n    [31.5],  # NeRF\n    [31],  # PREF\n    [30.7],  # cNeRF\n    [30.5]  # ccNeRF\n]\n\ncolors = ['blue', 'brown', 'green', 'red', 'purple', 'orange', 'pink']\nmarkers = ['^', '*', '>', 'o', 's', 'x', 'D']\n\n# Plot each method's data points\nfor method, size, psnr, color, marker in zip(methods, sizes, psnrs, colors, markers):\n    plt.scatter(size, psnr, color=color, label=method, marker=marker)\n\n# Add the legend outside the plot\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Set the labels and title\nplt.xlabel('Size (MB)')\nplt.ylabel('PSNR')\nplt.title('Rate-distortion curves on the NeRF synthetic dataset')\n\n# Set the range of the axes if necessary\nplt.xlim(0, 20)\nplt.ylim(30.5, 33.5)\n\n# Adjust layout to prevent the legend from covering the plot\nplt.tight_layout()\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('plot.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Placeholder data points, these should be replaced with actual data\nvm192_x = [2, 3, 5, 7, 9]  # Size in MB\nvm192_y = [31, 32, 32.5, 33, 33.2]  # PSNR\nvm384_x = [1, 2, 3, 4, 6]  # Size in MB\nvm384_y = [30.5, 31, 31.5, 32, 32.5]  # PSNR\nother_methods_x = [4,  6, 8, 10, 12]  # Size in MB\nother_methods_y = [30.5, 31, 31.5, 32, 32.5]  # PSNR (random values)\n\n# Plot the VM192 and VM384 rate-distortion curves\nplt.plot(vm192_x, vm192_y, marker='*', color='brown', label='VM192 (300) + Ours')\nplt.plot(vm384_x, vm384_y, marker='*', color='green', label='VM384 (500) + Ours')\n\n# Plot other methods as individual points (placeholder markers and colors)\nplt.scatter([other_methods_x[0]], [other_methods_y[0]], color='cyan', label='TensorRF (8bit)', marker='^') \nplt.scatter([other_methods_x[1]], [other_methods_y[1]], color='red', label='NeRF (8bit)', marker='o')\nplt.scatter([other_methods_x[2]], [other_methods_y[2]], color='purple', label='PREF (8bit)', marker='s')\nplt.scatter([other_methods_x[3]], [other_methods_y[3]], color='orange', label='cNeRF (8bit)', marker='x')\nplt.scatter([other_methods_x[4]], [other_methods_y[4]], color='pink', label='ccNeRF (32bit)', marker='D')\n\n# Annotate specific points with text, using placeholders\nplt.text(vm192_x[-1], vm192_y[-1], 'VM192', fontsize=9, ha='left')\nplt.text(vm384_x[-1], vm384_y[-1], 'VM48', fontsize=9, ha='left')\n\n# Set the labels and title\nplt.xlabel('Size (MB)')\nplt.ylabel('PSNR')\nplt.title('Rate-distortion curves on the NeRF synthetic dataset')\n\n# Add grid, legend, and set limits if necessary\nplt.grid(True)\nplt.legend()\n\n# Save the plot as a PNG file\nplt.savefig('rate_distortion_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.09069/new_synthetic_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.09069/fig_level.png",
        "caption": "The rate distortion curves of different signal representation schemes (spatial, DWT, and DCT). Sparsity on the x axis refers to the ratio of zeros in grid parameters. The grid sparsity was controlled by $\\lambda_m$. The numbers inside the parenthesis indicate the levels of the wavelet transform.",
        "source": "cvpr_3/2212.09069/true-arxiv.tex",
        "arxiv_id": "cvpr_3/2212.09069",
        "llava_output": "3. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data (you should replace these with your actual data points)\nsparsity = [0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98]\npsnr_dwt1 = [32.1, 32.05, 32.0, 31.95, 31.9, 31.85, 31.8, 31.75, 31.7]\npsnr_dwt2 = [32.15, 32.1, 32.05, 32.0, 31.95, 31.9, 31.85, 31.8, 31.75]\npsnr_dwt3 = [32.0, 31.95, 31.9, 31.85, 31.8, 31.75, 31.7, 31.65, 31.6]\npsnr_dwt4 = [31.9, 31.85, 31.8, 31.75, 31.7, 31.65, 31.6, 31.55, 31.5]\npsnr_dct = [31.8, 31.75, 31.7, 31.65, 31.6, 31.55, 31.5, 31.45, 31.4]\npsnr_spatial = [32.2, 32.15, 32.1, 32.05, 32.0, 31.95, 31.9, 31.85, 31.8]\n\n# Plot lines with markers\nplt.plot(sparsity, psnr_dwt1, label='DWT (1-level)', marker='o', linestyle='--', color='blue')\nplt.plot(sparsity, psnr_dwt2, label='DWT (2-level)', marker='o', linestyle='--', color='orange')\nplt.plot(sparsity, psnr_dwt3, label='DWT (3-level)', marker='o', linestyle='--', color='green')\nplt.plot(sparsity, psnr_dwt4, label='DWT (4-level)', marker='o', linestyle='--', color='red')\nplt.plot(sparsity, psnr_dct, label='DCT', marker='v', linestyle=':', color='purple')\nplt.plot(sparsity, psnr_spatial, label='Spatial', marker='x', linestyle='-', color='black')\n\n# Add title and labels\nplt.title('The rate distortion curves of different signal representation schemes')\nplt.xlabel('Sparsity')\nplt.ylabel('PSNR')\n\n# Show the legend\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('rate_distortion_curves.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Synthetic dataset resembling the curve patterns\nsparsity = np.array([0.90, 0.92, 0.94, 0.96, 0.98])\ndwt1 = np.array([32.1, 32.0, 31.9, 31.8, 31.7])\ndwt2 = np.array([32.0, 31.9, 31.85, 31.8, 31.75])\ndwt3 = np.array([31.9, 31.8, 31.75, 31.7, 31.65])\ndwt4 = np.array([31.8, 31.7, 31.65, 31.6, 31.55])\ndct = np.array([31.5, 31.45, 31.4, 31.35, 31.3])\nspatial = np.array([32.2, 32.1, 32.0, 31.9, 31.8])\n\n# Plotting the curves with markers, colors and styles as in the provided image\nplt.plot(sparsity, dwt1, 'o--', label='DWT (1-level)', color='blue')\nplt.plot(sparsity, dwt2, 'o--', label='DWT (2-level)', color='orange')\nplt.plot(sparsity, dwt3, 'o--', label='DWT (3-level)', color='green')\nplt.plot(sparsity, dwt4, 'o--', label='DWT (4-level)', color='red')\nplt.plot(sparsity, dct, 'v--', label='DCT', color='purple')\nplt.plot(sparsity, spatial, 'x--', label='Spatial', color='brown')\n\n# Adding title and labels\nplt.title('The rate distortion curves of different signal representation schemes')\nplt.xlabel('Sparsity')\nplt.ylabel('PSNR')\n\n# Adding legend\nplt.legend()\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('rate_distortion_curves.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.09069/fig_level_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.12053/Boundary_Example.png",
        "caption": "SegFormer-B5 ECE comparison in ADE20K between boundary and non-boundary pixels among uncalibration, temperature scaling, and selective scaling. Crosses and dots show means and outliers. The numbers are the means.",
        "source": "cvpr_3/2212.12053/main.tex",
        "arxiv_id": "cvpr_3/2212.12053",
        "llava_output": "The image is a scatter plot that shows the comparison of boundary and non-boundary pixels among uncalibration, temperature scaling, and selective scaling in ADE20K. The plot displays the mean values and outliers for each method. The x-axis represents the boundary pixels, while the y-",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample ECE data for the boxplots\n# The actual values need to be replaced with the ones from the paper\ndata_boundary = [[0.4, 0.35, 0.33], [0.31, 0.28, 0.3], [0.26, 0.24, 0.25]]  # Replace with actual data\ndata_non_boundary = [[0.12, 0.1, 0.11], [0.1, 0.095, 0.1], [0.08, 0.085, 0.083]]  # Replace with actual data\n\nmean_values_boundary = [0.331, 0.294, 0.253]  # Replace with actual mean values\nmean_values_non_boundary = [0.110, 0.099, 0.082]  # Replace with actual mean values\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n# Boxplot for boundary data\nbplot1 = axes[0].boxplot(data_boundary, widths=0.5, patch_artist=True, showmeans=True, meanline=False, meanprops={\"marker\":\"x\",\"markersize\":\"10\"}, medianprops=dict(color=\"black\"), flierprops=dict(marker='o', color='black', markersize=5))\naxes[0].set_title('Boundary')\naxes[0].set_xticklabels(['Uncal', 'TS', 'Selective'])\n\n# Add mean values as text\nfor i, mean in enumerate(mean_values_boundary):\n    axes[0].text(i+1, mean, str(mean), horizontalalignment='center', color='red')\n\n# Boxplot for non-boundary data\nbplot2 = axes[1].boxplot(data_non_boundary, widths=0.5, patch_artist=True, showmeans=True, meanline=False, meanprops={\"marker\":\"x\",\"markersize\":\"10\"}, medianprops=dict(color=\"black\"), flierprops=dict(marker='o', color='black', markersize=5))\naxes[1].set_title('Non-boundary')\naxes[1].set_xticklabels(['Uncal', 'TS', 'Selective'])\n\n# Add mean values as text\nfor i, mean in enumerate(mean_values_non_boundary):\n    axes[1].text(i+1, mean, str(mean), horizontalalignment='center', color='red')\n\n# Set y-axis label\nfor ax in axes:\n    ax.set_ylabel('ECE')\n\n# Customizing the colors of the boxplots\ncolors = ['skyblue', 'lightgray', 'salmon']\nfor bplot in (bplot1, bplot2):\n    for patch, color in zip(bplot['boxes'], colors):\n        patch.set_facecolor(color)\n\n# Adjust layout to prevent overlapping\nplt.tight_layout()\n\n# Save the figure with 300 DPI\nplt.savefig('ece_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Sample data for the boundary and non-boundary methods\nboundary_data = [[0.5, 0.6, 0.7, 0.8, 0.35, 0.4, 0.45, 0.3, 0.3, 0.35, 0.33],\n                 [0.2, 0.25, 0.3, 0.35, 0.27, 0.28, 0.29, 0.3, 0.29, 0.28, 0.29],\n                 [0.15, 0.2, 0.22, 0.25, 0.24, 0.23, 0.21, 0.25, 0.24, 0.25, 0.25]]\nnon_boundary_data = [[0.1, 0.11, 0.12, 0.13, 0.09, 0.1, 0.115, 0.109, 0.11, 0.109, 0.11],\n                     [0.08, 0.09, 0.1, 0.105, 0.095, 0.096, 0.097, 0.098, 0.099, 0.095, 0.099],\n                     [0.06, 0.07, 0.075, 0.08, 0.077, 0.076, 0.079, 0.078, 0.08, 0.08, 0.082]]\n\n# Function to add mean values to the boxplots\ndef add_mean_values(ax, means, colors):\n    for i, mean in enumerate(means):\n        ax.text(i+1, mean - 0.05, f'{mean:.3f}', color='black', \n                ha='center', va='bottom')\n\n# Mean values for each of the categories for boundary and non-boundary\nboundary_means = [0.331, 0.294, 0.253]\nnon_boundary_means = [0.110, 0.099, 0.082]\n\n# Colors for each boxplot category\nbox_colors = ['lightblue', 'lightgreen', 'salmon']\n\n# Create the figure and axes\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Plot the boundary boxplots\nbp1 = axes[0].boxplot(boundary_data, patch_artist=True)\nadd_mean_values(axes[0], boundary_means, box_colors)\n\n# Plot the non-boundary boxplots\nbp2 = axes[1].boxplot(non_boundary_data, patch_artist=True)\nadd_mean_values(axes[1], non_boundary_means, box_colors)\n\n# Set properties of the boxplots\nfor bplot, color in zip((bp1, bp2), (box_colors, box_colors)):\n    for patch, color in zip(bplot['boxes'], box_colors):\n        patch.set_facecolor(color)\n\n# Adding cross for mean values\nfor ax, means in zip(axes, [boundary_means, non_boundary_means]):\n    ax.scatter([1, 2, 3], means, color='red', zorder=3, label='Means')\n\n# Setting labels and titles\naxes[0].set_title('Boundary')\naxes[1].set_title('Non-boundary')\nfor ax in axes:\n    ax.set_xticklabels(['Uncal', 'TS', 'Selective'])\n    ax.set_ylabel('ECE')\n\n# Add grid for better readability\nfor ax in axes:\n    ax.yaxis.grid(True, linestyle='--', which='major', color='grey', alpha=0.7)\n    ax.set_axisbelow(True)\n\n# Tight layout to ensure everything fits without overlapping\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('ece_comparison.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.12053/Boundary_Example_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2212.12053/Boundary_Example.png",
        "caption": "SegFormer-B5 ECE comparison in ADE20K between boundary and non-boundary pixels among uncalibration, temperature scaling, and selective scaling. Crosses and dots show means and outliers. The numbers are the means.",
        "source": "cvpr_3/2212.12053/[Dongdong] Segmentation Calibration (2)/main.tex",
        "arxiv_id": "cvpr_3/2212.12053",
        "llava_output": "The image is a scatter plot showing the comparison between boundary and non-boundary pixels in ADE20K among uncalibration, temperature scaling, and selective scaling. The plot has crosses and dots representing the means and outliers, while the numbers are the means. The image does not represent",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample ECE data for the boxplots\n# The actual values need to be replaced with the ones from the paper\ndata_boundary = [[0.4, 0.35, 0.33], [0.31, 0.28, 0.3], [0.26, 0.24, 0.25]]  # Replace with actual data\ndata_non_boundary = [[0.12, 0.1, 0.11], [0.1, 0.095, 0.1], [0.08, 0.085, 0.083]]  # Replace with actual data\n\nmean_values_boundary = [0.331, 0.294, 0.253]  # Replace with actual mean values\nmean_values_non_boundary = [0.110, 0.099, 0.082]  # Replace with actual mean values\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n# Boxplot for boundary data\nbplot1 = axes[0].boxplot(data_boundary, widths=0.5, patch_artist=True, showmeans=True, meanline=False, meanprops={\"marker\":\"x\",\"markersize\":\"10\"}, medianprops=dict(color=\"black\"), flierprops=dict(marker='o', color='black', markersize=5))\naxes[0].set_title('Boundary')\naxes[0].set_xticklabels(['Uncal', 'TS', 'Selective'])\n\n# Add mean values as text\nfor i, mean in enumerate(mean_values_boundary):\n    axes[0].text(i+1, mean, str(mean), horizontalalignment='center', color='red')\n\n# Boxplot for non-boundary data\nbplot2 = axes[1].boxplot(data_non_boundary, widths=0.5, patch_artist=True, showmeans=True, meanline=False, meanprops={\"marker\":\"x\",\"markersize\":\"10\"}, medianprops=dict(color=\"black\"), flierprops=dict(marker='o', color='black', markersize=5))\naxes[1].set_title('Non-boundary')\naxes[1].set_xticklabels(['Uncal', 'TS', 'Selective'])\n\n# Add mean values as text\nfor i, mean in enumerate(mean_values_non_boundary):\n    axes[1].text(i+1, mean, str(mean), horizontalalignment='center', color='red')\n\n# Set y-axis label\nfor ax in axes:\n    ax.set_ylabel('ECE')\n\n# Customizing the colors of the boxplots\ncolors = ['skyblue', 'lightgray', 'salmon']\nfor bplot in (bplot1, bplot2):\n    for patch, color in zip(bplot['boxes'], colors):\n        patch.set_facecolor(color)\n\n# Adjust layout to prevent overlapping\nplt.tight_layout()\n\n# Save the figure with 300 DPI\nplt.savefig('ece_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data for the boxplots\n# Replace these arrays with the actual data from the paper\nboundary_data = [np.random.normal(0.3, 0.1, 100), np.random.normal(0.29, 0.1, 100), np.random.normal(0.25, 0.1, 100)]\nnon_boundary_data = [np.random.normal(0.11, 0.1, 100), np.random.normal(0.09, 0.1, 100), np.random.normal(0.08, 0.1, 100)]\nmeans_boundary = [0.331, 0.294, 0.253]\nmeans_non_boundary = [0.110, 0.099, 0.082]\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True) # Adjust the figsize as needed\n\n# Boundary boxplot\nbplot1 = axs[0].boxplot(boundary_data, patch_artist=True, showmeans=False)\naxs[0].set_title('Boundary')\naxs[0].set_xticklabels(['Uncal', 'TS', 'Selective'])\n\n# Non-boundary boxplot\nbplot2 = axs[1].boxplot(non_boundary_data, patch_artist=True, showmeans=False)\naxs[1].set_title('Non-boundary')\naxs[1].set_xticklabels(['Uncal', 'TS', 'Selective'])\n\n# Adding the mean values on the plot with markers and text annotations\nfor i, mean in enumerate(means_boundary):\n    axs[0].plot(i+1, mean, 'rx') # Add red cross for mean\n    axs[0].text(i+1.1, mean - 0.1, f'{mean:.3f}', color='black', va='center') # Adjust text positioning\n\nfor i, mean in enumerate(means_non_boundary):\n    axs[1].plot(i+1, mean, 'rx') # Add red cross for mean\n    axs[1].text(i+1.1, mean - 0.1, f'{mean:.3f}', color='black', va='center') # Adjust text positioning\n\n# Styling the boxplots (optional)\ncolors = ['lightblue', 'lightgreen', 'salmon']\nfor bplot, color in zip((bplot1, bplot2), colors):\n    for patch in bplot['boxes']:\n        patch.set_facecolor(color)\n\n# Sharing y-axis\nplt.ylim(0, 1)  # Adjust the limit to fit the data range\n\n# Save the figure\nplt.savefig('comparison_plot.png', dpi=300)\n\nplt.show()",
        "type": "Box Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2212.12053/Boundary_Example_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2301.06083/intro3.png",
        "caption": "The black-box attack success rate on the Mobileface of attacking target * 1, 2 and 3 during the testing. The three methods are exclusively trained on target *.",
        "source": "cvpr_3/2301.06083/main.tex",
        "arxiv_id": "cvpr_3/2301.06083",
        "llava_output": "The image is a Bar Chart, showing the attack success rate on the Mobileface of attacking target * 1, 2 and 3 during the testing.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data for the bar chart (replace with actual data)\nmethods = ['TIP-IM', 'AMT-GAN', 'MAA']\nattack_targets = ['Attack Target *', 'Attack Target 1', 'Attack Target 2', 'Attack Target 3']\nsuccess_rates = {\n    'TIP-IM': [90, 40, 30, 20],  # Replace with actual success rates for TIP-IM\n    'AMT-GAN': [60, 35, 25, 15],  # Replace with actual success rates for AMT-GAN\n    'MAA': [70, 30, 20, 10]  # Replace with actual success rates for MAA\n}\n\n# Number of groups\nn_groups = len(attack_targets)\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Index for the groups\nindex = np.arange(n_groups)\n\n# Bar width and opacity\nbar_width = 0.2\nopacity = 0.8\n\n# Plotting the bars for each method\nfor i, method in enumerate(methods):\n    plt.bar(index + i * bar_width, success_rates[method], bar_width, alpha=opacity, label=method)\n\n# Adding the mean values as text on the bars\nfor i, method in enumerate(methods):\n    for j, rate in enumerate(success_rates[method]):\n        ax.text(j + i * bar_width, rate + 3, str(rate), ha='center')\n\n# Add some text for labels, title, and custom x-axis tick labels, etc.\nplt.xlabel('Attack Target')\nplt.ylabel('Black-box attack success rate (%)')\nplt.title('Black-box attack success rate on MobileFace')\nplt.xticks(index + bar_width, attack_targets)\nplt.legend()\n\n# Set the limits for the Y axis\nplt.ylim(0, 100)\n\n# Layout\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('blackbox_attack_success_rate.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data placeholders - substitute these with actual values\ntargets = ['Attack Target *', 'Attack Target 1', 'Attack Target 2', 'Attack Target 3']\nTIP_IM_scores = [80, 40, 20, 60]  # Replace with actual values\nAMT_GAN_scores = [70, 35, 55, 45]  # Replace with actual values\nMAA_scores = [75, 50, 65, 30]  # Replace with actual values\n\n# Set up the bar width and positions\nbarWidth = 0.25\nr1 = np.arange(len(TIP_IM_scores))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n\n# Create the figure and the axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the bars\nplt.bar(r1, TIP_IM_scores, color='blue', width=barWidth, edgecolor='grey', label='TIP-IM')\nplt.bar(r2, AMT_GAN_scores, color='orange', width=barWidth, edgecolor='grey', label='AMT-GAN')\nplt.bar(r3, MAA_scores, color='grey', width=barWidth, edgecolor='grey', label='MAA')\n\n# Add labels and customizations\nplt.xlabel('Attack Target', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(TIP_IM_scores))], targets)\nplt.ylabel('Black-box attack success rate (%)')\nplt.title('The black-box attack success rate on the Mobileface\\nof attacking target 1, 2, and 3 during the testing')\n\n# Create legend & Show graphic\nplt.legend()\n\n# Save the figure\nplt.savefig('bar_plot.png', dpi=300)\n\n# Show the plot in a window (if necessary)\n# plt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2301.06083/intro3_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2301.06083/apiexp.png",
        "caption": "The left side of the image depicts the five expression states, while the right side of the image depicts the influence of varied AU on the attack performance of Face++ and Tencent.",
        "source": "cvpr_3/2301.06083/main.tex",
        "arxiv_id": "cvpr_3/2301.06083",
        "llava_output": "The image is a line chart that shows the influence of varied AU on the attack performance of Face++ and Tencent.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data for the five expression states (replace with actual values)\nexpressions = ['Happy', 'Angry', 'Sad', 'Surprised', 'Expressionless']\nfaceplusplus = [71.7035, 54.3533, 47.1902, 41.2885, 37.3255]  # Replace with actual values for Face++\ntencent = [67.0503, 59.9124, 45.0348, 39.1731, 32.3652]  # Replace with actual values for Tencent\n\n# Creating the figure and the line chart\nfig, ax = plt.subplots()\n\n# Plotting the lines\nax.plot(expressions, faceplusplus, label='Face++', marker='o', color='red')\nax.plot(expressions, tencent, label='Tencent', marker='o', color='blue')\n\n# Adding the data values as text annotations on the plot\nfor i, txt in enumerate(faceplusplus):\n    ax.annotate(txt, (expressions[i], faceplusplus[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\nfor i, txt in enumerate(tencent):\n    ax.annotate(txt, (expressions[i], tencent[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Adding the legend\nax.legend()\n\n# Adding a title to the plot (optional)\nax.set_title('Influence of Varied AU on the Attack Performance of Face++ and Tencent')\n\n# Saving the figure as a PNG file with 300 DPI\nplt.savefig('expression_influence_plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Example data (you need the real data here)\nexpressions = ['Happy', 'Angry', 'Sad', 'Surprised', 'Expressionless']\nfaceplusplus_celebahq = [70, 60, 50, 40, 30]  # Replace with the actual data\nfaceplusplus_lfw = [65, 55, 45, 35, 25]  # Replace with the actual data\ntencent_celebahq = [75, 65, 55, 45, 35]  # Replace with the actual data\ntencent_lfw = [80, 70, 60, 50, 40]  # Replace with the actual data\n\nplt.figure(figsize=(10, 5))  # Set the figure size as needed\n\n# Plot the lines\nplt.plot(expressions, faceplusplus_celebahq, marker='o', label='Face++/CelebA-HQ')\nplt.plot(expressions, faceplusplus_lfw, marker='o', label='Face++/LFW')\nplt.plot(expressions, tencent_celebahq, marker='o', label='Tencent/CelebA-HQ')\nplt.plot(expressions, tencent_lfw, marker='o', label='Tencent/LFW')\n\n# Add data labels to each point on the lines\nfor x, y in zip(expressions, faceplusplus_celebahq):\n    plt.text(x, y, f'{y:.2f}', ha='center')\n\nfor x, y in zip(expressions, faceplusplus_lfw):\n    plt.text(x, y, f'{y:.2f}', ha='center')\n\nfor x, y in zip(expressions, tencent_celebahq):\n    plt.text(x, y, f'{y:.2f}', ha='center')\n\nfor x, y in zip(expressions, tencent_lfw):\n    plt.text(x, y, f'{y:.2f}', ha='center')\n\n# Add the dashed line to separate the images from the plot\nplt.axvline(x=4.5, color=\"black\", linestyle=\"--\")\n\nplt.xlabel('Expression States')  # Set the label for the x-axis\nplt.ylabel('Attack Performance (%)')  # Set the label for the y-axis\nplt.title('Influence of varied AU on the Attack Performance')  # Set the title of the graph\nplt.legend()  # Display the legend\n\nplt.savefig('plot.png', dpi=300)  # Save the figure with 300 dpi\nplt.show()  # Show the plot\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# Define a function to display an image at a given position\ndef add_image(ax, filepath, xy, zoom):\n    img = plt.imread(filepath)\n    imbox = OffsetImage(img, zoom=zoom)\n    ab = AnnotationBbox(imbox, xy, frameon=False, xycoords='data', boxcoords=\"axes fraction\", pad=0)\n    ax.add_artist(ab)\n    return ax\n\n# Assuming you have the left plot's axis object, add images using the function\n# ax = plt.gca()  # Get the current axis\n# add_image(ax, 'path_to_clean_image.png', xy=(1, 0.5), zoom=0.1)\n# ... add other images similarly\n\n# Continue with the rest of the plotting code above.",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2301.06083/apiexp_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2301.07093/gamma_change.png",
        "caption": "learnable $\\gamma$ in the gated self attention layer in the middle of Unet changes during the training progress.",
        "source": "cvpr_3/2301.07093/supp.tex",
        "arxiv_id": "cvpr_3/2301.07093",
        "llava_output": "The image is a scatter plot showing the relationship between two variables.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data points using an exponential decay to simulate the trend\n# Replace these sample points with the actual data for accuracy\nx = np.linspace(0, 500000, 100)  # Training steps\ny = 0.08 * np.exp(-x / 150000)  # Simulated values for the learnable parameter\n\n# Create the plot\nplt.figure()\nplt.plot(x, y, marker='o', linestyle='-', color='blue')\n\n# Add labels and title if needed\nplt.xlabel('Training Steps')\nplt.ylabel('Value of Learnable \u03b3')\nplt.title('Learnable \u03b3 in Gated Self-Attention Layer')\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('learnable_gamma.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Speculative data to resemble the shape of the plot\ntraining_steps = np.arange(0, 500000, 10000)\ngamma_values = 0.08 * np.exp(-training_steps / 150000)\n\n# Noise could be added to the data to mimic the variability seen in the plot\nnoise = np.random.normal(0, 0.001, training_steps.shape)\ngamma_values_with_noise = np.maximum(gamma_values + noise, 0)  # ensure no negative values\n\n# Create the plot\nplt.figure(figsize=(6, 4))\nplt.plot(training_steps, gamma_values_with_noise, marker='o', color='blue', linestyle='-')\nplt.xlim(0, max(training_steps))\nplt.ylim(0, max(gamma_values_with_noise) + 0.01)\n\n# You may want to set the exact tick values and labels as visible in the original plot\nplt.xticks([0, 100000, 200000, 300000, 400000, 500000])\nplt.yticks(np.arange(0, 0.09, 0.01))\n\n# Save the plot as a PNG with 300 DPI\nplt.savefig('gamma_values_during_training.png', dpi=300)\n\n# Optionally, display the plot in a window\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2301.07093/gamma_change_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2302.09997/histogram_of_scale_transformation_errors12.png",
        "caption": "The histogram of the scale transformation ratio $\\Delta r_i$ and the weighted log-ratio $\\rho_i$ on 5.6M keypoint pairs.",
        "source": "cvpr_3/2302.09997/merged.tex",
        "arxiv_id": "cvpr_3/2302.09997",
        "llava_output": "The image is a histogram of the scale transformation ratio $\\Delta r_i$ and the weighted log-ratio $\\rho_i$ on 5.6M keypoint pairs.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming normal distribution for sample data, replace with actual data\ndelta_ri = np.random.normal(loc=1.0, scale=0.2, size=5600000)  # scale transformation ratio\nrho_i = np.random.normal(loc=0.0, scale=0.1, size=5600000)     # weighted log-scale ratio\n\n# Creating the figure and subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plotting the histogram for the scale transformation ratio\naxs[0].hist(delta_ri, bins=50, color='blue')\naxs[0].set_title('Histogram of the scale transformation error')\naxs[0].set_xlabel('Delta r_i')\naxs[0].set_ylabel('Occurrences')\n\n# Plotting the histogram for the weighted log-scale transformation error\naxs[1].hist(rho_i, bins=50, color='blue')\naxs[1].set_title('Histogram of weighted log-scale transformation error')\naxs[1].set_xlabel('rho_i')\n\n# Layout adjustment to prevent overlap\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('histograms.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Let's assume some random data for the purpose of this example\n# Normally you would load your data here\ndelta_ri = np.random.normal(loc=1.0, scale=0.2, size=5600000)\nrho_i = np.random.normal(loc=0.0, scale=0.1, size=5600000)\n\n# Setting up the figure and axes\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n\n# Plotting the histogram of scale transformation error\naxes[0].hist(delta_ri, bins=50, color='blue')\naxes[0].set_title(\"Histogram of the scale transformation error\")\naxes[0].set_xlabel(\"$\\Delta r_i$\")\naxes[0].set_ylabel(\"Occurrence\")\n\n# Plotting the histogram of weighted log-scale transformation error\naxes[1].hist(rho_i, bins=50, color='blue', range=(-0.8, 0.8))\naxes[1].set_title(\"Histogram of weighted log-scale transformation error\")\naxes[1].set_xlabel(\"$\\rho_i$\")\n# No need to set the y-axis label again, as it's the same for both\n\n# Tight layout to avoid overlapping\nplt.tight_layout()\n\n# Save the figure as a png file with 300 dpi\nplt.savefig('histograms.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2302.09997/histogram_of_scale_transformation_errors12_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2302.09997/histogram_of_detector_angular_transformation12.png",
        "caption": "The histogram of the detector angular transformation $\\alpha_i$ for 6.1M of keypoint pairs. The right histogram shows logarithmic scale of the occurrence to visualize the number of samples across the complete interval $[-180, 180)$ degrees.",
        "source": "cvpr_3/2302.09997/merged.tex",
        "arxiv_id": "cvpr_3/2302.09997",
        "llava_output": "The image is a histogram showing the distribution of the detector angular transformation $\\alpha_i$ for 6.1M of keypoint pairs. The histogram is plotted on a logarithmic scale to visualize the number of samples across the complete interval $[-180, 180",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming normal distribution for sample data, replace with actual data\ndelta_ri = np.random.normal(loc=1.0, scale=0.2, size=5600000)  # scale transformation ratio\nrho_i = np.random.normal(loc=0.0, scale=0.1, size=5600000)     # weighted log-scale ratio\n\n# Creating the figure and subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plotting the histogram for the scale transformation ratio\naxs[0].hist(delta_ri, bins=50, color='blue')\naxs[0].set_title('Histogram of the scale transformation error')\naxs[0].set_xlabel('Delta r_i')\naxs[0].set_ylabel('Occurrences')\n\n# Plotting the histogram for the weighted log-scale transformation error\naxs[1].hist(rho_i, bins=50, color='blue')\naxs[1].set_title('Histogram of weighted log-scale transformation error')\naxs[1].set_xlabel('rho_i')\n\n# Layout adjustment to prevent overlap\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('histograms.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate some data for the example (replace this with your actual data)\nnp.random.seed(0)  # for reproducibility\ndata = np.random.randn(6100000)  # Simulating 6.1M data points\n\n# Setup the figure and axes for two subplots with shared y-axis\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=True)\n\n# First histogram: linear scale\naxes[0].hist(data, bins=50, range=(-180, 180), color='blue')\naxes[0].set_title('Histogram of the detector angular transformation')\naxes[0].set_xlabel('The detector angular transformation \u03b1\u1d62')\naxes[0].set_ylabel('Occurrence')\n\n# Second histogram: logarithmic scale\naxes[1].hist(data, bins=50, range=(-180, 180), color='blue', log=True)\naxes[1].set_title('Histogram of the detector angular transformation')\naxes[1].set_xlabel('The detector angular transformation \u03b1\u1d62')\n# No need to set ylabel again, it's shared with the first subplot\n\n# Adjust layout to prevent overlapping\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('histogram.png', dpi=300)\n\n# Show the plot for interactive inspection (optional)\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2302.09997/histogram_of_detector_angular_transformation12_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2302.09997/example_969_cases_losses.png",
        "caption": "Data set B with 969 cases. Histograms of mean and maximum losses due to using an approximate/algebraic estimation method, and due to using equal weighting instead of scale dependent weighting",
        "source": "cvpr_3/2302.09997/merged.tex",
        "arxiv_id": "cvpr_3/2302.09997",
        "llava_output": "The image is a Histogram showing the mean and maximum losses in accuracy.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Random data generation for the purpose of the example\n# Replace with the actual data for accurate representation\ndata_mean_loss_algml = np.random.normal(loc=1.04, scale=0.1, size=969)\ndata_max_loss_algml = np.random.normal(loc=3.6, scale=0.5, size=969)\ndata_mean_loss_1s1g = np.random.normal(loc=1.51, scale=0.3, size=969)\ndata_max_loss_1s1g = np.random.normal(loc=7.17, scale=1.0, size=969)\n\n# Creating subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Plotting histograms\naxs[0, 0].hist(data_mean_loss_algml, bins=20, color='blue')\naxs[0, 0].set_title('mean loss(ALG|ML), mean = 1.04')\n\naxs[0, 1].hist(data_max_loss_algml, bins=20, color='blue')\naxs[0, 1].set_title('max loss(ALG|ML), max = 3.6')\n\naxs[1, 0].hist(data_mean_loss_1s1g, bins=20, color='blue')\naxs[1, 0].set_title('mean loss(1s|1g), mean = 1.51')\n\naxs[1, 1].hist(data_max_loss_1s1g, bins=20, color='blue')\naxs[1, 1].set_title('max loss(1s|1g), max = 17.17')\n\n# Adjust layout\nplt.tight_layout()\n\n# Set a common title for all subplots\nplt.suptitle('Losses in accuracy, #=969', y=1.05)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('losses_in_accuracy.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Placeholder for the actual data\n# Replace these with your actual data points\nmean_loss_alg = [...]\nmax_loss_alg = [...]\nmean_loss_1s = [...]\nmax_loss_1s = [...]\n\n# Set up the figure and axes\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Top Left: mean loss(ALG|ML)\naxs[0, 0].hist(mean_loss_alg, bins=20, color='blue', edgecolor='black')\naxs[0, 0].set_title('mean loss(ALG|ML), mean \u22481.04')\n\n# Top Right: max loss(ALG|ML)\naxs[0, 1].hist(max_loss_alg, bins=20, color='blue', edgecolor='black')\naxs[0, 1].set_title('max loss(ALG|ML), max \u22483.6')\n\n# Bottom Left: mean loss(1|s)\naxs[1, 0].hist(mean_loss_1s, bins=20, color='blue', edgecolor='black')\naxs[1, 0].set_title('mean loss(1|s), mean \u22481.51')\n\n# Bottom Right: max loss(1|s)\naxs[1, 1].hist(max_loss_1s, bins=20, color='blue', edgecolor='black')\naxs[1, 1].set_title('max loss(1|s), max \u224817.17')\n\n# Adjust layout so titles and axes don't overlap\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nfig.suptitle('Losses in accuracy, #=969', fontsize=16)\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('histograms.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2302.09997/example_969_cases_losses_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2302.10174/baseline_tsne_legend_side.png",
        "caption": "t-SNE visualization of real and fake images associated with two types of generative models. The feature space used is of a classifier trained to distinguish Fake (GAN) from Real (GAN).",
        "source": "cvpr_3/2302.10174/approach.tex",
        "arxiv_id": "cvpr_3/2302.10174",
        "llava_output": "The image is a scatter plot showing the feature space used by a classifier to distinguish between real and fake images generated by two types of generative models. The scatter plot is labeled with the names of the two models, \"Fake (GAN)\" and \"Real (GAN)\".</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n# Simulate some high-dimensional data\n# Replace this with your actual high-dimensional data\nnp.random.seed(0)\nfake_gan = np.random.normal(loc=-20, scale=5, size=(100, 50))\nreal_gan = np.random.normal(loc=20, scale=5, size=(100, 50))\nfake_diffusion = np.random.normal(loc=0, scale=5, size=(100, 50))\nreal_diffusion = np.random.normal(loc=0, scale=5, size=(100, 50))\n\n# Combine the data\ndata = np.vstack([fake_gan, real_gan, fake_diffusion, real_diffusion])\n\n# Apply t-SNE transformation\ntsne = TSNE(n_components=2, random_state=0)\ntransformed_data = tsne.fit_transform(data)\n\n# Split the transformed data into separate sets\ntransformed_fake_gan = transformed_data[:100]\ntransformed_real_gan = transformed_data[100:200]\ntransformed_fake_diffusion = transformed_data[200:300]\ntransformed_real_diffusion = transformed_data[300:]\n\n# Create the plot\nplt.figure(figsize=(8, 4))\n\n# Plot each set\nplt.scatter(transformed_fake_gan[:, 0], transformed_fake_gan[:, 1], c='red', label='Fake (GAN)', alpha=0.5)\nplt.scatter(transformed_real_gan[:, 0], transformed_real_gan[:, 1], c='yellow', label='Real (GAN)', alpha=0.5)\nplt.scatter(transformed_fake_diffusion[:, 0], transformed_fake_diffusion[:, 1], c='pink', label='Fake (Diffusion)', alpha=0.5)\nplt.scatter(transformed_real_diffusion[:, 0], transformed_real_diffusion[:, 1], c='green', label='Real (Diffusion)', alpha=0.5)\n\n# Add the legend\nplt.legend()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('tsne_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have t-SNE data stored in these variables\n# For example purposes, I'm generating random data.\n# Replace these with your actual t-SNE data.\ntsne_fake_gan = np.random.rand(50, 2)\ntsne_real_gan = np.random.rand(50, 2)\ntsne_fake_diffusion = np.random.rand(50, 2)\ntsne_real_diffusion = np.random.rand(50, 2)\n\n# Plot each category with a different color\nplt.scatter(tsne_fake_gan[:, 0], tsne_fake_gan[:, 1], color='red', label='Fake (GAN)')\nplt.scatter(tsne_real_gan[:, 0], tsne_real_gan[:, 1], color='blue', label='Real (GAN)')\nplt.scatter(tsne_fake_diffusion[:, 0], tsne_fake_diffusion[:, 1], color='yellow', label='Fake (Diffusion)')\nplt.scatter(tsne_real_diffusion[:, 0], tsne_real_diffusion[:, 1], color='green', label='Real (Diffusion)')\n\n# Add a legend\nplt.legend()\n\n# Optionally adjust the legend's appearance\n# For example, to move the legend outside the plot, use the bbox_to_anchor argument.\n# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Save the plot as a PNG with 300 DPI\nplt.savefig('tsne_visualization.png', dpi=300, bbox_inches='tight')\n\n# Show the plot for interactive inspection (if needed)\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2302.10174/baseline_tsne_legend_side_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2302.10174/tsne_pretrain_ablation_progan_small.png",
        "caption": "t-SNE visualization of real (red) and fake (blue) images using the feature space of different image encoders. CLIP:ViT's feature space best separates the real features from fake.",
        "source": "cvpr_3/2302.10174/results.tex",
        "arxiv_id": "cvpr_3/2302.10174",
        "llava_output": "The image is a scatter plot that shows the results of an experiment using different image encoders to separate real and fake images. The x-axis represents the real images, while the y-axis represents the fake images. The scatter plot is divided into six sections, each corresponding to a different image encoder",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Simulate t-SNE data\ndef simulate_tsne_data(num_points=300):\n    # Generate two clusters: cluster 0 for fake, cluster 1 for real\n    cluster_0 = np.random.randn(num_points, 2) + np.array([-2, -2])\n    cluster_1 = np.random.randn(num_points, 2) + np.array([2, 2])\n    return cluster_0, cluster_1\n\n# Create a figure and subplots\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\n# Titles for each subplot\ntitles = ['CLIP:ViT-L/14', 'CLIP:ResNet-50', 'ImageNet:ViT-B/16', 'ImageNet:ResNet-50']\n\n# Create each subplot\nfor i in range(4):\n    # Simulate the data differently for each subplot to mimic different separations\n    cluster_0, cluster_1 = simulate_tsne_data(num_points=300)\n\n    # Introduce variance to the clusters for different encoders\n    cluster_variance = 0.2 * i\n    cluster_0 += np.random.normal(0, cluster_variance, cluster_0.shape)\n    cluster_1 += np.random.normal(0, cluster_variance, cluster_1.shape)\n\n    # Plot the fake images in blue\n    axs[i].scatter(cluster_0[:, 0], cluster_0[:, 1], c='blue', label='Fake', edgecolors='none', alpha=0.5)\n\n    # Plot the real images in red\n    axs[i].scatter(cluster_1[:, 0], cluster_1[:, 1], c='red', label='Real', edgecolors='none', alpha=0.5)\n\n    axs[i].set_title(titles[i])\n    axs[i].axis('equal')  # Set equal scaling by changing axis limits.\n\n# Remove ticks from x and y axis\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Set the layout so that titles and subplots do not overlap\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('tsne_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some t-SNE data for demonstration purposes\nnp.random.seed(0)  # For reproducibility\nreal_data = [np.random.randn(100, 2) for _ in range(4)]\nfake_data = [np.random.randn(100, 2) for _ in range(4)]\nencoders = ['CLIP:ViT-L/14', 'CLIP:ResNet-50', 'ImageNet:ViT-B/16', 'ImageNet:ResNet-50']\n\n# Create a figure and a 1x4 grid of subplots\nfig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n# Scatter plots for each subplot\nfor ax, real, fake, title in zip(axes, real_data, fake_data, encoders):\n    ax.scatter(real[:, 0], real[:, 1], c='red', label='Real')\n    ax.scatter(fake[:, 0], fake[:, 1], c='blue', label='Fake')\n    ax.set_title(title)\n\n# Optional: setting the legend for each subplot if needed\n# for ax in axes:\n#     ax.legend()\n\n# Optional: Adjust layout for better spacing\nplt.tight_layout()\n\n# Optional: add overall title or adjust subplots\nfig.suptitle('t-SNE visualization of real and fake images', fontsize=16)\nplt.subplots_adjust(top=0.85)\n\n# Save the figure\nplt.savefig('tsne_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2302.10174/tsne_pretrain_ablation_progan_small_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.00340/eta_at_cifar_14.png",
        "caption": "AT",
        "source": "cvpr_3/2303.00340/main.tex",
        "arxiv_id": "cvpr_3/2303.00340",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming we don't have the exact data, let's generate some dummy data that looks like a monotonically increasing function\nnp.random.seed(0)\nx = np.linspace(0, 0.5, 100)\ny = np.cumsum(np.random.normal(0, 0.001, size=x.size)) + 0.07645\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, color='blue')\n\n# Set the title and labels\nplt.xlabel(r'$\\|\\delta\\|_\\infty$')  # LaTeX math mode for the x label\nplt.ylabel(r'$\\eta(x, \\delta)$')    # LaTeX math mode for the y label\n\n# Save the plot as a .png file with 300 dpi\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 2: Generate some sample data\n# In reality, you would load your dataset instead of generating random data\nx = np.linspace(0, 0.5, 100)  # X-axis data\nnoise = np.random.normal(0, 0.0001, x.shape)  # Some noise to add variation\ny = 0.07640 + x**2 + noise  # Y-axis data resembling the trend in the input image\n\n# Step 3: Create the plot\nplt.figure(figsize=(10, 6))  # Set the figure size as desired\nplt.plot(x, y, color='blue')  # Plot the data with blue color\n\n# Add proper labels and title if necessary (the example image does not show a title)\nplt.xlabel('||\u03b4||\u221e')  # Set the x-axis label\nplt.ylabel('\u03b7(x, \u03b4)')  # Set the y-axis label\n\n# Make sure the ticks match the image if required\nplt.xticks(np.arange(0, 0.6, 0.1))  # X-axis ticks\nplt.yticks(np.arange(0.07640, 0.07661, 0.00005))  # Y-axis ticks\n\n# Remove top and right spines (borders)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\n# Step 4: Save the plot as a .png file with 300 dpi\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot (not necessary if only saving to file)\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.00340/eta_at_cifar_14_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.00340/hess_mnist_top100.png",
        "caption": "",
        "source": "cvpr_3/2303.00340/main.tex",
        "arxiv_id": "cvpr_3/2303.00340",
        "llava_output": "The image is a line chart that shows the results of an experiment. The line is orange and yellow, and it is plotted against a black background. The chart is titled \"Experiment Results\" and has a vertical axis and a horizontal axis. The line represents the data collected during the experiment, and the orange",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 2: Create the matrix with the diagonal pattern\nsize = 100  # Assuming the size of the matrix is 100x100\ndata = np.zeros((size, size))\nfor i in range(size):\n    for j in range(size):\n        if i + j == size - 1:  # Fill diagonal with a value\n            data[i, j] = 0.4\n\n# Step 3: Display the matrix with imshow\nfig, ax = plt.subplots()\ncax = ax.imshow(data, interpolation='nearest', cmap='hot')\n\n# Step 4: Add a colorbar\ncbar = fig.colorbar(cax, ticks=[0, 0.1, 0.2, 0.3, 0.4])\ncbar.ax.set_yticklabels(['0', '0.1', '0.2', '0.3', '> 0.4'])\n\n# Set plot limits (optional, just to ensure it looks exactly like your example)\nplt.xlim(0, size - 1)\nplt.ylim(0, size - 1)\n\n# Step 5: Save the plot\nplt.savefig('data_visualization.png', dpi=300)\n\n# Display the plot (optional)\nplt.show()",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Simulating data: creating a matrix with ones on the diagonal and zeros elsewhere\ndata = np.identity(10) * np.random.rand(10)\n\n# Create a custom colormap\ncmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"black\",\"red\",\"orange\"])\n\n# Creating the heatmap\nfig, ax = plt.subplots()\n\n# Setting the colormap limit. Adjust vmin and vmax to control the scale.\ncbar = ax.imshow(data, cmap=cmap, interpolation='nearest', vmin=0, vmax=0.5)\n\n# Add color bar on the right side of the plot\nplt.colorbar(cbar, ax=ax)\n\n# Saving the figure with 300 DPI.\nplt.savefig('heatmap_example.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.00340/hess_mnist_top100_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.00340/vis_hess_mnist_full.png",
        "caption": "The full heatmap of attribution gradients of MNIST in size $784\\times 784$.",
        "source": "cvpr_3/2303.00340/main.tex",
        "arxiv_id": "cvpr_3/2303.00340",
        "llava_output": "1. The image is a scatter plot.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a 784x784 matrix with a diagonal gradient\nsize = 784\nheatmap_data = np.zeros((size, size))\nfor i in range(size):\n    for j in range(size):\n        heatmap_data[i, j] = min(i, j) / size\n\n# Create the heatmap using matplotlib\nplt.figure(figsize=(8, 8)) # Adjust figure size as needed\nplt.imshow(heatmap_data, cmap='hot', interpolation='nearest', aspect='auto')\n\n# Add a colorbar\nplt.colorbar()\n\n# Hide the axis labels\nplt.axis('off')\n\n# Save the heatmap to a .png file with 300 DPI\nplt.savefig('heatmap.png', dpi=300, bbox_inches='tight', pad_inches=0)\n\n# Display the plot\nplt.show()",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Since we don't have the actual data, let's create a placeholder matrix\n# that mimics the gradient pattern seen in the image.\ndata = np.identity(784)\n\n# Create the heatmap\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='hot')\n\n# Add a color bar to the side\nfig.colorbar(cax)\n\n# Remove axis labels and ticks if necessary\nax.set_xticks([])\nax.set_yticks([])\n\n# Save the heatmap as a PNG file with 300dpi\nplt.savefig('heatmap.png', dpi=300)\n\n# Display the plot if you wish\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.00340/vis_hess_mnist_full_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.00340/vis_hess_mnist_full.png",
        "caption": "The full heatmap of attribution gradients of CIFAR-100 in size $3072\\times 3072$.",
        "source": "cvpr_3/2303.00340/main.tex",
        "arxiv_id": "cvpr_3/2303.00340",
        "llava_output": "The image is a scatter plot, showing the attribution gradients of CIFAR-100 in size $3072\\times 3072$.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming random data because the actual data is not available.\n# Size is 3072x3072 as per the description provided.\ndata = np.random.rand(3072, 3072)\n\n# Create a diagonal matrix with ones on the diagonal and zeros elsewhere.\n# This simulates the diagonal line observed in the image.\ndiagonal_data = np.eye(3072)\n\n# Plotting the heatmap using matplotlib\nplt.figure(figsize=(10, 10))\nplt.imshow(diagonal_data, cmap='hot', interpolation='nearest')\nplt.colorbar()\n\n# Remove the axis labels\nplt.axis('off')\n\n# Save the figure with 300 DPI\nplt.savefig('heatmap.png', dpi=300, bbox_inches='tight', pad_inches=0)\n\nplt.show()",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulating the heatmap data, since we don't have the original data.\n# Dimension is set to 3072x3072 as per the requirement.\ndata = np.random.rand(3072, 3072)\n\n# Ensure the diagonal is zero or near zero to match the provided image\nfor i in range(data.shape[0]):\n    for j in range(data.shape[1]):\n        if i == j:\n            data[i, j] = 0\n\n# Create the heatmap using matplotlib\nplt.figure(figsize=(10, 10))  # Adjust the size to your needs\nplt.imshow(data, cmap='hot', interpolation='nearest')\n\n# Add the colorbar to the right\nplt.colorbar()\n\n# Save the figure to a PNG file with 300 dpi\nplt.savefig('heatmap_cifar100.png', dpi=300)\n\n# Optionally, display the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.00340/vis_hess_mnist_full_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.00340/dist_at.png",
        "caption": "AT",
        "source": "cvpr_3/2303.00340/main.tex",
        "arxiv_id": "cvpr_3/2303.00340",
        "llava_output": "3000</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating mock data - adjust this with your actual data\ndata = np.random.exponential(scale=0.2, size=10000)\ndata = data[data < 1.5]  # To mimic the cutoff in the plot\n\n# Plotting the histogram\nplt.figure(figsize=(10, 7.5))  # Adjust figure size as needed\nplt.hist(data, bins=30, color='blue', edgecolor='black')  # Adjust bins and color as needed\n\n# Setting the labels and title (if any)\nplt.xlabel('Differences', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\n\n# Save the figure as a .png file with 300 dpi\nplt.savefig('histogram.png', dpi=300)\n\nplt.show()  # To display the plot",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Since I don't have the exact data, I will generate example data that fits the general shape of your histogram.\n# You can replace `data` with the actual dataset you want to plot.\ndata = np.random.gamma(shape=2, scale=0.2, size=10000)\n\n# Create a histogram plot\nplt.hist(data, bins=30, color='blue', edgecolor='black')\n\n# Set the title and labels\nplt.title('AT')  # Replace with your actual title\nplt.xlabel('Differences')\nplt.ylabel('Frequency')\n\n# Set the desired tick mark frequency if needed; the example uses automatic tick marks\n# plt.xticks(np.arange(0, max(data), 0.1))\n# plt.yticks(np.arange(0, max(plt.hist(data)[0]), 500))\n\n# Save the figure with 300 DPI\nplt.savefig('histogram.png', dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.00340/dist_at_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.00521/bid_heatmap_new.png",
        "caption": "Results in BID",
        "source": "cvpr_3/2303.00521/paper.tex",
        "arxiv_id": "cvpr_3/2303.00521",
        "llava_output": "The image is a scatter plot showing the results of an experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from the image\ndata = np.array([\n    [0.7383, 0.8394, 0.8301],\n    [0.8356, 0.7678, 0.8532],\n    [0.8215, 0.8555, 0.7439]\n])\n\n# Labels for the heatmap\nrow_labels = ['Geometry', 'Color', 'Texture']\ncolumn_labels = ['Geometry', 'Color', 'Texture']\n\n# Create the heatmap\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='hot')\n\n# Add colorbar\ncbar = fig.colorbar(cax)\n\n# Add text annotations\nfor (i, j), val in np.ndenumerate(data):\n    ax.text(j, i, f'{val:.4f}', ha='center', va='center', color='black')\n\n# Set labels\nax.set_xticklabels([''] + column_labels)\nax.set_yticklabels([''] + row_labels)\n\n# Rotate the tick labels for aesthetics\nplt.xticks(rotation=45)\n\n# Save the figure\nplt.savefig('heatmap.png', dpi=300)\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Prepare the data\ndata = np.array([\n    [0.7383, 0.8394, 0.8301],\n    [0.8356, 0.7678, 0.8532],\n    [0.8215, 0.8555, 0.7439]\n])\n\n# Labels for the rows and columns\nrow_labels = ['Geometry', 'Color', 'Texture']\ncol_labels = ['Geometry', 'Color', 'Texture']\n\n# Step 2: Create the plot\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='hot')\n\n# Set color bar\nplt.colorbar(cax)\n\n# Set axis labels\nax.set_xticklabels([''] + col_labels)\nax.set_yticklabels([''] + row_labels)\n\n# Rotate the x-axis labels if necessary\nplt.xticks(rotation=45)\n\n# Step 3: Annotate each cell with the corresponding value\nfor i in range(data.shape[0]):\n    for j in range(data.shape[1]):\n        c = data[j,i]\n        ax.text(i, j, str(c), va='center', ha='center')\n\n# Optional: Set the aspect of the plot to 'auto' so the cells are not square shaped\nax.set_aspect('auto')\n\n# Step 4: Save the plot as a PNG file with 300 dpi\nplt.savefig('heatmap.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.00521/bid_heatmap_new_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 5,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.01311/ablation_studies.png",
        "caption": "Curves of CLIP scores increasing within 300s under three different module settings.",
        "source": "cvpr_3/2303.01311/main.tex",
        "arxiv_id": "cvpr_3/2303.01311",
        "llava_output": "The image is a line chart showing the evolution of CLIP scores over time.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data to simulate the curves for illustration purposes\nnp.random.seed(0)\nx = np.linspace(0, 300, 100)\ny_evolution = np.cumsum(np.random.normal(loc=0.003, scale=0.01, size=x.shape))\ny_translator = np.cumsum(np.random.normal(loc=0.003, scale=0.01, size=x.shape))\ny_full_impl = np.cumsum(np.random.normal(loc=0.003, scale=0.01, size=x.shape))\n\n# Create means by adding a base score and scaling it up \nbase_score = 0.25\ny_evolution = base_score + y_evolution * 2\ny_translator = base_score + y_translator * 3\ny_full_impl = base_score + y_full_impl * 4\n\n# Create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# Plot data with fill between for standard deviation effect\nax.plot(x, y_evolution, label='Evolution Search', color='blue')\nax.fill_between(x, y_evolution - 0.01, y_evolution + 0.01, color='blue', alpha=0.2)\n\nax.plot(x, y_translator, label='Translator', color='orange')\nax.fill_between(x, y_translator - 0.01, y_translator + 0.01, color='orange', alpha=0.2)\n\nax.plot(x, y_full_impl, label='Full Implementation', color='green')\nax.fill_between(x, y_full_impl - 0.01, y_full_impl + 0.01, color='green', alpha=0.2)\n\n# Add labels and title (if any)\nax.set_xlabel('Running Time /s')\nax.set_ylabel('CLIP Score')\n\n# Add legend\nax.legend()\n\n# Optionally, adjust the axes' limits if necessary\nax.set_xlim([0, 300])\nax.set_ylim([0.24, 0.34])\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data generation (replace with your actual data).\n# Assuming here that 'time' is from 0 to 300 seconds.\ntime = np.arange(0, 300, 1)\n\n# Generate some random data to simulate the CLIP scores for the three different settings.\nevolution_search_mean = np.random.normal(0.3, 0.02, size=time.shape[0]).cumsum() / 100\ntranslator_mean = np.random.normal(0.3, 0.02, size=time.shape[0]).cumsum() / 100\nfull_implementation_mean = np.random.normal(0.3, 0.02, size=time.shape[0]).cumsum() / 100\n\n# Generate some random variances to simulate the confidence intervals.\nevolution_search_variance = np.random.normal(0.005, 0.002, size=time.shape[0])\ntranslator_variance = np.random.normal(0.005, 0.002, size=time.shape[0])\nfull_implementation_variance = np.random.normal(0.005, 0.002, size=time.shape[0])\n\n# Plot the mean CLIP scores.\nplt.plot(time, evolution_search_mean, label='Evolution Search', color='blue')\nplt.plot(time, translator_mean, label='Translator', color='orange')\nplt.plot(time, full_implementation_mean, label='Full Implementation', color='green')\n\n# Fill the area around the mean lines to show confidence intervals.\nplt.fill_between(time, evolution_search_mean - evolution_search_variance,\n                 evolution_search_mean + evolution_search_variance, color='blue', alpha=0.2)\n\nplt.fill_between(time, translator_mean - translator_variance,\n                 translator_mean + translator_variance, color='orange', alpha=0.2)\n\nplt.fill_between(time, full_implementation_mean - full_implementation_variance,\n                 full_implementation_mean + full_implementation_variance, color='green', alpha=0.2)\n\n# Add labels and title (if necessary).\nplt.xlabel('Running Time /s')\nplt.ylabel('CLIP Score')\nplt.legend()\nplt.grid(True)\n\n# Save the plot as a PNG file with 300 dpi.\nplt.savefig('clip_scores_plot.png', dpi=300)\n\n# Display the plot.\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.01311/ablation_studies_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.01311/ablation_studies.png",
        "caption": "Curves of CLIP scores increasing within 300s under three different module settings.",
        "source": "cvpr_3/2303.01311/Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation/main.tex",
        "arxiv_id": "cvpr_3/2303.01311",
        "llava_output": "The image is a line chart that shows the evolution of CLIP scores over time.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have the data stored in arrays\n# Replace this with your actual data\ntimes = np.linspace(0, 300, 300)  # Simulated time points\nscore_evolution_search = np.random.normal(0.3, 0.01, size=times.shape)  # Replace with actual data\nscore_translator = np.random.normal(0.29, 0.01, size=times.shape)       # Replace with actual data\nscore_full_implementation = np.random.normal(0.31, 0.01, size=times.shape)  # Replace with actual data\n\n# Standard error or confidence intervals could be included\n# These are just dummy arrays for illustration purposes\nerr_evolution_search = np.random.normal(0.005, 0.001, size=times.shape)\nerr_translator = np.random.normal(0.005, 0.001, size=times.shape)\nerr_full_implementation = np.random.normal(0.005, 0.001, size=times.shape)\n\n# Create the plot\nplt.figure(figsize=(10, 5))\n\n# Plot the data with shaded error regions\nplt.plot(times, score_evolution_search, label='Evolution Search', color='blue')\nplt.fill_between(times, score_evolution_search - err_evolution_search, score_evolution_search + err_evolution_search, color='blue', alpha=0.2)\n\nplt.plot(times, score_translator, label='Translator', color='orange')\nplt.fill_between(times, score_translator - err_translator, score_translator + err_translator, color='orange', alpha=0.2)\n\nplt.plot(times, score_full_implementation, label='Full Implementation', color='green')\nplt.fill_between(times, score_full_implementation - err_full_implementation, score_full_implementation + err_full_implementation, color='green', alpha=0.2)\n\n# Customize the plot\nplt.xlabel('Running Time /s')\nplt.ylabel('CLIP Score')\nplt.title('Curves of CLIP scores increasing within 300s under three different module settings')\nplt.legend()\nplt.grid(True)\n\n# Save the figure\nplt.savefig('clip_scores_plot.png', dpi=300)\n\n# Show the plot for interactive inspection, if desired\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate synthetic data\nnp.random.seed(0)\nx = np.linspace(0, 300, 300)\ny1 = 0.25 + np.random.rand(300) * 0.005 + np.linspace(0, 0.07, 300)\ny2 = 0.26 + np.random.rand(300) * 0.005 + np.linspace(0, 0.05, 300)\ny3 = 0.27 + np.random.rand(300) * 0.005 + np.linspace(0, 0.04, 300)\n\n# Introduce some noise\ne1 = np.random.normal(0.005, 0.002, size=x.shape)\ne2 = np.random.normal(0.005, 0.002, size=x.shape)\ne3 = np.random.normal(0.005, 0.002, size=x.shape)\n\n# Create plot\nplt.figure(figsize=(10, 6))\n\n# Plot each line with error bands\nplt.plot(x, y1, label='Evolution Search', color='blue')\nplt.fill_between(x, y1-e1, y1+e1, color='blue', alpha=0.2)\n\nplt.plot(x, y2, label='Translator', color='red')\nplt.fill_between(x, y2-e2, y2+e2, color='red', alpha=0.2)\n\nplt.plot(x, y3, label='Full Implementation', color='green')\nplt.fill_between(x, y3-e3, y3+e3, color='green', alpha=0.2)\n\n# Add grid, labels, and legend\nplt.grid(True)\nplt.xlabel('Running Time /s')\nplt.ylabel('CLIP Score')\nplt.legend()\n\n# Save plot to a PNG file with a resolution of 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.01311/ablation_studies_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.04341/loss.png",
        "caption": "Training curves of models w/ and w/o codebook. The models w/ codebooks converge faster than those w/o codebooks.",
        "source": "cvpr_3/2303.04341/camera_ready.tex",
        "arxiv_id": "cvpr_3/2303.04341",
        "llava_output": "The image is a line chart that shows the training steps of two models, one with a codebook and one without a codebook. The line chart is labeled with the names of the models and the type of codebook used. The chart indicates that the model with the codebook converges faster than the model without",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data (Replace with your actual data)\ntraining_step = range(1, 201)  # Assuming 200 training steps\nloss_with_codebook_k8 = [100 - 0.4*x for x in training_step]  # Placeholder data\nloss_with_codebook_k16 = [95 - 0.35*x for x in training_step]  # Placeholder data\nloss_without_codebook_k8 = [110 - 0.3*x for x in training_step if x < 100] + [70 for x in training_step if x >= 100]  # Placeholder data\nloss_without_codebook_k16 = [105 - 0.25*x for x in training_step if x < 100] + [65 for x in training_step if x >= 100]  # Placeholder data\n\n# Creating a figure and axis with specific size\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Plotting the data\nax.plot(training_step, loss_with_codebook_k8, linestyle='-', color='red', label='k=8, w/ codebook')\nax.plot(training_step, loss_with_codebook_k16, linestyle='-', color='blue', label='k=16, w/ codebook')\nax.plot(training_step[:len(loss_without_codebook_k8)], loss_without_codebook_k8, linestyle='--', color='red', label='k=8, w/o codebook')\nax.plot(training_step[:len(loss_without_codebook_k16)], loss_without_codebook_k16, linestyle='--', color='blue', label='k=16, w/o codebook')\n\n# Adding a legend with specific location outside the plot area\nax.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n\n# Adding titles and labels\nax.set_title('Training curves of models w/ and w/o codebook')\nax.set_xlabel('training step')\nax.set_ylabel('training loss')\n\n# Save plot as a png file with 300dpi\nplt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Assuming you have your training step data and corresponding loss data in these variables\n# Replace 'your_data_here' with your actual data arrays.\ntraining_steps = range(1, 201)\nk8_wo_codebook_loss = 'your_data_here'\nk16_wo_codebook_loss = 'your_data_here'\nk8_w_codebook_loss = 'your_data_here'\nk16_w_codebook_loss = 'your_data_here'\n\nplt.figure(figsize=(10, 7))\n\n# Plotting the curves - replace 'your_data_here' with actual loss data\nplt.plot(training_steps, k8_wo_codebook_loss, 'r--', label='k=8, w/o codebook')\nplt.plot(training_steps, k16_wo_codebook_loss, 'b--', label='k=16, w/o codebook')\nplt.plot(training_steps, k8_w_codebook_loss, 'r-', label='k=8, w/ codebook')\nplt.plot(training_steps, k16_w_codebook_loss, 'b-', label='k=16, w/ codebook')\n\n# Adding titles and labels\nplt.title('Training curves of models w/ and w/o codebook')\nplt.xlabel('training step')\nplt.ylabel('training loss')\n\n# Adding a legend\nplt.legend()\n\n# Save the figure\nplt.savefig('training_curves.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.04341/loss_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.10406/moti3.png",
        "caption": "Our shape generation model is a unified and efficient prior model to produce high-fidelity, diverse results on multiple tasks, while most previous approaches are task-specific.",
        "source": "cvpr_3/2303.10406/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2303.10406",
        "llava_output": "1. The image is a combination of a Bar Chart and a Line Chart presenting experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data for the bar chart\nbar_labels = ['Ours', 'AutoSDF', 'IMLE']\nbar_values = [0.35, 0.2, 0.15]\nbar_colors = ['blue', 'orange', 'green']\n\n# Create the bar chart\nplt.figure(figsize=(8, 6))\nplt.bar(bar_labels, bar_values, color=bar_colors)\nplt.ylabel('Diversity (MMD)')\nplt.title('Evaluation on language-guided generation')\nplt.savefig('bar_chart.png', dpi=300)\n\n# Sample data for the box plot\nbox_labels = ['Ours', 'AutoSDF', 'SeedFormer']\nbox_data = [np.random.rand(10) * 0.001, np.random.rand(10) * 0.002, np.random.rand(10) * 0.0015]\n\n# Create the box plot\nplt.figure(figsize=(8, 6))\nplt.boxplot(box_data, labels=box_labels)\nplt.ylabel('Completion error (MMD)')\nplt.title('Evaluation on shape completion')\nplt.savefig('box_plot.png', dpi=300)",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data for the bar plot\nbar_categories = ['Ours', 'AutoSDF', 'IMLE']\nbar_values = [0.3, 0.2, 0.15]\n\n# Sample data for the box plot\nbox_categories = ['Ours', 'AutoSDF', 'SeedFormer']\nbox_data = [np.random.rand(10) * 0.006, np.random.rand(10) * 0.004, np.random.rand(10) * 0.002]\n\n# Create a figure with subplots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n\n# Bar plot\nax1.bar(bar_categories, bar_values, color=['blue', 'orange', 'green'])\nax1.set_ylabel('Diversity (MMD)')\n\n# Box plot\nax2.boxplot(box_data, labels=box_categories)\nax2.set_ylabel('Completion error (MMD)')\n\n# Sample data for the scatter plot with a line\nscatter_x = np.array([50, 150, 250, 350, 450, 550, 650])\nscatter_y = np.array([80, 70, 60, 50, 40, 30, 20])\nline_x = np.linspace(0, 700, 100)\nline_y = np.sqrt(max(scatter_y) - (line_x / max(line_x) * max(scatter_y)))\n\n# Scatter plot with a line\nax3.scatter(scatter_x, scatter_y, label='Ours')\nax3.plot(line_x, line_y, label='Trend', color='red')\nax3.set_xlabel('Training Time (hours/GPU)')\nax3.set_ylabel('Average JSD on shape generation')\nax3.legend()\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.10406/moti3_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.10406/freq.png",
        "caption": "Power spectral density of different frequencies on 1000 generated shapes along with visual comparison. Our MFM layers successfully suppress high-frequency components.",
        "source": "cvpr_3/2303.10406/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2303.10406",
        "llava_output": "The image is a scatter plot that shows the power spectral density of different frequencies on 1000 generated shapes. The scatter plot is used to present the experiment results, with the x-axis representing the frequency and the y-axis representing the power spectral density. The plot also includes a comparison of the generated",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some dummy data\nfrequencies = np.linspace(0, 0.5, 100)\npsd_ours = np.exp(-frequencies * 10) + np.random.normal(0, 0.01, frequencies.shape)\npsd_ours_no_mfm = np.exp(-frequencies * 8) + np.random.normal(0, 0.015, frequencies.shape)\npsd_autosdf = np.exp(-frequencies * 6) + np.random.normal(0, 0.02, frequencies.shape)\n\n# Plot the dummy data\nplt.figure()\nplt.plot(frequencies, psd_ours, label='Ours', color='red')\nplt.plot(frequencies, psd_ours_no_mfm, label='Ours w/o MFM', color='salmon')\nplt.plot(frequencies, psd_autosdf, label='AutoSDF', color='green')\n\n# Set the scale to log for y-axis\nplt.yscale('log')\n\n# Label the axes, add title and legend, etc.\nplt.xlabel('Spatial volume frequencies (fraction of sampling rate)')\nplt.ylabel('Power spectral density')\nplt.title('Power spectral density of different frequencies on 1000 generated shapes')\nplt.legend()\n\n# Save the figure. Adjust the dpi to 300 for high resolution.\nplt.savefig('power_spectral_density.png', dpi=300)\n\n# Show the plot (not needed if you just want to save)\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some fake data\nx = np.linspace(0, 0.5, 100)\ny1 = np.exp(-5*x)  # Fake data for \"Ours\"\ny2 = np.exp(-4*x)  # Fake data for \"Ours w/o MFM\"\ny3 = np.exp(-3*x)  # Fake data for \"AutoSDF\"\n\n# Create the plot\nplt.figure()\nplt.loglog(x, y1, label='Ours', color='red')\nplt.loglog(x, y2, label='Ours w/o MFM', color='salmon')\nplt.loglog(x, y3, label='AutoSDF', color='green')\n\n# Add labels, title, legend, etc.\nplt.xlabel('Spatial volume frequencies (fraction of sampling rate)')\nplt.ylabel('Power spectral density')\nplt.title('Power Spectral Density Comparison')\nplt.legend()\n\n# Save the figure with high resolution\nplt.savefig('power_spectral_density_comparison.png', dpi=300)\n\n# Show the plot (optional, mostly used in interactive environments like Jupyter Notebook)\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.10406/freq_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.11674/PACS_low_pass_test.jpg",
        "caption": "Low-pass Filter on PACS.",
        "source": "cvpr_3/2303.11674/ms.tex",
        "arxiv_id": "cvpr_3/2303.11674",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulating some data (replace this with your actual data)\nfilter_sizes = np.linspace(20, 200, 100)\nresnet18 = np.tanh(0.02*(filter_sizes-20))\nresnet50 = np.tanh(0.025*(filter_sizes-20))\ngfnet = np.tanh(0.03*(filter_sizes-20))\naloft_s = np.tanh(0.035*(filter_sizes-20))\naloft_e = np.tanh(0.04*(filter_sizes-20))\n\n# Normalizing the simulated accuracy to a percentage scale\ndata_max = max(np.max(resnet18), np.max(resnet50), np.max(gfnet), np.max(aloft_s), np.max(aloft_e))\nresnet18 = resnet18 / data_max * 100\nresnet50 = resnet50 / data_max * 100\ngfnet = gfnet / data_max * 100\naloft_s = aloft_s / data_max * 100\naloft_e = aloft_e / data_max * 100\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nplt.plot(filter_sizes, aloft_e, color='red', linestyle='-', linewidth=2, label='ALOFT-E')\nplt.plot(filter_sizes, aloft_s, color='purple', linestyle='-', linewidth=2, label='ALOFT-S')\nplt.plot(filter_sizes, gfnet, color='cyan', linestyle='--', linewidth=2, label='GFNet')\nplt.plot(filter_sizes, resnet50, color='green', linestyle='-.', linewidth=2, label='ResNet-50')\nplt.plot(filter_sizes, resnet18, color='blue', linestyle=':', linewidth=2, label='ResNet-18')\n\n# Adding labels and title\nplt.xlabel('Filter size')\nplt.ylabel('Accuracy (%)')\nplt.title('Low-pass Filter on PACS')\nplt.legend()\n\n# Save the figure\nplt.savefig('low_pass_filter_pacs.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data approximating the trends in the graph\nfilter_sizes = np.linspace(20, 220, 200)\nresnet18 = np.clip(30 + np.log(filter_sizes) * 10, None, 80)\nresnet50 = np.clip(35 + np.log(filter_sizes) * 12, None, 82)\ngfnet = np.clip(40 + np.log(filter_sizes) * 8, None, 78)\naloft_s = np.clip(50 + np.log(filter_sizes) * 5, None, 85)\naloft_e = np.clip(52 + np.log(filter_sizes) * 5, None, 88)\n\n# Plots lines with specific styles as in the original graph\nplt.plot(filter_sizes, resnet18, 'b--', label='ResNet-18', linewidth=2, dashes=(5, 5))\nplt.plot(filter_sizes, resnet50, 'g--', label='ResNet-50', linewidth=2, dashes=(8, 5))\nplt.plot(filter_sizes, gfnet, 'c--', label='GFNet', linewidth=2, dashes=(3, 5))\nplt.plot(filter_sizes, aloft_s, 'm-', label='ALOFT-S', linewidth=2)\nplt.plot(filter_sizes, aloft_e, 'r-', label='ALOFT-E', linewidth=2)\n\n# Setting the labels and title\nplt.xlabel('Filter size')\nplt.ylabel('Accuracy (%)')\nplt.title('Low-pass Filter on PACS')\n\n# Adding a legend\nplt.legend()\n\n# Save the figure as a high-resolution png file\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.11674/PACS_low_pass_test_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.12071/rand_orth.png",
        "caption": "Illustration of the predicted trajectories by ProphNet-S with randomly (left) and orthogonally (right) initialized queries.",
        "source": "cvpr_3/2303.12071/appendix.tex",
        "arxiv_id": "cvpr_3/2303.12071",
        "llava_output": "The image is a scatter plot that shows the predicted trajectories by ProphNet-S with randomly and orthogonally initialized queries. The scatter plot is divided into two sections, with the left section displaying the randomly initialized queries and the right section displaying the orthogonally initialized queries. The plot is filled",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_trajectory():\n    # This function is a placeholder that will generate a synthetic trajectory.\n    # Real data would be needed for an exact replica of the plot.\n    return np.cumsum(np.random.randn(50, 2), axis=0)\n\n# Create figure and axis instances\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n# Example synthetic data for illustration purposes only\n# Agent history (past)\nhistory_x, history_y = create_trajectory().T\n# Predicted future positions\npredictions_x, predictions_y = create_trajectory().T\n# Ground truth future positions\nground_truth_x, ground_truth_y = create_trajectory().T\n\nfor ax in axs:\n    # Plot the map and road infrastructure layout (Here, just a simple representation)\n    for _ in range(5):\n        ax.plot(create_trajectory()[:, 0], create_trajectory()[:, 1], color='grey', alpha=0.5)\n    \n    # Plot the agent history\n    ax.scatter(history_x, history_y, label='Agent History', color='cyan')\n    # Plot the predicted future positions\n    ax.scatter(predictions_x, predictions_y, label='Prediction', color='blue')\n    # Plot the predicted endpoints\n    ax.scatter(predictions_x[-1], predictions_y[-1], label='Predicted Endpoints', color='blue', edgecolors='black')\n    # Plot the ground truth future positions\n    ax.scatter(ground_truth_x, ground_truth_y, label='Ground Truth', color='orange')\n    # Plot the ground truth endpoints\n    ax.scatter(ground_truth_x[-1], ground_truth_y[-1], label='Ground Truth Endpoints', color='red', edgecolors='black')\n    \n    # Set aspects of the plot for better visualization\n    ax.set_aspect('equal')\n    ax.legend()\n\nplt.tight_layout()\n# Save the plot as a PNG file\nplt.savefig('trajectory_prediction.png', dpi=300)\n\n# Show the plot on screen\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some example data (replace this with your actual data)\nagent_history_x = np.linspace(0, 5, 6)\nagent_history_y = np.exp(agent_history_x/3)\npredicted_traj_x = np.linspace(5, 10, 31)\npredicted_traj_y = np.exp(predicted_traj_x/3)\npredicted_endpoints_x = [predicted_traj_x[-1]]\npredicted_endpoints_y = [predicted_traj_y[-1]]\nground_truth_x = [7]\nground_truth_y = [np.exp(7/3)]\n\n# Create the figure and axes\nfig, ax = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True)\n\n# Plot the first subplot\nax[0].plot(agent_history_x, agent_history_y, 'b--', label='Agent History')  # Example for agent history\nax[0].plot(predicted_traj_x, predicted_traj_y, 'c.', label='Prediction')  # Example prediction trajectory\nax[0].scatter(predicted_endpoints_x, predicted_endpoints_y, color='blue', label='Predicted Endpoints')  # Example predicted endpoints\nax[0].scatter(ground_truth_x, ground_truth_y, color='red', label='Ground Truth Endpoints')  # Example ground truth endpoints\nax[0].legend()\n\n# Plot the second subplot (the contents would differ based on actual data or if you're showing something else)\n# For the example, I'll just copy the first subplot data, but in practice, this should be different\nax[1].plot(agent_history_x, agent_history_y, 'b--', label='Agent History')  # Example for agent history\nax[1].plot(predicted_traj_x, predicted_traj_y, 'c.', label='Prediction')  # Example prediction trajectory\nax[1].scatter(predicted_endpoints_x, predicted_endpoints_y, color='blue', label='Predicted Endpoints')  # Example predicted endpoints\nax[1].scatter(ground_truth_x, ground_truth_y, color='red', label='Ground Truth Endpoints')  # Example ground truth endpoints\nax[1].legend()\n\n# Customize the plot (axis labels, title, legend, etc.) depending on your requirements\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('plot.png', dpi=300)\n\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.12071/rand_orth_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.12071/teaser.png",
        "caption": "Overview of accuracy-and\u2013latency trade-off for the task of motion forecasting on Argoverse-1. ProphNet outperforms the state-of-the-art methods in prediction accuracy and considerably speeds up the agent-centric inference latency, leading to the best balance between accuracy and latency.",
        "source": "cvpr_3/2303.12071/arxiv.tex",
        "arxiv_id": "cvpr_3/2303.12071",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data points (example values, you should enter the exact numbers if you have them)\nnames = ['VectorNet', 'LaneGCN', 'mmTransformer', 'Wayformer', 'MultiPath++', 'DenseTNT', 'ProphNet']\nlatency = [400, 300, 250, 50, 100, 450, 80]  # Latency values\naccuracy = [1.5, 1.4, 1.35, 1.1, 1.15, 1.45, 1.05]  # Accuracy values (minFDE)\ncolors = ['red', 'red', 'red', 'blue', 'blue', 'blue', 'blue']  # Color for scene-centric (red) and agent-centric (blue)\n\n# Create the scatter plot\nplt.figure(figsize=[8, 6])\nfor i, name in enumerate(names):\n    plt.scatter(latency[i], accuracy[i], color=colors[i], label=name if i >= 3 else \"\", s=100)\n\n# Annotate the data points\nfor i, txt in enumerate(names):\n    plt.annotate(txt, (latency[i], accuracy[i]), textcoords=\"offset points\", xytext=(-10,+10), ha='center')\n\n# Set the axes labels\nplt.xlabel('Latency (ms)')\nplt.ylabel('minFDE (m)')\n\n# Set the grid and its style\nplt.grid(True, which='both', linestyle='--', linewidth=0.5, color='red')\n\n# Set the legend for method types\nplt.legend(['Scene-Centric', 'Agent-Centric'], loc='best', frameon=False)\n\n# Set the aspect ratio of the plot to be equal\n# This might not be necessary, depends on the original plot's aspect ratio\nplt.gca().set_aspect('auto', adjustable='box')\n\n# Save the plot with 300 DPI\nplt.savefig('accuracy_latency_tradeoff.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Estimated data points extracted from the image\ndata_points = {\n    'Scene-Centric': {\n        'VectorNet': (450, 1.5),\n        'LaneGCN': (300, 1.4),\n        'mmTransformer': (100, 1.35),\n    },\n    'Agent-Centric': {\n        'ProphNet': (50, 1.1),\n        'Wayformer': (150, 1.2),\n        'MultiPath++': (250, 1.25),\n        'DenseTNT': (400, 1.3),\n    }\n}\n\n# Plotting\nfig, ax = plt.subplots()\n\n# Plot scene-centric points\nfor name, point in data_points['Scene-Centric'].items():\n    ax.plot(point[0], point[1], 'ro')  # Red 'o' for Scene-Centric\n    ax.text(point[0], point[1], name)\n\n# Plot agent-centric points\nfor name, point in data_points['Agent-Centric'].items():\n    ax.plot(point[0], point[1], 'bo')  # Blue 'o' for Agent-Centric\n    ax.text(point[0], point[1], name)\n\n# Adding legend\nax.legend(['Scene-Centric', 'Agent-Centric'])\n\n# Set axis labels\nax.set_xlabel('Latency (ms)')\nax.set_ylabel('minFDE (m)')\n\n# Set grid\nax.grid(True, which='both', linestyle='--', color='r', linewidth=0.5)\n\n# Save the plot as a PNG file\nplt.savefig('accuracy_latency_tradeoff.png', dpi=300)\n\n# Show plot for verification\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.12071/teaser_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.12071/orthogonality.png",
        "caption": "Illustration of the orthogonality of learnable proposal queries before (right) and after (left) training.",
        "source": "cvpr_3/2303.12071/arxiv.tex",
        "arxiv_id": "cvpr_3/2303.12071",
        "llava_output": "The image is a scatter plot showing the orthogonality of learnable proposal queries before and after training.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate pre-training matrix\nnp.random.seed(0)\npre_training = np.random.rand(10, 10)\npre_training = (pre_training + pre_training.T)/2  # Make it symmetric for aesthetics\n\n# Simulate post-training matrix (diagonal with randomness)\npost_training = np.eye(10) + np.random.rand(10, 10) * 0.1\npost_training = np.tril(post_training) + np.tril(post_training, -1).T\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Plot pre-training matrix\ncax_1 = axes[0].matshow(pre_training, cmap='viridis')\nfig.colorbar(cax_1, ax=axes[0], fraction=0.046, pad=0.04)\n\n# Plot post-training matrix\ncax_2 = axes[1].matshow(post_training, cmap='viridis')\nfig.colorbar(cax_2, ax=axes[1], fraction=0.046, pad=0.04)\n\n# Turn off the axis labels\naxes[0].axis('off')\naxes[1].axis('off')\n\n# Adjust space between subplots\nplt.subplots_adjust(wspace=0.5)\n\n# Save the figure\nplt.savefig('orthogonality_queries.png', dpi=300)\n\nplt.show()",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Function to create the \"before training\" data with random values\ndef create_random_data(size):\n    return np.random.rand(size, size)\n\n# Function to create the \"after training\" data with diagonal values\ndef create_diagonal_data(size):\n    data = np.zeros((size, size))\n    np.fill_diagonal(data, 1)\n    return data + np.random.rand(size, size) * 0.1  # Add small noise\n\n# The size of the array could be inferred from the image; let's assume it's 10x10\nsize = 10\n\n# Create the \"before\" and \"after\" training data\nbefore_data = create_random_data(size)\nafter_data = create_diagonal_data(size)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Define a colormap similar to the one in the image\ncmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"navy\", \"darkblue\", \"blue\", \"yellow\"])\n\n# Plot \"before training\" data\nim_before = axes[0].imshow(before_data, cmap=cmap)\n\n# Plot \"after training\" data\nim_after = axes[1].imshow(after_data, cmap=cmap)\n\n# Add a colorbar\ncbar = plt.colorbar(im_after, ax=axes.ravel().tolist(), shrink=0.95)\n\n# Save the figure with 300 DPI\nplt.savefig('training_plots.png', dpi=300)\n\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.12071/orthogonality_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.12756/hyperparameters.png",
        "caption": "Recall@1 w.r.t $w$ and $\\tau$ on CIFAR100 dataset",
        "source": "cvpr_3/2303.12756/camera-ready.tex",
        "arxiv_id": "cvpr_3/2303.12756",
        "llava_output": "The image is a scatter plot showing the relationship between recall and the number of epochs for a neural network.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Example data (replace these with the actual data points)\nw_values = [0, 0.2, 0.5, 0.8, 1.0]\ndata_points = {\n    'tau_0': [42, 49, 58, 63, 65],\n    'tau_0.01': [43, 50, 59, 64, 66],\n    'tau_0.05': [44, 51, 60, 65, 67],\n    'tau_0.1': [45, 52, 61, 66, 68],\n    'tau_0.5': [46, 53, 62, 67, 69],\n    'tau_inf': [47, 54, 63, 68, 70]\n}\n\n# Plot style (colors and markers)\nstyles = {\n    'tau_0': ('-o', 'blue'),\n    'tau_0.01': ('-x', 'orange'),\n    'tau_0.05': ('-s', 'green'),\n    'tau_0.1': ('-^', 'red'),\n    'tau_0.5': ('-+', 'purple'),\n    'tau_inf': ('-D', 'brown')\n}\n\n# Create the plot\nplt.figure(figsize=(8, 5))\n\nfor tau_key, style in styles.items():\n    plt.plot(w_values, data_points[tau_key], style[0], color=style[1], label=tau_key.replace('_', ' = '))\n\nplt.xlabel('w')\nplt.ylabel('Recall(1) (%)')\nplt.title('Recall@1 w.r.t $w$ and $\\\\tau$ on CIFAR100 dataset')\nplt.legend()\nplt.grid(True)\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('recall_at_1.png', dpi=300)\n\n# Show the plot (optional, as the image has already been saved)\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data: Replace these lists with your actual data\nw_values = np.array([0.0, 0.2, 0.5, 0.8, 1.0])\n\nrecalls_for_t = {\n    'T=0': np.array([42, 50, 58, 62, 64]),\n    'T=0.01': np.array([44, 52, 59, 63, 65]),\n    'T=0.05': np.array([46, 54, 60, 64, 66]),\n    'T=0.1': np.array([47, 55, 61, 65, 67]),\n    'T=0.5': np.array([49, 56, 62, 66, 68]),\n    'T=\u221e': np.array([50, 57, 63, 67, 69])\n}\n\n# Define markers for each plot\nmarkers = ['o', 'X', '*', '^', 'P', 's']\n\n# Plotting the data\nfor (t, recalls), marker in zip(recalls_for_t.items(), markers):\n    plt.plot(w_values, recalls, marker=marker, label=t)\n\n# Setting up the plot labels and title\nplt.xlabel('w')\nplt.ylabel('Recall(1) (%)')\nplt.title('Recall@1 w.r.t $w$ and $\\\\tau$ on CIFAR100 dataset')\n\n# Adding a legend\nplt.legend()\n\n# Save plot as PNG file with 300 dpi\nplt.savefig('recall_wrt_w_and_tau.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.12756/hyperparameters_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.12756/num_sop.jpg",
        "caption": "Statistics on SOP dataset.",
        "source": "cvpr_3/2303.12756/camera-ready.tex",
        "arxiv_id": "cvpr_3/2303.12756",
        "llava_output": "The image is a bar chart showing the number of samples per class in a dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming data is available as lists\nsamples_per_class = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nclasses_with_samples = [4000, 3500, 3000, 2500, 2000, 1500, 1000, 750, 500, 250, 300] # Adjust these numbers based on your data\n\n# Create bar plot\nplt.figure(figsize=(10, 7.5)) # Adjust the figure size to match your requirements\nplt.bar(samples_per_class, classes_with_samples, color='blue')\n\n# Set labels and title\nplt.xlabel('Number of samples per class')\nplt.ylabel('Number of classes with number of samples')\n\n# Save the plot with 300 DPI\nplt.savefig('statistics_on_sop_dataset.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Placeholder data to mimic the bar chart\nsamples_per_class = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nnumber_of_classes = [4000, 3500, 3000, 2500, 2000, 1500, 1000, 750, 500, 250, 300]  # Example counts\n\n# Create the bar chart\nplt.bar(samples_per_class, number_of_classes, color='blue')\n\n# Label the axes\nplt.xlabel('Number of samples per class')\nplt.ylabel('Number of classes with number of samples')\n\n# Set the title (optional)\nplt.title('Statistics on SOP dataset')\n\n# Save the plot with 300 dpi\nplt.savefig('bar_chart.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.12756/num_sop_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.14080/IG_Emb_20.png",
        "caption": "Top 20 most impactful features for calculating embeddings determined using integrated gradient feature attribution method. The morphometric features are colored orange and comprise 15 of the 20 most impactful features.",
        "source": "cvpr_3/2303.14080/ms.tex",
        "arxiv_id": "cvpr_3/2303.14080",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder feature names and importance values\nfeatures = [\n    \"LVM\", \"LVESV\", \"LVEDV\", \"Whole body fat\", \"RVESV\", \"RVEDV\",\n    \"Weight\", \"Number of days/week walked 10+ minutes\", \"Body fat percentage\",\n    \"Freq. of consuming 6+ units of alcohol\", \"Body mass index\", \"Total mass\",\n    \"LVEF\", \"Waist circumference\", \"Freq. of walking in last 4 weeks\",\n    \"Whole body fat mass\", \"Average heart rate\", \"LSV\", \"Alcohol intake frequency\",\n    \"RVEF\"\n]\nimportance = [0.025] + [0.02]*4 + [0.015]*3 + [0.01]*12\nmorphometric = [True]*15 + [False]*5\n\n# Create a bar plot\nfig, ax = plt.subplots(figsize=(10, 6))\nbars = ax.bar(features, importance, color=['orange' if is_morph else 'blue' for is_morph in morphometric])\n\n# Add title and labels\nax.set_title('Top 20 Cardiac Embedding Feature Importance by Integrated Gradients')\nax.set_xlabel('Feature')\nax.set_ylabel('Importance')\n\n# Rotate the x-axis labels for better readability\nplt.xticks(rotation=45, ha='right')\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor='orange', label='Morphometric'),\n                   Patch(facecolor='blue', label='False')]\nax.legend(handles=legend_elements, loc='upper right')\n\n# Save the plot as a PNG file with 300 dpi\nplt.tight_layout() # Adjust the layout to make room for the rotated x-axis labels\nplt.savefig('feature_importance.png', dpi=300)\n\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Let's assume we have some example data for the 20 features and their importances\nfeatures = [\n    \"LVM\", \"LVESV\", \"LVEDV\", \"Whole body fat\", \"RVESV\", \n    \"RVEDV\", \"Weight\", \"Number of days/week walked 10+ minutes\", \n    \"Body fat percentage\", \"Freq. of consuming 6+ units of alcohol\", \n    \"Body mass index\", \"Total mass\", \"LVEF\", \"Waist circumference\", \n    \"Freq. of walking in last 4 weeks\", \"Whole body fat mass\", \n    \"Average heart rate\", \"LSV\", \"Alcohol intake frequency\", \"RVEF\"\n]\n\nimportance_values = [\n    0.030, 0.025, 0.022, 0.020, 0.018, \n    0.016, 0.015, 0.013, 0.012, 0.010, \n    0.009, 0.008, 0.007, 0.006, 0.005, \n    0.004, 0.003, 0.002, 0.001, 0.0005\n]\n\n# Assuming that the morphometric features are the first 15\ncolors = ['orange' if i < 15 else 'blue' for i in range(len(features))]\n\n# Plotting the data\nfig, ax = plt.subplots(figsize=(10,6))\nax.bar(features, importance_values, color=colors)\n\n# Adding the title\nax.set_title(\"Top 20 Cardiac Embedding Feature Importance by Integrated Gradients\")\n\n# Rotating the feature names on the x-axis for better readability\nplt.xticks(rotation=45, ha=\"right\")\n\n# Adding a legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor='orange', edgecolor='orange', label='Morphometric'),\n                   Patch(facecolor='blue', edgecolor='blue', label='False')]\nax.legend(handles=legend_elements, loc='upper right')\n\n# Adjust layout\nplt.tight_layout()\n\n# Saving the plot as a PNG file\nplt.savefig(\"feature_importance.png\", dpi=300)\n\n# Showing the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.14080/IG_Emb_20_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.14080/Top20DVM_AV_Physical.png",
        "caption": "Impact of features for calculating DVM embeddings determined using integrated gradient feature attribution method. The morphometric features are colored orange and comprise the four most impactful features.",
        "source": "cvpr_3/2303.14080/ms.tex",
        "arxiv_id": "cvpr_3/2303.14080",
        "llava_output": "1. Bar Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data (assuming hypothetical values)\nfeatures = ['Wheelbase', 'Length', 'Width', 'Height', 'Entry price', 'Gearbox',\n            'Price', 'Engine size', 'Bodytype', 'Fuel type', 'Reg year',\n            'Color', 'Door num', 'Runned miles', 'Seat num', 'Adv month']\nimportance = [0.017, 0.016, 0.015, 0.014, 0.011, 0.010,\n              0.009, 0.010, 0.009, 0.008, 0.007, 0.006, 0.005, 0.004, 0.003, 0.002]\nmorphometric = [True, True, True, True] + [False] * (len(features) - 4)\n\n# Colors for morphometric/non-morphometric features\ncolors = ['#FFA07A' if is_morphometric else '#6495ED' for is_morphometric in morphometric]\n\n# Create bar chart\nplt.figure(figsize=(16,6))\nbars = plt.bar(features, importance, color=colors)\n\n# Adding the title and labels\nplt.title('DVM Embedding Feature Importance by Integrated Gradients')\nplt.xlabel('Feature')\nplt.ylabel('Importance')\n\n# Create legend manually\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor='#FFA07A', label='Morphometric'),\n                   Patch(facecolor='#6495ED', label='False')]\nplt.legend(handles=legend_elements, title='Morphometric')\n\n# Save the figure\nplt.savefig('DVM_Embedding_Feature_Importance.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Assuming the feature importances are represented by the following values.\n# These are just example values since the exact numerical values can't be determined from the image.\nfeatures = [\n    'Wheelbase', 'Length', 'Width', 'Height', 'Entry price', \n    'Gearbox', 'Price', 'Engine size', 'Bodytype', 'Fuel type',\n    'Reg year', 'Color', 'Door num', 'Runed Miles', 'Seat num', 'Adv month'\n]\nimportances = [\n    0.017, 0.0165, 0.015, 0.0135, 0.011, \n    0.0105, 0.009, 0.0085, 0.008, 0.0075,\n    0.007, 0.006, 0.005, 0.004, 0.003, 0.002\n]\nmorphometric = [True, True, True, True] + [False] * (len(features) - 4)\n\n# Colors for the bars\ncolors = ['orange' if is_morph else 'blue' for is_morph in morphometric]\n\n# Create the bar plot\nplt.figure(figsize=(10, 5))\nbars = plt.bar(features, importances, color=colors)\n\n# Add title and labels\nplt.title('DVM Embedding Feature Importance by Integrated Gradients')\nplt.xlabel('Feature')\nplt.ylabel('Importance')\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor='blue', label='Morphometric', edgecolor='blue'),\n                   Patch(facecolor='orange', label='False', edgecolor='orange')]\nplt.legend(handles=legend_elements, loc='upper right')\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45, ha='right')\n\n# Save the figure\nplt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n\n# Show the plot, this line can be commented out if not needed\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.14080/Top20DVM_AV_Physical_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.14080/Training_Feature_Subsets.png",
        "caption": "Contrastive loss during multimodal pretraining. Training with only morphometric features converged to a similar loss of the baseline which included all 120 features. Training with no morphometric features had markedly less similarity between the projected embeddings of the same subject, showing the importance of the morphometric features for the multimodal training process.",
        "source": "cvpr_3/2303.14080/sm.tex",
        "arxiv_id": "cvpr_3/2303.14080",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming 500 epochs for the x-axis\nepochs = np.arange(1, 501)\n\n# Hypothetical data - replace these with your actual data\nbaseline_loss = np.random.normal(0, 0.1, size=500).cumsum() + 4\nmorphometric_only_loss = np.random.normal(0, 0.1, size=500).cumsum() + 3.5\nno_morphometric_loss = np.random.normal(0, 0.1, size=500).cumsum() + 5\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, baseline_loss, label='Baseline')\nplt.plot(epochs, morphometric_only_loss, label='Morphometric Only')\nplt.plot(epochs, no_morphometric_loss, label='No Morphometric')\n\n# Adding titles and labels\nplt.title('Cardiac Multimodal Contrastive Learning with Feature Subsets')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Adding a legend\nplt.legend()\n\n# Saving the plot to a PNG file with 300 DPI\nplt.savefig('contrastive_loss_plot.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Let's generate some mock data for demonstration purposes.\nepochs = np.arange(0, 500)\nbaseline_loss = np.exp(-epochs/100) + 2 + (np.random.rand(len(epochs)) - 0.5) * 0.3\nmorphometric_only_loss = np.exp(-epochs/110) + 2.1 + (np.random.rand(len(epochs)) - 0.5) * 0.3\nno_morphometric_loss = np.exp(-epochs/120) + 2.2 + (np.random.rand(len(epochs)) - 0.5) * 0.3\n\n# Create the plot\nplt.figure(figsize=(10, 7))\n\nplt.plot(epochs, baseline_loss, label='Baseline', color='orange')\nplt.plot(epochs, morphometric_only_loss, label='Morphometric Only', color='green')\nplt.plot(epochs, no_morphometric_loss, label='No Morphometric', color='blue')\n\n# Set title and labels for axes\nplt.title('Cardiac Multimodal Contrastive Learning with Feature Subsets')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Add a legend\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('contrastive_loss.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.14080/Training_Feature_Subsets_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.14773/grad_100dim.png",
        "caption": "(Left) loss curve and (right) noise sensitivity analysis of 100-Dimensional Rosenbrock optimization.",
        "source": "cvpr_3/2303.14773/main.tex",
        "arxiv_id": "cvpr_3/2303.14773",
        "llava_output": "The image is a line chart showing the loss curve and noise sensitivity analysis of a 100-Dimensional Rosenbrock optimization.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data generation (replace this with your actual data)\niterations = np.arange(0, 1000, 1)\nrgf = np.log10(iterations + 1)     # Simulate RGF data\nspsa = np.log10(iterations + 1)**2  # Simulate SPSA data\nspsa_gc = np.sqrt(iterations)       # Simulate SPSA-GC data\nsgd = iterations**0.5               # Simulate SGD data\nsgd_nag = np.exp(-iterations/200)*10# Simulate SGD-NAG data\n\n# Plotting the loss curves\nplt.figure()\nplt.plot(iterations, sgd_nag, label='SGD-NAG', color='black')\nplt.plot(iterations, sgd, label='SGD', linestyle='--', color='black')\nplt.plot(iterations, spsa_gc, label='SPSA-GC', color='red')\nplt.plot(iterations, spsa, label='SPSA', linestyle='-.', color='red')\nplt.plot(iterations, rgf, label='RGF', linestyle='-.', color='green')\n\n# Adding labels and title\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\n\n# Adding a legend\nplt.legend()\n\n# Saving the figure as a PNG file with 300 dpi\nplt.savefig('loss_curve.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Placeholder data - replace these with your actual data points\niterations = np.linspace(0, 1000, 100)\nloss_rgf = np.exp(-iterations * 0.01) * 10\nloss_spsa = np.exp(-iterations * 0.02) * 10\nloss_spsa_gc = np.exp(-iterations * 0.03) * 10\nloss_sgd = np.exp(-iterations * 0.04) * 10\nloss_sgd_nag = np.exp(-iterations * 0.05) * 10\n\n# Create figure and plot the placeholder loss data\nplt.figure(figsize=(6, 6))\nplt.plot(iterations, loss_rgf, 'g--', label='RGF')\nplt.plot(iterations, loss_spsa, 'r-.', label='SPSA')\nplt.plot(iterations, loss_spsa_gc, 'r-', label='SPSA-GC')\nplt.plot(iterations, loss_sgd, 'k-.', label='SGD')\nplt.plot(iterations, loss_sgd_nag, 'k-', label='SGD-NAG')\n\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('(Left) Loss Curve')\n\n# Save the plot with high resolution\nplt.savefig('loss_curve.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.14773/grad_100dim_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.14773/query_efficiency_0224.png",
        "caption": "Ablation study for backbone architecture. Classification accuracy on EuroSAT across pre-trained target backbone architectures and BlackVIP's Coordinators (SSL encoder backbone).",
        "source": "cvpr_3/2303.14773/main.tex",
        "arxiv_id": "cvpr_3/2303.14773",
        "llava_output": "2 Line Charts are provided to present the experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Fake data to approximate the look of the plots\nbudgets = [5, 10, 25, 50]  # Corresponding to 5K, 10K, 25K, 50K\nzs_svhn = [15, 18, 22, 25]\nvp_svhn = [20, 23, 30, 35]\nbar_svhn = [23, 27, 33, 40]\nblackvip_svhn = [25, 29, 35, 43]\n\nzs_clevr = [10, 13, 17, 20]\nvp_clevr = [15, 18, 23, 28]\nbar_clevr = [18, 22, 27, 33]\nblackvip_clevr = [20, 25, 30, 35]\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plotting SVHN data\naxes[0].plot(budgets, zs_svhn, 'k--', label='ZS')\naxes[0].plot(budgets, vp_svhn, 'c-*', label='VP w/ SPSA-GC')\naxes[0].plot(budgets, bar_svhn, 'y-o', label='BAR')\naxes[0].plot(budgets, blackvip_svhn, 'r-s', label='BlackVIP')\naxes[0].set_title('SVHN')\naxes[0].set_xlabel('Budget')\naxes[0].set_ylabel('Accuracy')\naxes[0].legend()\n\n# Plotting CLEVR data\naxes[1].plot(budgets, zs_clevr, 'k--', label='ZS')\naxes[1].plot(budgets, vp_clevr, 'c-*', label='VP w/ SPSA-GC')\naxes[1].plot(budgets, bar_clevr, 'y-o', label='BAR')\naxes[1].plot(budgets, blackvip_clevr, 'r-s', label='BlackVIP')\naxes[1].set_title('CLEVR')\naxes[1].set_xlabel('Budget')\n# Y-label intentionally left to avoid redundancy, as it's shared with the left plot\naxes[1].legend()\n\n# Tight layout to ensure everything fits without overlapping\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('ablation_study.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example budget data\nbudgets = np.array([5, 10, 25, 50])\n\n# Example accuracy data for each method\n# Replace with actual values\nzs_acc_svhn = [20, 23, 27, 28]\nvp_acc_svhn = [21, 24, 29, 33]\nbar_acc_svhn = [22, 25, 30, 35]\nblackvip_acc_svhn = [23, 27, 33, 40]\n\nzs_acc_clevr = [15, 18, 20, 22]\nvp_acc_clevr = [17, 20, 23, 26]\nbar_acc_clevr = [18, 21, 25, 28]\nblackvip_acc_clevr = [20, 24, 29, 33]\n\n# Create figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n# SVHN subplot\nax1.plot(budgets, zs_acc_svhn, 'ko--', label='ZS')\nax1.plot(budgets, vp_acc_svhn, 'c*-', label='VP w/ SPSA-GC')\nax1.plot(budgets, bar_acc_svhn, 'yo-', label='BAR')\nax1.plot(budgets, blackvip_acc_svhn, 'ro-', label='BlackVIP')\n\n# CLEVR subplot\nax2.plot(budgets, zs_acc_clevr, 'ko--', label='ZS')\nax2.plot(budgets, vp_acc_clevr, 'c*-', label='VP w/ SPSA-GC')\nax2.plot(budgets, bar_acc_clevr, 'yo-', label='BAR')\nax2.plot(budgets, blackvip_acc_clevr, 'ro-', label='BlackVIP')\n# Set plot titles and labels\nax1.set_title('SVHN')\nax2.set_title('CLEVR')\n\n# Set x and y axis labels\nfor ax in [ax1, ax2]:\n    ax.set_xlabel('Budget')\n    ax.set_ylabel('Accuracy')\n\n# Customize x-axis to show the budget with currency\nbudget_labels = ['5K ($6)', '10K ($12)', '25K ($30)', '50K ($60)']\nfor ax in [ax1, ax2]:\n    ax.set_xticks(budgets)\n    ax.set_xticklabels(budget_labels)\n\n# Add legends\nax1.legend()\nax2.legend()\n\n# Set y-axis limits if needed to match the ones on the image\nax1.set_ylim([15, 45])\nax2.set_ylim([15, 35])\nplt.tight_layout()\nplt.savefig('comparison_plot.png', dpi=300)",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.14773/query_efficiency_0224_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2303.15786/data_efficiency.jpg",
        "caption": "Data Efficiency Comparison",
        "source": "cvpr_3/2303.15786/main.tex",
        "arxiv_id": "cvpr_3/2303.15786",
        "llava_output": "The image is a line chart showing the performance of different data efficiency methods.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Example data - replace with your actual data\npercentages = [10, 30, 50, 70, 90, 100]\ngen_vlkt_full = [15, 18, 22, 25, 29, 32]\nhoiclip_full = [17, 20, 24, 28, 30, 33]\ngen_vlkt_rare = [10, 14, 17, 19, 22, 24]\nhoiclip_rare = [12, 15, 18, 21, 23, 25]\n\n# Plotting the lines\nplt.plot(percentages, gen_vlkt_full, marker='*', color='orange', label='GEN-VLKT on Full')\nplt.plot(percentages, hoiclip_full, marker='o', color='blue', label='HOICLIP on Full')\nplt.plot(percentages, gen_vlkt_rare, linestyle='--', marker='*', color='orange', label='GEN-VLKT on Rare')\nplt.plot(percentages, hoiclip_rare, linestyle='--', marker='o', color='blue', label='HOICLIP on Rare')\n\n# Adding the legend\nplt.legend()\n\n# Adding title and labels\nplt.title('Performance on Full and Rare Categories')\nplt.xlabel('Percentage (100%)')\nplt.ylabel('mAP')\n\n# Saving the plot as png file with 300dpi\nplt.savefig('data_efficiency_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Create some example data similar to what's shown in the image\npercentages = [20, 40, 60, 80, 100]\ngen_vlkt_full = [18, 22, 26, 29, 33]\nhoiclip_full = [17, 21, 24, 28, 32]\ngen_vlkt_rare = [15, 18, 22, 25, 29]\nhoiclip_rare = [16, 19, 23, 26, 30]\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.plot(percentages, gen_vlkt_full, marker='*', color='orange', linestyle='-', label='GEN-VLKT on Full')\nplt.plot(percentages, hoiclip_full, marker='*', color='blue', linestyle='-', label='HOICLIP on Full')\nplt.plot(percentages, gen_vlkt_rare, marker='*', color='orange', linestyle='--', label='GEN-VLKT on Rare')\nplt.plot(percentages, hoiclip_rare, marker='*', color='blue', linestyle='--', label='HOICLIP on Rare')\n\n# Annotate the plot like the example\nplt.title('Performance on Full and Rare Categories')\nplt.xlabel('Percentage (100%)')\nplt.ylabel('mAP')\nplt.legend(loc='upper left')\nplt.grid(True)\n\n# Save the plot to a PNG file with 300 dpi\nplt.savefig('data_efficiency_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2303.15786/data_efficiency_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.00733/sgdet_per_class_wc.jpg",
        "caption": "SGDET",
        "source": "cvpr_3/2304.00733/mR_per_class.tex",
        "arxiv_id": "cvpr_3/2304.00733",
        "llava_output": "The image is a scatter plot showing the relationship between two variables. The x-axis represents the variable \"head,\" while the y-axis represents the variable \"tail.\" The plot consists of multiple points, with some points closer to the origin and others farther away. The plot is labeled with the name \"temp",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\ncategories = ['HEAD', 'BODY', 'TAIL']\ntempura_values = [25, 23, 5]\nstran_values = [20, 18, 3]\ntrace_values = [10, 10, 1]\n\n# Set the width of the bars\nbar_width = 0.25\n\n# Set positions of the bars\nr1 = np.arange(len(tempura_values))\nr2 = [x + bar_width for x in r1]\nr3 = [x + bar_width for x in r2]\n\n# Creating the figure and a set of subplots\nfig, ax = plt.subplots()\n\n# Creating bars for each group\nplt.bar(r1, tempura_values, color='blue', width=bar_width, edgecolor='grey', label='TEMPURA')\nplt.bar(r2, stran_values, color='lightblue', width=bar_width, edgecolor='grey', label='STTran')\nplt.bar(r3, trace_values, color='skyblue', width=bar_width, edgecolor='grey', label='TRACE')\n\n# Adding labels\nplt.xlabel('Category', fontweight='bold', fontsize=15)\nplt.xticks([r + bar_width for r in range(len(tempura_values))], categories)\nplt.ylabel('mR@10', fontweight='bold', fontsize=15)\n\n# Adding a legend\nplt.legend()\n\n# Setting the figure size to match the original image\nfig.set_size_inches(7, 3.52)  # Size may need to be adjusted\n\n# Save the figure\nplt.savefig('barplot.png', dpi=300)\n\n# Show the plot (would display the plot in a window if running interactively)\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the data\ncategories = ['HEAD', 'BODY', 'TAIL']\ntempura = [25, 20, 5]\nsttran = [20, 15, 3]\ntrace = [10, 8, 1]\n\n# Define the position of the bars\nbarWidth = 0.25\nr1 = np.arange(len(tempura))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n\n# Create the plot\nplt.figure(figsize=(8, 4)) # Set the figure size as per the original image\n\n# Plot the bars\nplt.bar(r1, tempura, color='blue', width=barWidth, edgecolor='grey', label='TEMPURA')\nplt.bar(r2, sttran, color='lightblue', width=barWidth, edgecolor='grey', label='STTran')\nplt.bar(r3, trace, color='lightgrey', width=barWidth, edgecolor='grey', label='TRACE')\n\n# Add the labels and title\nplt.xlabel('Category', fontweight='bold')\nplt.ylabel('mR@10', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(tempura))], categories)\n\n# Create Legend\nplt.legend()\n\n# Add the grid (optional)\nplt.grid(axis='y', linestyle='--', linewidth=0.5)\n\n# Save the figure\nplt.savefig('bar_chart.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.00733/sgdet_per_class_wc_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.00733/comparative_per_class_recall.png",
        "caption": "\\small Comparative per class performance for PREDCLS task. Results are in terms of R@10 under ``with constraint''.",
        "source": "cvpr_3/2304.00733/predcls_per_class_line.tex",
        "arxiv_id": "cvpr_3/2304.00733",
        "llava_output": "The image is a line chart showing the performance of different classes in a PREDCLS task. The chart displays the R@10 values for each class, with the x-axis representing the class number and the y-axis representing the R@10 score. The chart has multiple lines, one for",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data - since exact data is not available from image\ncategories = [\n    'category1', 'category2', 'category3', 'category4', 'category5',\n    'category6', 'category7', 'category8', 'category9', 'category10',\n    'category11', 'category12', 'category13', 'category14', 'category15',\n    'category16', 'category17', 'category18', 'category19', 'category20'\n]\ntempura = [50, 60, 70, 40, 55, 65, 50, 75, 70, 40, 60, 50, 70, 60, 75, 55, 65, 50, 40, 30]\nsttran = [45, 55, 65, 35, 50, 60, 45, 70, 65, 35, 55, 45, 65, 55, 70, 50, 60, 45, 35, 25]\ntrace = [40, 50, 60, 45, 65, 75, 60, 85, 80, 55, 75, 65, 80, 70, 85, 65, 75, 60, 55, 45]\n\nplt.figure(figsize=(14, 8))\n\n# Plotting the data\nplt.plot(categories, tempura, marker='o', label='TEMPURA', color='blue')\nplt.plot(categories, sttran, marker='o', label='STTran', color='green')\nplt.plot(categories, trace, marker='o', label='TRACE', color='orange')\n\n# Adding titles and labels\nplt.title('Comparative per class performance for PREDCLS task')\nplt.xlabel('Categories')\nplt.ylabel('R@10')\nplt.xticks(rotation=90)  # Rotate the x-axis labels for better readability\n\n# Adding the legend\nplt.legend()\n\n# Save the figure\nplt.tight_layout()  # Adjust the plot to ensure everything fits without overlapping\nplt.savefig('performance_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Example categories and data\ncategories = [\"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\", \"cat8\", \"cat9\", \"cat10\", \"cat11\", \"cat12\"]\ntempura = [60, 70, 80, 50, 60, 70, 80, 50, 60, 70, 80, 50]\nsttran = [65, 75, 85, 55, 65, 75, 85, 55, 65, 75, 85, 55]\ntrace = [70, 80, 90, 60, 70, 80, 90, 60, 70, 80, 90, 60]\n\n# Create the plot\nplt.figure(figsize=(10, 6))  # Adjust the size to match your specific figure dimensions\n\nplt.plot(categories, tempura, marker='o', color='b', label='TEMPURA')\nplt.plot(categories, sttran, marker='o', color='g', label='STTran')\nplt.plot(categories, trace, marker='o', color='r', label='TRACE')\n\nplt.xticks(rotation=90)  # Rotate category labels to prevent overlap\nplt.xlabel('Categories', fontsize=14)  # Update with your actual axis label\nplt.ylabel('R@10', fontsize=14)\nplt.title('Comparative per class performance for PREDCLS task', fontsize=16)\nplt.legend()\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.00733/comparative_per_class_recall_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.00733/al_vs_epoch_predcls.jpg",
        "caption": "",
        "source": "cvpr_3/2304.00733/predictive_uncertainty_all.tex",
        "arxiv_id": "cvpr_3/2304.00733",
        "llava_output": "This is a line chart that shows the total aleotoric uncertainty. The chart has a green line that is labeled \"epochs\" and is plotted against the total aleotoric uncertainty. The chart also has a black line that is labeled \"epochs\" and is plotted",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# For reproducibility, let's simulate some data that might resemble the original plot.\n# Let's assume the original plot represents an exponential decay curve.\n\nepochs = np.arange(0, 10, 1)  # X-axis: epochs from 0 to 9\ndecay_constant = -0.5\ntotal_aleatoric_uncertainty = np.exp(decay_constant * epochs)  # Y-axis: Simulated data, exponential decay\n\n# Plotting the simulated data\nplt.figure()\nplt.plot(epochs, total_aleatoric_uncertainty, 'g--', label=\"Total Aleotoric Uncertainty\")  # Green dashed line\nplt.xlabel('Epochs')\nplt.ylabel('Uncertainty')\nplt.title('Total Aleotoric Uncertainty over Epochs')\nplt.legend()\nplt.tight_layout()\n\n# Save the plot with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the decay follows an exponential pattern, we create a similar dataset\nepochs = np.arange(10)  # 0 to 9\nuncertainty = np.exp(-epochs * 0.5)  # An example exponential decay\n\nplt.figure()\n\n# Plotting the uncertainty as a dashed line\nplt.plot(epochs, uncertainty, 'g--', label='Total Aleotoric Uncertainty')\n\n# Add legend\nplt.legend()\n\n# X and Y labels\nplt.xlabel('Epochs')\nplt.ylabel('Value')\n\n# Title of the plot\nplt.title('Aleotoric Uncertainty over Epochs')\n\n# Saving the figure as a png file with 300 dpi\nplt.savefig('aleotoric_uncertainty.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.00733/al_vs_epoch_predcls_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.00733/sgcls_tab4_per_class.png",
        "caption": "SGCLS",
        "source": "cvpr_3/2304.00733/sgdet_sgcls_ablations_per_class_wc.tex",
        "arxiv_id": "cvpr_3/2304.00733",
        "llava_output": "The image is a bar chart that shows the results of an experiment. The chart has four bars, each representing a different aspect of the experiment. The bars are labeled with the following terms: Head, Body, Tail, and Full Model. The chart is likely used to display the performance or effectiveness of",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data approximation from the provided image\ncategories = ['HEAD', 'BODY', 'TAIL']\nfull_model = [45, 40, 5]\nospu_mdu = [43, 38, 3]\nospu_gmm = [40, 37, 1]\ngmm_mdu = [35, 33, 0.5]\n\n# Setting up the bar width\nbarWidth = 0.2\n\n# Setting the position of the bars on the x-axis\nr1 = range(len(full_model))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\n\n# Creating the bars\nplt.bar(r1, full_model, width = barWidth, color = 'blue', edgecolor = 'black', label='FULL MODEL')\nplt.bar(r2, ospu_mdu, width = barWidth, color = 'lightblue', edgecolor = 'black', label='OSPU+MDU')\nplt.bar(r3, ospu_gmm, width = barWidth, color = 'skyblue', edgecolor = 'black', label='OSPU+GMM')\nplt.bar(r4, gmm_mdu, width = barWidth, color = 'steelblue', edgecolor = 'black', label='GMM+MDU')\n\n# Adding xticks\nplt.xlabel('Group', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(full_model))], categories)\n\n# Adding the legend and setting the title\nplt.title('SGCLS')\nplt.legend()\n\n# Saving the plot as a .png file with 300 dpi\nplt.savefig('SGCLS_plot.png', dpi=300)\n\n# Showing the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data assuming some values\ncategories = ['HEAD', 'BODY', 'TAIL']\nfull_model = [40, 35, 5]     # Replace with actual values\nospu_mdu = [38, 32, 2]       # Replace with actual values\nospu_gmm = [35, 30, 1]       # Replace with actual values\ngmm_mdu = [30, 25, 0.5]      # Replace with actual values\n\nbar_width = 0.2\nindex = np.arange(len(categories))\n\n# Create bars\nfig, ax = plt.subplots()\nbars1 = ax.bar(index, full_model, bar_width, color='blue', label='FULL MODEL')\nbars2 = ax.bar(index + bar_width, ospu_mdu, bar_width, color='lightblue', label='OSPU+MDU')\nbars3 = ax.bar(index + 2*bar_width, ospu_gmm, bar_width, color='lightgrey', label='OSPU+GMM')\nbars4 = ax.bar(index + 3*bar_width, gmm_mdu, bar_width, color='darkgrey', label='GMM+MDU')\n\n# Add labels, title, and custom x-axis tick labels, etc.\nax.set_xlabel('')\nax.set_ylabel('mRR@10')\nax.set_title('SGCLS')\nax.set_xticks(index + bar_width)\nax.set_xticklabels(categories)\nax.legend()\n\n# Save the figure\nplt.savefig('bar_chart.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.00733/sgcls_tab4_per_class_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.04420/EMRNet_features_all.png",
        "caption": "The feature distributions of EMRNet, FGRL-AUF and our proposed FRL-DGT on the evaluation datasets.",
        "source": "cvpr_3/2304.04420/FRL-DGT.tex",
        "arxiv_id": "cvpr_3/2304.04420",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Random seed for reproducibility\nnp.random.seed(0)\n\n# Generate random data to simulate the feature distributions\nnegative_x = np.random.normal(0, 1, 100)\nnegative_y = np.random.normal(0, 1, 100)\n\npositive_x = np.random.normal(1, 1, 100)\npositive_y = np.random.normal(1, 1, 100)\n\nsurprise_x = np.random.normal(2, 1, 100)\nsurprise_y = np.random.normal(2, 1, 100)\n\n# Create the plot\nplt.scatter(negative_x, negative_y, color='red', label='Negative')\nplt.scatter(positive_x, positive_y, color='green', label='Positive')\nplt.scatter(surprise_x, surprise_y, color='blue', label='Surprise')\n\n# Add legend\nplt.legend(loc='best')\n\n# Set limits for x and y axis\nplt.xlim(-3, 5)\nplt.ylim(-5, 4)\n\n# Save the figure as a PNG file with 300dpi\nplt.savefig('feature_distributions.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random sample data for demo purposes\nnp.random.seed(0) # for reproducibility\nnegative = np.random.randn(100, 2)\npositive = np.random.randn(100, 2) + 1\nsurprise = np.random.randn(100, 2) + [2, -2]\n\n# Create the scatter plot\nplt.scatter(negative[:, 0], negative[:, 1], color='red', label='Negative')\nplt.scatter(positive[:, 0], positive[:, 1], color='green', label='Positive')\nplt.scatter(surprise[:, 0], surprise[:, 1], color='blue', label='Surprise')\n\n# Customize the plot\nplt.legend()\nplt.xlim(-3, 5)\nplt.ylim(-5, 4)\n\n# Save the plot to a PNG file\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.04420/EMRNet_features_all_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.06287/0616_eval.png",
        "caption": "Left(a): Performance along the view coverage. Right(b): Trajectory with train, interpolation, and extrapolation viewpoints.",
        "source": "cvpr_3/2304.06287/5experiments.tex",
        "arxiv_id": "cvpr_3/2304.06287",
        "llava_output": "The image is a line chart that shows the performance along the view coverage.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data (Approximated from the given chart)\ncategories = ['<10', '10-20', '20-30', '30-40', '40-50']\npsnr_ours = [15, 17.5, 20, 22.5, 25]\npsnr_nerf = [18.34, 19.86, 22.59, 23.64, 26.60]\ndelta_psnr = [3.34, 2.36, 2.59, 1.14, 1.60]\n\n# Indices and width for the bars\nind = range(len(categories))\nwidth = 0.35\n\nfig, ax1 = plt.subplots()\n\n# First bar chart for PSNR values\nbars1 = ax1.bar(ind, psnr_ours, width, label='PSNR_ours', color='blue')\nbars2 = ax1.bar([i + width for i in ind], psnr_nerf, width, label='PSNR_nerf', color='orange')\n\n# Second y-axis for the Delta PSNR\nax2 = ax1.twinx()\nlines = ax2.plot(ind, delta_psnr, 'ro-', label='\u0394PSNR')\n\n# Annotating the Delta PSNR values\nfor i, txt in enumerate(delta_psnr):\n    ax2.annotate(f'+{txt}', (ind[i], delta_psnr[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Setting the x-axis labels\nax1.set_xticks([i + width/2 for i in ind])\nax1.set_xticklabels(categories)\n\nax1.set_xlabel('view coverage')\nax1.set_ylabel('PSNR', color='blue')\nax2.set_ylabel('\u0394PSNR', color='red')\n\nax1.tick_params(axis='y', labelcolor='blue')\nax2.tick_params(axis='y', labelcolor='red')\n\n# Adding a legend\nfig.tight_layout()\nfig.legend(loc='upper left', bbox_to_anchor=(0.15, 0.85))\n\n# Title of the plots\nplt.title('Performance along the view coverage')\n\n# Saving the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the figure\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming some example data similar to what's seen in the image provided\ncategories = ['<10', '10-20', '20-30', '30-40', '40-50']\nPSNR_ours = [18, 21, 23, 23.5, 24]\nPSNR_nerf = [15, 18.5, 20.5, 22.4, 22.5]\ndelta_PSNR = [3.34, 2.36, 2.59, 1.14, 1.60]\ndelta_annotations = [\"+3.34\", \"+2.36\", \"+2.59\", \"+1.14\", \"+1.60\"]\n\nx = np.arange(len(categories))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax1 = plt.subplots()\n\n# Plotting the bar plots\nbars1 = ax1.bar(x - width/2, PSNR_ours, width, label='PSNR_ours', color='blue')\nbars2 = ax1.bar(x + width/2, PSNR_nerf, width, label='PSNR_nerf', color='orange')\n\n# Set the y-axis label\nax1.set_ylabel('PSNR', color='blue')\nax1.set_ylim([10,30])\n\n# Plotting the line plot on the secondary y-axis\nax2 = ax1.twinx()\nline1 = ax2.plot(x, delta_PSNR, color='red', marker='o', linestyle='-', linewidth=2, markersize=5, label='\u0394PSNR')\nax2.set_ylabel('\u0394PSNR', color='red')\nax2.set_ylim([1,5])\nax2.tick_params(axis='y', labelcolor='red')\n\n# Annotate the line plot with values\nfor i, txt in enumerate(delta_annotations):\n    ax2.annotate(txt, (x[i], delta_PSNR[i]), textcoords=\"offset points\", xytext=(0,10), ha='center', color='red')\n\n# Setting the x-axis with categories\nax1.set_xticks(x)\nax1.set_xticklabels(categories)\nax1.set_xlabel('view coverage')\n\n# Adding the legend\nlines, labels = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(lines + lines2, labels + labels2, loc='upper left')\n\n# Set the grid\nax1.grid(True)\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('chart.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.06287/0616_eval_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.06537/validation.png",
        "caption": "The reliability diagrams of (a) the validation set before calibration, (b) the test set before calibration, and (c) the test set after calibration with temperature scaling.",
        "source": "cvpr_3/2304.06537/0.introduction.tex",
        "arxiv_id": "cvpr_3/2304.06537",
        "llava_output": "The image is a line chart showing the reliability diagrams of the validation set and test set before and after calibration.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have the following data from your model's predictions\nconfidence_bins = [0.1, 0.3, 0.5, 0.7, 0.9]  # Replace these with your actual bins.\naccuracy = [0.2, 0.4, 0.5, 0.6, 0.9]          # Replace with accuracy per bin.\ngap = [(conf - acc) for conf, acc in zip(confidence_bins, accuracy)]  # Calculate the gap.\n\n# Create a reliability diagram\nplt.bar(confidence_bins, accuracy, width=0.1, label='Accuracy', color='blue', edgecolor='k')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n\n# Add the gap (the difference between confidence and accuracy) as hatched areas\nfor i, (conf, acc) in enumerate(zip(confidence_bins, accuracy)):\n    plt.bar(conf, gap[i], bottom=acc, width=0.1, color='red', hatch='/', alpha=0.5)\n\n# Adding ECE text\nece_text = 'ECE: 5.67'  # You would calculate this from your model's predictions.\nplt.text(0.05, 0.9, ece_text, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n\n# Set the labels and title\nplt.xlabel('Confidence')\nplt.ylabel('Accuracy')\nplt.title('Reliability Diagram')\n\n# Create legend\nplt.legend()\n\n# Save the figure\nplt.savefig('reliability_diagram.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Assuming example values for the binned confidences and accuracies\nconfidence_bins = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\naccuracy = [0.15, 0.25, 0.35, 0.50, 0.55, 0.60, 0.75, 0.85, 0.90, 0.95]\n\n# Create the figure and the axes\nfig, ax = plt.subplots()\n\n# Plot the bar chart\nbars = ax.bar(confidence_bins, accuracy, width=0.1, edgecolor='k', label='Accuracy')\n\n# Add hatching pattern to the bars to show gap\nfor bar in bars:\n    bar.set_hatch('//')\n    bar.set_facecolor('blue')\n    bar.set_alpha(0.5)\n\n# Plot the diagonal line\nax.plot([0, 1], [0, 1], 'k--')\n\n# Add the legend with custom patches\naccuracy_patch = mpatches.Patch(color='blue', label='Accuracy')\ngap_patch = mpatches.Patch(facecolor='none', hatch='//', label='Gap')\nax.legend(handles=[accuracy_patch, gap_patch])\n\n# Add ECE text\nax.text(0.7, 0.2, 'ECE: 5.67', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n\n# Set labels and titles\nax.set_xlabel('Confidence')\nax.set_ylabel('Accuracy')\nax.set_title('Reliability Diagram')\n\n# Display the plot\nplt.savefig('reliability_diagram.png', dpi=300)  # Save the plot as a PNG file with 300 DPI\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.06537/validation_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.06537/CLTS0.998.png",
        "caption": "The reliability diagram of our method with (a) $\\alpha=0.998$, (b) $\\alpha=0.997$, and (c) $\\alpha=0.996$.",
        "source": "cvpr_3/2304.06537/3.experiment.tex",
        "arxiv_id": "cvpr_3/2304.06537",
        "llava_output": "The image is a line chart that presents the reliability diagram of a method with different values of $\\alpha$.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# These are fake data points to represent the figure you've shown.\n# Replace these with your actual data.\nconfidence = np.linspace(0, 1, 10)\naccuracy = np.array([0.1, 0.15, 0.2, 0.3, 0.35, 0.5, 0.65, 0.75, 0.85, 0.95])\ngap = accuracy + np.random.uniform(0.05, 0.1, len(accuracy)) - 0.025\n\n# Create the figure and axes\nfig, ax = plt.subplots()\n\n# Plot the accuracy bars\nax.bar(confidence, accuracy, width=0.1, color='blue', label='Accuracy', align='center')\n\n# Plot the gap as a hatched area\nax.bar(confidence, gap, width=0.1, color='none', hatch='//', edgecolor='red', label='Gap', align='center')\n\n# Plot the diagonal dashed line\nax.plot([0, 1], [0, 1], '--', color='gray')\n\n# Add text for Expected Calibration Error (ECE)\nplt.text(0.7, 0.9, 'ECE: 9.84', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n\n# Set the title and labels\nax.set_title('Reliability Diagram')\nax.set_xlabel('Confidence')\nax.set_ylabel('Accuracy')\n\n# Add the legend\nax.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('reliability_diagram.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data (please replace with your actual data)\nconfidence_bins = np.linspace(0, 1, 10)\naccuracy = np.random.rand(10)\ngap = np.random.rand(10) * 0.3  # Let's assume the gap is always smaller than accuracy.\n\n# Create the reliability diagram\nplt.bar(confidence_bins, accuracy, width=0.1, color='blue', label='Accuracy', edgecolor='black')\nplt.bar(confidence_bins, gap, width=0.1, bottom=accuracy - gap, color='none', hatch='/', edgecolor='red', label='Gap')\n\n# Add diagonal dotted line from (0, 0) to (1, 1)\nplt.plot([0, 1], [0, 1], 'k--')\n\n# Add ECE (Expected Calibration Error) text - this is a placeholder\nplt.text(0.7, 0.1, 'ECE: 9.84', fontsize=12, bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=1'))\n\n# Labeling the plot\nplt.xlabel('Confidence')\nplt.ylabel('Accuracy')\nplt.title('Reliability Diagram')\nplt.legend()\nplt.grid(True)\n\n# Set the axis limits\nplt.xlim(0, 1)\nplt.ylim(0, 1)\n\n# Save the figure\nplt.savefig('reliability_diagram.png', dpi=300)\n\n# Show the figure\nplt.show()",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.06537/CLTS0.998_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.08028/marin-two.png",
        "caption": "The prediction distribution of the SF-MD model assisted by SP and MAD on CASIA-SURF dataset. X-axis represents the normalized logit output and x=0.5 is the classification boundary. orange and blue dots denotes two different classes.",
        "source": "cvpr_3/2304.08028/PaperForReview.tex",
        "arxiv_id": "cvpr_3/2304.08028",
        "llava_output": "The image is a scatter plot, which shows the prediction distribution of the SF-MD model assisted by SP and MAD on the CASIA-SURF dataset. The x-axis represents the normalized logit output, and x=0.5 is the classification boundary. The orange and blue",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - replace with actual data\nnp.random.seed(0)\nclass1_data = np.random.rand(100, 2)\nclass2_data = np.random.rand(100, 2)\n\n# Function to plot a single subplot\ndef plot_data(ax, class1_data, class2_data, title):\n    ax.scatter(class1_data[:, 0], class1_data[:, 1], color='orange', label='Class 1')\n    ax.scatter(class2_data[:, 0], class2_data[:, 1], color='blue', label='Class 2')\n    ax.axvline(x=0.5, color='k', linestyle='--')  # Classification boundary\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_title(title)\n    ax.set_xlabel('Normalized logit output')\n    ax.legend(loc='upper right')\n\n# Set up the figure with 2 subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\n# Plot data for SP and MAD\nplot_data(axs[0], class1_data, np.zeros_like(class1_data), \"(a) SP\")\nplot_data(axs[1], np.zeros_like(class2_data), class2_data, \"(b) MAD\")\n\n# Adjust the layout and save the plot\nplt.tight_layout()\nplt.savefig('prediction_distribution.png', dpi=300)\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Since we don't have the actual data points, we'll generate some dummy data\n# that mimics the distribution we see in the example image.\nnp.random.seed(0)\ndata_sp_class1 = np.random.normal(loc=0.2, scale=0.1, size=100)\ndata_sp_class2 = np.random.normal(loc=0.8, scale=0.1, size=100)\ndata_mad_class1 = np.random.normal(loc=0.2, scale=0.1, size=100)\ndata_mad_class2 = np.random.normal(loc=0.8, scale=0.1, size=100)\n\n# Dummy Y values to generate a vertical scatter\ny_sp_class1 = np.random.uniform(low=0, high=1, size=100)\ny_sp_class2 = np.random.uniform(low=0, high=1, size=100)\ny_mad_class1 = np.random.uniform(low=0, high=1, size=100)\ny_mad_class2 = np.random.uniform(low=0, high=1, size=100)\n\n# Set up the figure and subplots\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\n# Plot for SP\naxes[0].scatter(data_sp_class1, y_sp_class1, color='orange')\naxes[0].scatter(data_sp_class2, y_sp_class2, color='blue')\naxes[0].axvline(x=0.5, color='black', linestyle='--')\naxes[0].set_title('(a) SP')\naxes[0].set_xlim([0, 1])\n\n# Plot for MAD\naxes[1].scatter(data_mad_class1, y_mad_class1, color='orange')\naxes[1].scatter(data_mad_class2, y_mad_class2, color='blue')\naxes[1].axvline(x=0.5, color='black', linestyle='--')\naxes[1].set_title('(b) MAD')\naxes[1].set_xlim([0, 1])\n\n# Adjust the spacing between the plots\nplt.tight_layout()\n\n# Save as PNG file with 300 DPI\nplt.savefig('scatter_plot.png', dpi=300)\n\n# Show the plot (not necessary if you only want to save the figure)\nplt.show()",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.08028/marin-two_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "cvpr_figures_2/2304.11598/convergence.png",
        "caption": "The loss curve (mini-ImageNet, tiered-ImageNet).",
        "source": "cvpr_3/2304.11598/arxiv_version.tex",
        "arxiv_id": "cvpr_3/2304.11598",
        "llava_output": "The image is a line chart showing the loss curve for two different networks, mini-ImageNet and tiered-ImageNet.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have the epoch numbers in lists\nepochs = list(range(1, 51))\n\n# You need to replace these sample loss values with your actual data\nmini_imagenet_loss = np.random.normal(loc=1, scale=0.5, size=50).cumsum() + 1000000\ntiered_imagenet_loss = np.random.normal(loc=1, scale=0.3, size=50).cumsum() + 950000\n\n# Making sure the generated data has a downward trend for visualization purposes\nmini_imagenet_loss = np.sort(mini_imagenet_loss)[::-1]\ntiered_imagenet_loss = np.sort(tiered_imagenet_loss)[::-1]\n\nplt.figure(figsize=(10, 6))\nplt.plot(epochs, mini_imagenet_loss, 'o-', label='mini-ImageNet', color='blue')\nplt.plot(epochs, tiered_imagenet_loss, 'o-', label='tiered-ImageNet', color='red')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss convergence')\nplt.legend()\nplt.grid(True)\n\n# Save the figure with a resolution of 300 DPI\nplt.savefig('loss_curve.png', dpi=300)\n\n# Optionally, display the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some data\nepochs = np.arange(1, 51, 1)  # 1 through 50\n# Assuming we have an exponential decay of loss just for illustration purposes\nmini_imagenet_loss = 1.2e6 * np.exp(-0.1 * epochs) + 5e5\ntiered_imagenet_loss = 1.1e6 * np.exp(-0.1 * epochs) + 5.2e5\n\n# Create the plot\nplt.figure(figsize=(10, 6))  # Adjust the figure size as needed\nplt.plot(epochs, mini_imagenet_loss, 'o-', label='mini-ImageNet', color='blue')\nplt.plot(epochs, tiered_imagenet_loss, 'o-', label='tiered-ImageNet', color='red')\n\n# Add title and labels with font-size adjustments\nplt.title('Loss convergence', fontsize=16)\nplt.xlabel('Epochs', fontsize=14)\nplt.ylabel('Loss', fontsize=14)\n\n# Set the xticks to match the plot provided\nplt.xticks(np.arange(min(epochs), max(epochs)+1, 3))\nplt.yticks(fontsize=12)\n\n# Set the range for the y-axis if needed to match the plot\nplt.ylim(500000, 1300000)\n\n# Add a legend\nplt.legend()\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('loss_convergence.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2304.11598/convergence_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "cvpr_figures_2/2305.00562/cifar100_tcfg_fid.png",
        "caption": "FID",
        "source": "cvpr_3/2305.00562/PaperSuppMaterials.tex",
        "arxiv_id": "cvpr_3/2305.00562",
        "llava_output": "1. The image is a line chart that shows the results of an experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Replace these lists with the actual data points from the paper\nomega_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]  # Example x-axis values\ntfcg_values = [6, 7, 9, 13, 16, 18]  # Example TCFG values\ncfg_values = [5, 6, 8, 11, 14, 17]  # Example CFG values\n\n# Create the plot\nplt.figure()\nplt.plot(omega_values, tfcg_values, marker='o', linestyle='-', color='blue', label='TCFG')\nplt.plot(omega_values, cfg_values, marker='o', linestyle='-', color='red', label='CFG')\n\n# Add labels and legend\nplt.xlabel('\u03c9')\nplt.ylabel('IS')\nplt.legend()\n\n# Customize the grid\nplt.grid(True)\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 2: Generate synthetic data\n# These numbers are example data; you should replace them with your actual data.\nomega = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0])\nTCFG_values = np.array([6, 8, 10, 13, 16, 18])\nCFG_values = np.array([5, 7, 9, 11, 14, 17])\n\n# Step 3: Create the plot\nplt.figure()\nplt.plot(omega, TCFG_values, marker='o', color='blue', label='TCFG')\nplt.plot(omega, CFG_values, marker='o', color='red', label='CFG')\nplt.xlabel(r'$\\omega$')\nplt.ylabel(r'IS')\nplt.title('FID')\nplt.legend()\n\n# Optional: Customize your plot's appearance with `matplotlib` functions\n\n# Step 4: Save the plot to a PNG file\nplt.savefig('plot.png', dpi=300)\nplt.show()  # If you also want to display it in your Python environment",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/cvpr_3/2305.00562/cifar100_tcfg_fid_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2207.11209/density_cal.png",
        "caption": "Point Density.",
        "source": "iccv_1/2207.11209/PaperForReview.tex",
        "arxiv_id": "iccv_1/2207.11209",
        "llava_output": "The image is a scatter plot showing the density of points on a sphere.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a 3D axis\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the points on the sphere\ntheta = np.linspace(0, 2*np.pi, 100)\nphi = np.linspace(0, np.pi, 100)\ntheta, phi = np.meshgrid(theta, phi)\nx = np.sin(phi) * np.cos(theta)\ny = np.sin(phi) * np.sin(theta)\nz = np.cos(phi)\n\nax.plot_wireframe(x, y, z, color='black', linewidth=0.5, rstride=5, cstride=5)\n\n# Plot the dots on the sphere\ndots_theta = [0, np.pi/2, np.pi, 3*np.pi/2]\ndots_phi = [np.pi/6, 5*np.pi/6]\ndots_x = []\ndots_y = []\ndots_z = []\nfor dt in dots_theta:\n    for dp in dots_phi:\n        dots_x.append(np.sin(dp) * np.cos(dt))\n        dots_y.append(np.sin(dp) * np.sin(dt))\n        dots_z.append(np.cos(dp))\nax.scatter(dots_x, dots_y, dots_z, color='black', s=50)\n\n# Plot the line and point P in the middle\nax.plot([0, 0], [0, 0], [0, 2], color='red', linewidth=2.5)\nax.scatter([0], [0], [1.5], color='red', s=100, label='P')\nax.text(0, 0, 1.6, 'P', color='red', fontsize=15, verticalalignment='bottom')\n\n# Set the limits\nax.set_xlim([-1.5, 1.5])\nax.set_ylim([-1.5, 1.5])\nax.set_zlim([0, 2])\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Hide the axes\nax.axis('off')\n\n# Save the figure with 300 dpi resolution\nplt.savefig('sphere_plot.png', dpi=300, bbox_inches='tight')\nplt.show()",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2207.11209/density_cal_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2207.11209/sen_map_c.png",
        "caption": "Parameter Sensitivity Analysis.",
        "source": "iccv_1/2207.11209/PaperForReview.tex",
        "arxiv_id": "iccv_1/2207.11209",
        "llava_output": "The image is a line chart that shows the sensitivity of a parameter in an experiment. The chart has a blue line and a red line, with the blue line representing the parameter sensitivity and the red line representing the parameter value. The chart also includes a vertical axis and a horizontal axis, indicating the values of the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data for d\u03b8 values\nd_theta = np.linspace(10, 60, 500)\n\n# Sample functions for r^d values and DKNet (these are dummy functions and may need to be adjusted)\nrd_002 = 58 - 0.1 * (d_theta - 10)\nrd_004 = 56 - 0.1 * (d_theta - 10)\nrd_006 = 54 - 0.1 * (d_theta - 10)\ndknet = 51 * np.ones_like(d_theta)  # constant line for DKNet\n\n# Create the plot\nfig, ax = plt.subplots()\n\nax.plot(d_theta, rd_002, 'k--', label='r^d=0.02')\nax.plot(d_theta, rd_004, 'k-.', label='r^d=0.04')\nax.plot(d_theta, rd_006, 'k:', label='r^d=0.06')\nax.plot(d_theta, dknet, 'b-', label='DKNet')\n\n# Setting the labels and title\nax.set_xlabel('d\u03b8')\nax.set_ylabel('mAP')\nax.set_ylim(48, 58)\nax.legend(loc='upper right')\n\n# Save the plot as a PNG with 300 dpi\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming d0 values range from 10 to 60\nd0 = np.linspace(10, 60, 100)\n\n# Approximating values for each line from the plot\nrd_002 = np.linspace(55, 53, 100)\nrd_004 = np.linspace(54, 52, 100)\nrd_006 = np.linspace(53, 51, 100)\nDKNet = np.full((100,), 50)  # DKNet seems to be a constant line at y=50\n\n# Plotting the data\nplt.plot(d0, rd_002, label=\"rd_002\")\nplt.plot(d0, rd_004, label=\"rd_004\")\nplt.plot(d0, rd_006, label=\"rd_006\")\nplt.plot(d0, DKNet, label=\"DKNet\", linestyle=\"--\")\nplt.xlabel(\"d0\")\nplt.ylabel(\"Value\")\nplt.title(\"Data Plot\")\nplt.legend()\nplt.show()",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2207.11209/sen_map_c_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 5,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "iccv_figures_1/2210.01208/heatmap.jpg",
        "caption": "Illustration of distributions of (a) post-activation distribution in ANN, and (b-c) cumulative membrane potential distributions of SNN model with BN and LN, respectively. The heatmap shows a similar distribution between ANN and SNN(BN) model, but the distribution between ANN and SNN(LN) is quite different, which leads to performance degradation.",
        "source": "iccv_1/2210.01208/egpaper_for_review.tex",
        "arxiv_id": "iccv_1/2210.01208",
        "llava_output": "The image is a line chart showing the distribution of post-activation and cumulative membrane potential distributions of ANN, SNN with BN, and SNN with LN.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data (you need to replace this with the actual data from your research)\ndata_ann = np.random.rand(10, 10)  # For ANN\ndata_snn_bn = np.random.rand(10, 10)  # For SNN(BN)\ndata_snn_ln = np.random.rand(10, 10)  # For SNN(LN)\n\n# Create a figure and subplots\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\n\n# Plotting heatmaps\ncmap = \"bwr\"  # Blue-White-Red colormap, you can adjust this\n\nim1 = axes[0].imshow(data_ann, cmap=cmap, aspect='auto')\naxes[0].set_title(\"(a) ANN\")\n\nim2 = axes[1].imshow(data_snn_bn, cmap=cmap, aspect='auto')\naxes[1].set_title(\"(b) SNN (BN)\")\n\nim3 = axes[2].imshow(data_snn_ln, cmap=cmap, aspect='auto')\naxes[2].set_title(\"(b) SNN (LN)\")\n\n# Remove x and y ticks as they are not present in your figure\nfor ax in axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Add colorbar\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\nfig.colorbar(im3, cax=cbar_ax)\n\n# Save the figure with 300dpi\nplt.tight_layout()\nplt.subplots_adjust(right=0.9)\nplt.savefig(\"heatmap.png\", dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data. Replace with your actual data.\ndata_ANN = np.random.rand(10, 10)\ndata_SNN_BN = np.random.rand(10, 10)\ndata_SNN_LN = np.random.rand(10, 10)\n\n# Create a figure and a 1x3 subplot.\nfig, axarr = plt.subplots(1, 3, figsize=(10, 3))\n\n# Plot data using imshow.\ncax1 = axarr[0].imshow(data_ANN, cmap='bwr', vmin=-1, vmax=1)\ncax2 = axarr[1].imshow(data_SNN_BN, cmap='bwr', vmin=-1, vmax=1)\ncax3 = axarr[2].imshow(data_SNN_LN, cmap='bwr', vmin=-1, vmax=1)\n\n# Set titles.\naxarr[0].set_title('(a) ANN')\naxarr[1].set_title('(b) SNN (BN)')\naxarr[2].set_title('(b) SNN (LN)')\n\n# Remove ticks.\nfor ax in axarr:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n# Add a colorbar.\ncbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\nfig.colorbar(cax3, cax=cbar_ax)\n\n# Save the figure with 300dpi.\nplt.savefig('heatmap.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2210.01208/heatmap_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2210.01208/resneterror.jpg",
        "caption": "The effectiveness of the RSM method in (a) SA and (b) MLP modules of the MST model, as well as in Spiking (c) ResNet18 and (d) VGG16 models, with varying masking ratios. The inset photographs show the standard deviation of accuracy in 10 runs.",
        "source": "iccv_1/2210.01208/egpaper_for_review.tex",
        "arxiv_id": "iccv_1/2210.01208",
        "llava_output": "4 Line Charts are provided to present the experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\ndatasets = ['CIFAR-100', 'ImageNet']\ntime_steps = np.logspace(0, 4, num=5)\n\nmodels = {\n    'MST': {'accuracy': [86.91, 78.51], 'time_steps': [10**3, 10**3], 'size': [40, 40], 'marker': '*'},\n    'Spikformer': {'accuracy': [82, 74], 'time_steps': [10**1, 10**1], 'size': [10, 10], 'marker': 'o'},\n    'QCFS': {'accuracy': [82.5, 74.5], 'time_steps': [10**1.5, 10**1.5], 'size': [20, 20], 'marker': 'o'},\n    'DSR': {'accuracy': [80, 70], 'time_steps': [10**1, 10**1], 'size': [10, 10], 'marker': 'o'},\n    'TET': {'accuracy': [78, 68], 'time_steps': [10**0.5, 10**0.5], 'size': [10, 10], 'marker': 'o'},\n    'RNL': {'accuracy': [79, 69], 'time_steps': [10**0.5, 10**0.5], 'size': [20, 20], 'marker': 'o'},\n    'tdBN': {'accuracy': [76, 66], 'time_steps': [10**1, 10**1], 'size': [20, 20], 'marker': 'o'},\n    'Diet-SNN': {'accuracy': [78, 70], 'time_steps': [10**2, 10**2], 'size': [20, 20], 'marker': 'o'},\n    'RMP': {'accuracy': [76.5, 69], 'time_steps': [10**3.5, 10**3.5], 'size': [20, 20], 'marker': 'o'}\n}\n\n# Plot\nfig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\nfor i, dataset in enumerate(datasets):\n    for model, data in models.items():\n        axs[i, 0].scatter(data['time_steps'][0], data['accuracy'][0], s=data['size'][0]*20, label=model if i == 0 else \"\", marker=data['marker'])\n        axs[i, 1].scatter(data['time_steps'][1], data['accuracy'][1], s=data['size'][1]*20, label=model if i == 0 else \"\", marker=data['marker'])\n    \n    axs[i, 0].set_title(dataset)\n    axs[i, 1].set_title(dataset)\n    axs[i, 0].set_xlabel('Time Steps')\n    axs[i, 1].set_xlabel('Time Steps')\n    axs[i, 0].set_ylabel('Accuracy (%)')\n    axs[i, 1].set_ylabel('Accuracy (%)')\n    axs[i, 0].set_xscale('log')\n    axs[i, 1].set_xscale('log')\n    axs[i, 0].set_xticks(time_steps)\n    axs[i, 1].set_xticks(time_steps)\n    axs[i, 0].set_xticklabels(time_steps)\n    axs[i, 1].set_xticklabels(time_steps)\n    axs[i, 0].grid(True, which=\"both\", ls=\"--\", c='0.7')\n    axs[i, 1].grid(True, which=\"both\", ls=\"--\", c='0.7')\n\naxs[0, 0].legend(loc=\"upper left\")\n\n# Save the figure\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Step 1: Define the sample data (replace with your data)\nmasking_ratios = list(range(101))  # 0 to 100\nsa_cifar10 = [97.2 - 0.01 * i for i in masking_ratios]  # Placeholder data\nmlp_cifar10 = [96.5 - 0.015 * i for i in masking_ratios]  # Placeholder data\nsa_cifar100 = [86.4 - 0.02 * i for i in masking_ratios]  # Placeholder data\nmlp_cifar100 = [84.5 - 0.025 * i for i in masking_ratios]  # Placeholder data\nresnet18 = [100 - 0.5 * i for i in masking_ratios]  # Placeholder data\nvgg16 = [95 - 0.45 * i for i in masking_ratios]  # Placeholder data\n\n# Step 2: Initialize the figure with 2x2 subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Step 3: Plot the data on each subplot\n# (a) SA and MLP for CIFAR-10\naxs[0, 0].plot(masking_ratios, sa_cifar10, 'r-^', label='SA')\naxs[0, 0].plot(masking_ratios, mlp_cifar10, 'b--s', label='MLP')\naxs[0, 0].set_title('CIFAR-10')\naxs[0, 0].legend()\n\n# (b) SA and MLP for CIFAR-100\naxs[0, 1].plot(masking_ratios, sa_cifar100, 'r-^', label='SA')\naxs[0, 1].plot(masking_ratios, mlp_cifar100, 'b--s', label='MLP')\naxs[0, 1].set_title('CIFAR-100')\n\n# (c) ResNet18\naxs[1, 0].plot(masking_ratios, resnet18, 'r-', label='ResNet18')\naxs[1, 0].set_title('Spiking ResNet18')\n\n# (d) VGG16\naxs[1, 1].plot(masking_ratios, vgg16, 'b--', label='VGG16')\naxs[1, 1].set_title('Spiking VGG16')\n\n# Step 4: Add inset plots for standard deviations\n# Placeholder standard deviation data\nstd_deviation = [0.1 * i for i in masking_ratios]  # Placeholder data\naxins_cifar10 = axs[0, 0].inset_axes([0.1, 0.1, 0.3, 0.3])\naxins_cifar10.plot(masking_ratios, std_deviation, 'k-')\naxins_cifar10.set_title('Std Deviation')\n\naxins_cifar100 = axs[0, 1].inset_axes([0.1, 0.1, 0.3, 0.3])\naxins_cifar100.plot(masking_ratios, std_deviation, 'k-')\n\naxins_resnet18 = axs[1, 0].inset_axes([0.6, 0.1, 0.3, 0.3])\naxins_resnet18.plot(masking_ratios, std_deviation, 'k-')\n\naxins_vgg16 = axs[1, 1].inset_axes([0.6, 0.1, 0.3, 0.3])\naxins_vgg16.plot(masking_ratios, std_deviation, 'k-')\n\n# Step 5: Save the figure\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2210.01208/resneterror_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2210.01208/blockcompare.jpg",
        "caption": "Comparison of the accuracy of masking different numbers of blocks on the CIFAR-100 dataset.",
        "source": "iccv_1/2210.01208/egpaper_for_review.tex",
        "arxiv_id": "iccv_1/2210.01208",
        "llava_output": "The image is a bar chart that compares the accuracy of masking different numbers of blocks on the CIFAR-100 dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\ndatasets = ['CIFAR-100', 'ImageNet']\ntime_steps = np.logspace(0, 4, num=5)\n\nmodels = {\n    'MST': {'accuracy': [86.91, 78.51], 'time_steps': [10**3, 10**3], 'size': [40, 40], 'marker': '*'},\n    'Spikformer': {'accuracy': [82, 74], 'time_steps': [10**1, 10**1], 'size': [10, 10], 'marker': 'o'},\n    'QCFS': {'accuracy': [82.5, 74.5], 'time_steps': [10**1.5, 10**1.5], 'size': [20, 20], 'marker': 'o'},\n    'DSR': {'accuracy': [80, 70], 'time_steps': [10**1, 10**1], 'size': [10, 10], 'marker': 'o'},\n    'TET': {'accuracy': [78, 68], 'time_steps': [10**0.5, 10**0.5], 'size': [10, 10], 'marker': 'o'},\n    'RNL': {'accuracy': [79, 69], 'time_steps': [10**0.5, 10**0.5], 'size': [20, 20], 'marker': 'o'},\n    'tdBN': {'accuracy': [76, 66], 'time_steps': [10**1, 10**1], 'size': [20, 20], 'marker': 'o'},\n    'Diet-SNN': {'accuracy': [78, 70], 'time_steps': [10**2, 10**2], 'size': [20, 20], 'marker': 'o'},\n    'RMP': {'accuracy': [76.5, 69], 'time_steps': [10**3.5, 10**3.5], 'size': [20, 20], 'marker': 'o'}\n}\n\n# Plot\nfig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\nfor i, dataset in enumerate(datasets):\n    for model, data in models.items():\n        axs[i, 0].scatter(data['time_steps'][0], data['accuracy'][0], s=data['size'][0]*20, label=model if i == 0 else \"\", marker=data['marker'])\n        axs[i, 1].scatter(data['time_steps'][1], data['accuracy'][1], s=data['size'][1]*20, label=model if i == 0 else \"\", marker=data['marker'])\n    \n    axs[i, 0].set_title(dataset)\n    axs[i, 1].set_title(dataset)\n    axs[i, 0].set_xlabel('Time Steps')\n    axs[i, 1].set_xlabel('Time Steps')\n    axs[i, 0].set_ylabel('Accuracy (%)')\n    axs[i, 1].set_ylabel('Accuracy (%)')\n    axs[i, 0].set_xscale('log')\n    axs[i, 1].set_xscale('log')\n    axs[i, 0].set_xticks(time_steps)\n    axs[i, 1].set_xticks(time_steps)\n    axs[i, 0].set_xticklabels(time_steps)\n    axs[i, 1].set_xticklabels(time_steps)\n    axs[i, 0].grid(True, which=\"both\", ls=\"--\", c='0.7')\n    axs[i, 1].grid(True, which=\"both\", ls=\"--\", c='0.7')\n\naxs[0, 0].legend(loc=\"upper left\")\n\n# Save the figure\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\nx = np.arange(0, 13)  # Number of masked blocks from 0 to 12\ny = [86.5, 86.4, 86.3, 86.2, 86.1, 85.9, 85.8, 85.7, 85.5, 85.3, 85.2, 85.1, 84.8]  # Accuracy values\nerror = [0.1] * 13  # Error bars (assumed to be constant for all the bars)\n\n# Create the figure and axis\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot the data with error bars\nax.bar(x, y, yerr=error, color='blue', capsize=5)\n\n# Set the axis labels and title\nax.set_xlabel(\"# Masked Blocks\", fontsize=14)\nax.set_ylabel(\"Accuracy (%)\", fontsize=14)\nax.set_title(\"Comparison of the accuracy of masking different numbers of blocks on the CIFAR-100 dataset\", fontsize=14)\n\n# Set the y-axis limits\nax.set_ylim(84.5, 86.7)\n\n# Save the figure as a PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"accuracy_comparison.png\", dpi=300)\n\n# Show the plot\nplt.show()\n",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2210.01208/blockcompare_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2205.14900/Of_converge.png",
        "caption": "OfficeHome",
        "source": "iccv_1/2205.14900/main.tex",
        "arxiv_id": "iccv_1/2205.14900",
        "llava_output": "0.01 0.14 0.99 0.26 0.01 0.14 0.99 0.26 0.01 0.14 0.99 0.26 0.01",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate dummy data to replicate the shape of the graph\nx = np.linspace(0, 200, 200)\ny1 = 0.6 + 0.05 * np.sin(0.1 * x) + 0.03 * np.cos(0.2 * x) + 0.02 * np.random.rand(200)  # Ours\ny2 = 0.6 + 0.04 * np.sin(0.1 * x) + 0.02 * np.random.rand(200)  # FedAvg\ny3 = 0.6 + 0.03 * np.sin(0.1 * x) + 0.02 * np.cos(0.15 * x) + 0.01 * np.random.rand(200)  # FedBN\n\n# Plotting the data\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, label='Ours', color='blue')\nplt.plot(x, y2, label='FedAvg', color='orange')\nplt.plot(x, y3, label='FedBN', color='green')\nplt.xlabel(\"Communication round (r)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"OfficeHome\")\nplt.legend()\n\n# Save plot as png file with 300dpi\nplt.savefig(\"plot_officehome.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Setting up the Environment\n# (Already done with the import statements)\n\n# 2. Data Preparation\nx = np.linspace(0, 200, 200)  # Assuming 200 communication rounds\n# Create dummy data that mimics the trend in the figure\ny_ours = 0.6 + 0.05 * np.sin(0.1 * x) + 0.02 * np.random.rand(200)\ny_fedavg = 0.6 + 0.03 * np.sin(0.12 * x) + 0.01 * np.random.rand(200)\ny_fedbn = 0.6 + 0.04 * np.sin(0.11 * x) + 0.015 * np.random.rand(200)\n\n# 3. Creating the Plot\nplt.figure(figsize=(8, 6))\nplt.plot(x, y_ours, label='Ours', color='blue')\nplt.plot(x, y_fedavg, label='FedAvg', color='orange')\nplt.plot(x, y_fedbn, label='FedBN', color='green')\nplt.xlabel(\"Communication round (r)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"OfficeHome\")\nplt.legend()\n\n# 4. Save the Plot\nplt.savefig(\"officehome_plot.png\", dpi=300)\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2205.14900/Of_converge_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2211.07157/corresponding_kernel.png",
        "caption": "The corresponding oversized convolution kernel of the last uniform block of the third stage. We randomly selected 32 channels as examples.",
        "source": "iccv_1/2211.07157/arxiv.tex",
        "arxiv_id": "iccv_1/2211.07157",
        "llava_output": "1. Bar Chart: The image is a bar chart, showing the corresponding oversized convolution kernel of the last uniform block of the third stage.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Setting up the Environment\n# (Already done with the import statements)\n\n# 2. Generate Dummy Kernels\n# For simplicity, let's create 32 random 7x7 \"kernels\" as placeholders\nkernels = [np.random.rand(7, 7) for _ in range(32)]\n\n# 3. Plotting\nfig, axs = plt.subplots(4, 8, figsize=(15, 8))\nfor i, ax in enumerate(axs.ravel()):\n    ax.imshow(kernels[i], cmap='viridis')\n    ax.axis('off')\n\nplt.tight_layout()\n\n# 4. Save the Plot\nplt.savefig(\"conv_kernels.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Setting up the Environment\n# (Already done with the import statements)\n\n# 2. Data Preparation\n# Generate 32 random 7x7 \"kernels\"\nkernels = [np.random.rand(7, 7) for _ in range(32)]\n\n# 3. Creating the Heatmaps & 4. Design the Plot\nfig, axs = plt.subplots(4, 8, figsize=(15, 8))\nfor i, ax in enumerate(axs.ravel()):\n    ax.imshow(kernels[i], cmap='viridis')  # Using viridis colormap for heatmaps\n    ax.axis('off')\n\nplt.tight_layout()\n\n# 5. Save the Plot\nplt.savefig(\"conv_kernels.png\", dpi=300)\nplt.show()\n",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2211.07157/corresponding_kernel_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2211.14646/all_metrics_16x16_patches.png",
        "caption": "Metrics plotted as a function of fraction of $16 \\times 16$ patches masked out in a random order using a given masking method and model. ResNet50 (Aug) refers to ResNet50 pretrained with grey missingness augmentations",
        "source": "iccv_1/2211.14646/main.tex",
        "arxiv_id": "iccv_1/2211.14646",
        "llava_output": "1. The image is a set of four line charts, each showing a different metric plotted as a function of fraction of $16 \\times 16$ patches masked out in a random order using a given masking method and model.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Setting up the Environment\n# (Already done with the import statements)\n\n# 2. Data Preparation\nx = np.linspace(0, 1, 100)\n# Using sigmoid functions to simulate trends in the data\ndef sigmoid(a, b, x):\n    return 1 / (1 + np.exp(-a * (x - b)))\n\ny_data = {\n    \"ResNet50 (Blackout)\": sigmoid(10, 0.6, x),\n    \"ResNet50 (Layer masking)\": sigmoid(10, 0.5, x),\n    \"ResNet50 (Aug) (Blackout)\": 0.8*sigmoid(-10, 0.4, x) + 0.2,\n    \"ResNet50 (Greyout)\": sigmoid(9, 0.7, x),\n    \"DeiT-B-16 (Greyout)\": sigmoid(11, 0.45, x),\n    \"ResNet50 (Aug) (Greyout)\": 0.8*sigmoid(-10, 0.5, x) + 0.2\n}\n\n# 3. Plot Design\ncolors = [\"blue\", \"red\", \"green\", \"purple\", \"cyan\", \"lime\"]\nline_styles = [\"-\", \"-\", \"--\", \"-.\", \":\", \":\"]\n\n# 4. Creating the Plots & 5. Design the Subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\ntitles = [\"Accuracy\", \"Wordnet similarity\", \"Class Entropy\", \"Unchanged predictions\"]\nfor ax, title in zip(axs.ravel(), titles):\n    for color, style, (label, y) in zip(colors, line_styles, y_data.items()):\n        ax.plot(x, y, color=color, linestyle=style, label=label)\n    ax.set_title(title)\n    ax.set_xlabel(\"Fraction of image masked out\")\n    ax.set_xlim(0, 1)\n    ax.legend()\n    \nplt.tight_layout()\n\n# 6. Save the Plot\nplt.savefig(\"metrics_vs_fraction.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Setting up the Environment\nplt.figure(figsize=(10, 8))\n\n# 2. Data Preparation\nx = np.linspace(0, 1, 100)\ndef sigmoid(a, b, x):\n    return 1 / (1 + np.exp(-a * (x - b)))\n\ny_data = {\n    \"ResNet50 (Blackout)\": sigmoid(10, 0.6, x),\n    \"ResNet50 (Layer masking)\": sigmoid(10, 0.5, x),\n    \"ResNet50 (Aug) (Blackout)\": 0.8*sigmoid(-10, 0.4, x) + 0.2,\n    \"ResNet50 (Greyout)\": sigmoid(9, 0.7, x),\n    \"DeiT-B-16 (Greyout)\": sigmoid(11, 0.45, x),\n    \"ResNet50 (Aug) (Greyout)\": 0.8*sigmoid(-10, 0.5, x) + 0.2\n}\n\n# 3. Plot Design\ncolors = [\"blue\", \"red\", \"green\", \"purple\", \"cyan\", \"lime\"]\nline_styles = [\"-\", \"-\", \"--\", \"-.\", \":\", \":\"]\ntitles = [\"Accuracy\", \"Wordnet similarity\", \"Class Entropy\", \"Unchanged predictions\"]\n\n# 4. Plotting & 5. Design the Subplots\nfor i, title in enumerate(titles):\n    plt.subplot(2, 2, i+1)\n    for color, style, (label, y) in zip(colors, line_styles, y_data.items()):\n        plt.plot(x, y, color=color, linestyle=style, label=label)\n        plt.title(title)\n        plt.xlabel(\"Fraction of image masked out\")\n        plt.xlim(0, 1)\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2211.14646/all_metrics_16x16_patches_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2211.14646/accuracy_16x16_patches_partial_conv.png",
        "caption": "Accuracy and class entropy vs fraction of $16 \\times 16$ patches of the image masked out in random order using various masking methods on ResNet-50",
        "source": "iccv_1/2211.14646/main.tex",
        "arxiv_id": "iccv_1/2211.14646",
        "llava_output": "16x16 Patches: ResNet-50</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nx = np.linspace(0, 1, 100)\n\n# Simulating the data based on the given plot. Adjust as needed.\nblackout = -0.8*x + 0.8\ngreyout = -0.75*x + 0.78\nlayer_masking = -1.3*x + 0.8\nlayer_masking_zero_padding = -1.5*x + 0.8\npartial_convolution = -0.5*x**2\n\n# Plotting\nplt.figure(figsize=(10,6))\nplt.plot(x, blackout, label='resnet50 (Blackout)', color='blue')\nplt.plot(x, greyout, label='resnet50 (Greyout)', color='orange')\nplt.plot(x, layer_masking, label='resnet50 (Layer masking)', color='green')\nplt.plot(x, layer_masking_zero_padding, label='resnet50 (Layer masking (zero padding))', color='purple')\nplt.plot(x, partial_convolution, label='resnet50 (Partial convolution)', color='red')\n\n# Setting title and labels\nplt.title('16x16 patches')\nplt.xlabel('Fraction of Masked Image')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper right')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Save the plot as a .png file with 300dpi\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 2: Define the x-values\nx = np.linspace(0, 1, 100)\n\n# Step 3: Simulate y-values for each masking method\nblackout = -0.8*x + 0.8\ngreyout = -0.75*x + 0.78\nlayer_masking = -1.3*x + 0.8\nlayer_masking_zero_padding = -1.5*x + 0.8\npartial_convolution = -0.5*x**2\n\n# Step 4: Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, blackout, label='resnet50 (Blackout)', color='blue')\nplt.plot(x, greyout, label='resnet50 (Greyout)', color='orange')\nplt.plot(x, layer_masking, label='resnet50 (Layer masking)', color='green')\nplt.plot(x, layer_masking_zero_padding, label='resnet50 (Layer masking (zero padding))', color='purple')\nplt.plot(x, partial_convolution, label='resnet50 (Partial convolution)', color='red')\n\n# Step 5: Configure plot settings\nplt.title('16x16 patches')\nplt.xlabel('Fraction of Masked Image')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper right')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Step 6: Save as PNG\nplt.savefig('plot.png', dpi=300)\n\n# Step 7: Display the plot\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2211.14646/accuracy_16x16_patches_partial_conv_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2211.14646/shape_bias_EfficientNet_barplot_with_gap.png",
        "caption": "EfficientNet",
        "source": "iccv_1/2211.14646/main.tex",
        "arxiv_id": "iccv_1/2211.14646",
        "llava_output": "The image is a line chart that presents the results of an experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 2: Define categories and number of groups\ncategories = [\"Average\", \"hammerhead shark\", \"parmigan\", \"American coot\", \"albatross\", \n              \"sea cucumber\", \"megalith\", \"sailboat\", \"freight car\", \"front curtain\", \n              \"carbonara\", \"pizza\"]\nn_groups = 6\n\n# Step 3: Define values and errors (these are just placeholder values)\nvalues = [\n    [0.9, 0.85, 0.8, 0.78, 0.82, 0.8],  # For \"Average\"\n    # ... (similarly for other categories)\n]\nerrors = [\n    [0.02, 0.02, 0.03, 0.02, 0.03, 0.02],  # For \"Average\"\n    # ... (similarly for other categories)\n]\n# For simplicity, let's use the same values and errors for all categories (adjust as needed)\nvalues = values * len(categories)\nerrors = errors * len(categories)\n\n# Step 4: Define positions for each group\nbar_width = 0.15\nindex = np.arange(len(categories))\npositions = [index + i*bar_width for i in range(n_groups)]\n\n# Step 5: Plot the bars\ncolors = [\"#FF0000\", \"#00FF00\", \"#0000FF\", \"#FF69B4\", \"#4B0082\", \"#DAA520\"]\nlabels = [\"Blackout (object mask)\", \"Blackout (broken mask)\", \"Greyout (object mask)\", \n          \"Greyout (broken mask)\", \"Layer masking (object mask)\", \"Layer masking (broken mask)\"]\n\nplt.figure(figsize=(15, 5))\nfor i, (val, err, color, label) in enumerate(zip(values[0], errors[0], colors, labels)):\n    plt.bar(positions[i], val, yerr=err, width=bar_width, color=color, label=label, \n            error_kw={'capsize': 5, 'elinewidth': 1, 'alpha': 0.7})\n\n# Step 6: Configure plot settings\nplt.title('EfficientNet')\nplt.ylabel('Accuracy')\nplt.xticks(index + bar_width * 2.5, categories)\nplt.legend(loc='upper right')\nplt.tight_layout()\n\n# Step 7: Save as PNG\nplt.savefig('efficientnet_plot.png', dpi=300)\n\n# Step 8: Display the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Already done by importing libraries.\n\n# Step 2: Define categories\ncategories = [\"Average\", \"hammerhead shark\", \"parmigan\", \"American coot\", \n              \"albatross\", \"sea cucumber\", \"megalith\", \"sailboat\", \n              \"freight car\", \"front curtain\", \"carbonara\", \"pizza\"]\n\n# Step 3: Define mean values and errors (these are approximations based on the visual)\nvalues = np.array([\n    [0.95, 0.9, 0.85, 0.92, 0.9, 0.85],  # for \"Average\"\n    # ... Repeat similar arrays for each category\n])\n\n# Errors are assumed to be constant across all categories for simplicity\nerrors = np.array([\n    [0.02, 0.015, 0.025, 0.02, 0.015, 0.025]  # errors for each group\n] * len(categories))\n\n# Step 4: Calculate width and positions\nbar_width = 0.15\npositions = [np.arange(len(categories)) + bar_width * i for i in range(6)]\n\n# Step 5: Plot bars\ncolors = ['#FF0000', '#FF69B4', '#0000FF', '#4B0082', '#00FF00', '#DAA520']\nlabels = [\"Blackout (object mask)\", \"Blackout (broken mask)\", \"Greyout (object mask)\", \n          \"Greyout (broken mask)\", \"Layer masking (object mask)\", \"Layer masking (broken mask)\"]\n\nplt.figure(figsize=(15, 7))\nfor i in range(6):\n    plt.bar(positions[i], values[:, i], yerr=errors[:, i], color=colors[i], \n            width=bar_width, label=labels[i], capsize=5, alpha=0.8)\n\n# Step 6: Configure plot settings\nplt.xlabel('Categories')\nplt.ylabel('Accuracy')\nplt.title('EfficientNet')\nplt.xticks(positions[2], categories, rotation=15)\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.ylim(0, 1)\nplt.grid(axis='y')\n\n# Step 7: Save as PNG\nplt.savefig('efficientnet_plot.png', dpi=300)\n\n# Step 8: Display the plot\nplt.show()\n",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2211.14646/shape_bias_EfficientNet_barplot_with_gap_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2211.14646/cos_sim.png",
        "caption": "Average difference in cosine similarity vs image size. Since model features of ViTs can be negative unlike ResNet-50, cosine similarity can vary from -1 to 1",
        "source": "iccv_1/2211.14646/main.tex",
        "arxiv_id": "iccv_1/2211.14646",
        "llava_output": "The image is a scatter plot that shows the average difference in cosine similarity vs image size. The plot includes different models such as ResNet-50, ViTs, and Greyout ResNet-50. The x-axis represents the image size, and the y-axis represents the cosine similarity",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Already done by importing libraries.\n\n# Step 2: Define x-axis values\nimage_size = np.array([50, 100, 150, 200])\n\n# Step 3: Define y-axis values\n# Note: The following values are approximate and based on visual interpretation\nblackout_resnet = np.array([0.75, 0.62, 0.3, 0.08])\ngreyout_resnet = np.array([0.72, 0.6, 0.28, 0.05])\nlayer_masking_resnet = np.array([0.68, 0.56, 0.25, 0.02])\nblackout_vit = np.array([0.6, 0.48, 0.5, 0.65])\ngreyout_vit = np.array([0.57, 0.45, 0.47, 0.62])\ntoken_dropping_vit = np.array([0.2, 0.3, 0.5, 0.72])\n\n# Step 4: Plot values\nplt.figure(figsize=(8, 5))\nplt.plot(image_size, blackout_resnet, label='Blackout (ResNet-50)', color='blue')\nplt.plot(image_size, greyout_resnet, label='Greyout (ResNet-50)', color='orange')\nplt.plot(image_size, layer_masking_resnet, label='Layer Masking (ResNet-50)', color='green')\nplt.plot(image_size, blackout_vit, label='Blackout (ViT-B/16)', color='red')\nplt.plot(image_size, greyout_vit, label='Greyout (ViT-B/16)', color='purple')\nplt.plot(image_size, token_dropping_vit, label='Token Dropping (ViT-B/16)', color='brown')\n\n# Step 5: Configure plot settings\nplt.xlabel('Image size')\nplt.ylabel('Average \u0394 Cosine Similarity')\nplt.title('Average difference in cosine similarity vs image size')\nplt.legend()\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.tight_layout()\n\n# Step 6: Save as PNG\nplt.savefig('cosine_similarity_plot.png', dpi=300)\n\n# Step 7: Display the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# 1. Libraries have been imported.\n\n# 2. Define data for plotting.\nimage_sizes = [50, 100, 150, 200]\n\n# Approximated values based on the visual representation:\nblackout_resnet50 = [0.8, 0.6, 0.4, 0.2]\ngreyout_resnet50 = [0.75, 0.55, 0.35, 0.15]\nlayer_masking_resnet50 = [0.7, 0.5, 0.3, 0.1]\nblackout_vit = [0.6, 0.4, 0.5, 0.7]\ngreyout_vit = [0.55, 0.35, 0.45, 0.65]\ntoken_dropping_vit = [0.2, 0.3, 0.5, 0.8]\n\n# 3. Plot the data.\nplt.figure(figsize=(10, 6))\nplt.plot(image_sizes, blackout_resnet50, color='blue', label='Blackout (ResNet-50)')\nplt.plot(image_sizes, greyout_resnet50, color='orange', label='Greyout (ResNet-50)')\nplt.plot(image_sizes, layer_masking_resnet50, color='green', label='Layer Masking (ResNet-50)')\nplt.plot(image_sizes, blackout_vit, color='red', label='Blackout (ViT-B/16)')\nplt.plot(image_sizes, greyout_vit, color='purple', label='Greyout (ViT-B/16)')\nplt.plot(image_sizes, token_dropping_vit, color='brown', label='Token Dropping (ViT-B/16)')\n\n# 4. Configure plot aesthetics.\nplt.xlabel(\"Image size\")\nplt.ylabel(\"Average \u0394 Cosine Similarity\")\nplt.title(\"Average difference in cosine similarity vs image size\")\nplt.grid(True, linestyle='--')\nplt.legend()\n\n# 5. Save the plot.\nplt.savefig(\"cosine_similarity_vs_image_size.png\", dpi=300)\n\n# 6. Display the plot.\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2211.14646/cos_sim_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2211.14646/mag_plot.png",
        "caption": "Mean magnitude of output feature vectors vs image size",
        "source": "iccv_1/2211.14646/main.tex",
        "arxiv_id": "iccv_1/2211.14646",
        "llava_output": "The image is a scatter plot that shows the mean magnitude of output feature vectors versus image size. The plot has a horizontal axis representing image size and a vertical axis representing the mean magnitude of output feature vectors. The data points are scattered across the plot, indicating the relationship between the two variables.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data for plotting\nimage_sizes = [50, 100, 150, 200]\nblackout_resnet = [0, 10, 15, 28]\ngreyout_resnet = [2, 8, 20, 28]\ntoken_dropping_resnet = [5, 12, 18, 25]\neif_n2_resnet = [2, 15, 23, 28]\nblackout_vitb16 = [7, 15, 23, 30]\ngreyout_vitb16 = [0, 8, 17, 26]\ntoken_dropping_vitb16 = [5, 13, 20, 28]\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.plot(image_sizes, blackout_resnet, label='Blackout (ResNet-50)', color='blue')\nplt.plot(image_sizes, greyout_resnet, label='Greyout  (ResNet-50)', color='orange')\nplt.plot(image_sizes, token_dropping_resnet, label='Token Dropping (ResNet-50)', color='green')\nplt.plot(image_sizes, eif_n2_resnet, label='E[f] \u03b1 n\u00b2 (ResNet-50)', color='red')\nplt.plot(image_sizes, blackout_vitb16, label='Blackout (ViT-B/16)', color='purple')\nplt.plot(image_sizes, greyout_vitb16, label='Greyout (ViT-B/16)', color='brown')\nplt.plot(image_sizes, token_dropping_vitb16, label='Token Dropping (ViT-B/16)', color='pink')\n\n# Axis labels and title\nplt.xlabel('Image size (n)')\nplt.ylabel('Avg magnitude (E[f])')\nplt.legend(loc='upper left')\n\n# Save the plot\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2211.14646/mag_plot_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2212.14306/aucrocvst.png",
        "caption": "$T_0$ vs AUC-ROC on CUB. Incorporating more reverse diffusion steps into the attention computation improves the AUC-ROC against the groundtruth only up to roughly $T_0 = 40$.",
        "source": "iccv_1/2212.14306/main.tex",
        "arxiv_id": "iccv_1/2212.14306",
        "llava_output": "The image is a scatter plot that shows the relationship between $T\\_0$ and AUC-ROC on CUB. The plot demonstrates how incorporating more reverse diffusion steps into the attention computation improves the AUC-ROC against the groundtruth only up to roughly $T\\_0 =",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\nt = np.array([i for i in range(51)])\nauc_roc = np.array([0.950 + 0.01 * np.sin(i/15) for i in t])\n\n# Plotting\nplt.figure(figsize=(7, 4))\nplt.plot(t, auc_roc, 'o-', color='blue', markersize=4)\nplt.xlabel('t')\nplt.ylabel('AUC-ROC')\nplt.ylim(0.950, 0.960)\nplt.xticks(np.arange(0, 51, 10))\nplt.yticks(np.arange(0.950, 0.961, 0.005))\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('plot.png', dpi=300)\nplt.show()\n",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2212.14306/aucrocvst_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2301.01218/OD_Boundary_20220514.png",
        "caption": "The distributions of output differences with different black-box attacks.",
        "source": "iccv_1/2301.01218/main.tex",
        "arxiv_id": "iccv_1/2301.01218",
        "llava_output": "The image is a scatter plot that shows the distribution of output differences with different black-box attacks.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\noutput_differences = [0.7, 1, 1.3, 1.6, 1.9]\nalpha_005 = [20, 50, 120, 230, 580]\nalpha_01 = [10, 40, 150, 300, 550]\nalpha_015 = [5, 30, 90, 270, 520]\n\nbar_width = 0.2\n\n# Plotting\nplt.figure(figsize=(7, 5))\nr1 = np.arange(len(output_differences))\nr2 = [x + bar_width for x in r1]\nr3 = [x + bar_width for x in r2]\n\nplt.bar(r1, alpha_005, width=bar_width, color='blue', label='\u03b1 = 0.05')\nplt.bar(r2, alpha_01, width=bar_width, color='orange', label='\u03b1 = 0.1')\nplt.bar(r3, alpha_015, width=bar_width, color='brown', label='\u03b1 = 0.15')\n\nplt.xlabel('Output differences')\nplt.ylabel('Numbers of sample')\nplt.xticks([r + bar_width for r in range(len(output_differences))], output_differences)\nplt.legend()\nplt.grid(True, which='both', linestyle='--', linewidth=0.5, axis='y')\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('bar_plot.png', dpi=300)\nplt.show()\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2301.01218/OD_Boundary_20220514_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2301.10460/iterations.png",
        "caption": "A plot of the number of human-verified shapes (max = 400) at different nodes (only those which existed in more than 100 shapes) of the chair category. The plot shows that with hierarchical active learning, many more shapes pass the verification, hence saving on the costly label modification, especially at higher hierarchy levels (i.e., earlier in the labeling iterations).",
        "source": "iccv_1/2301.10460/5_exp.tex",
        "arxiv_id": "iccv_1/2301.10460",
        "llava_output": "The image is a line chart that presents the number of human-verified shapes at different nodes of the chair category. The chart shows that with hierarchical active learning, many more shapes pass the verification, especially at higher hierarchy levels.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data\nx = [0, 1, 2, 3, 4]\nnon_hierarchy = [0, 250, 60, 50, 50]\nchair = [0, 250, 220, 210, 210]\nchair_arm = [0, 50, 150, 140, 140]\nchair_back = [0, 50, 150, 130, 130]\nchair_base = [0, 100, 120, 110, 110]\nchair_seat = [0, 50, 100, 90, 90]\nchair_regular_base = [0, 100, 110, 100, 100]\n\n# Plotting\nplt.plot(x, non_hierarchy, 'r--o', label=\"Non-hierarchy\")\nplt.plot(x, chair, 'r-', label=\"Chair\")\nplt.plot(x, chair_arm, 'g-', label=\"Chair_arm\")\nplt.plot(x, chair_back, 'b-', label=\"Chair_back\")\nplt.plot(x, chair_base, 'y-', label=\"Chair_base\")\nplt.plot(x, chair_seat, 'm-', label=\"Chair_seat\")\nplt.plot(x, chair_regular_base, 'c-', label=\"Chair_regular_base\")\n\n# Axes Labels and Title\nplt.xlabel(\"Active Learning Iterations\")\nplt.ylabel(\"#Verified Shapes\")\nplt.legend(loc=\"upper right\")\n\n# Save the plot\nplt.savefig(\"plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\niterations = np.array([0, 1, 2, 3, 4])\nnon_hierarchy = np.array([300, 50, 25, 20, 15])\nchair = np.array([250, 200, 175, 150, 125])\nchair_arm = np.array([200, 150, 100, 80, 65])\nchair_back = np.array([175, 125, 90, 70, 60])\nchair_base = np.array([150, 125, 100, 80, 70])\nchair_seat = np.array([100, 85, 75, 70, 65])\nchair_regular_base = np.array([100, 80, 70, 65, 60])\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, non_hierarchy, 'r--', label='Non-hierarchy', marker='o')\nplt.plot(iterations, chair, 'r-', label='Chair', marker='o')\nplt.plot(iterations, chair_arm, 'g-', label='Chair_arm', marker='o')\nplt.plot(iterations, chair_back, 'b-', label='Chair_back', marker='o')\nplt.plot(iterations, chair_base, 'y-', label='Chair_base', marker='o')\nplt.plot(iterations, chair_seat, 'c-', label='Chair_seat', marker='o')\nplt.plot(iterations, chair_regular_base, 'm-', label='Chair_regular_base', marker='o')\n\n# Axis labels and title\nplt.xlabel('Active Learning Iterations')\nplt.ylabel('# Verified Shapes')\nplt.legend(loc='upper right')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Save figure\nplt.savefig(\"plot.png\", dpi=300)\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2301.10460/iterations_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2302.12986/dataset-scale-invariant.png",
        "caption": "The scale variation of the same person on PRW and CUHK-SYSU datasets.",
        "source": "iccv_1/2302.12986/PaperForReview.tex",
        "arxiv_id": "iccv_1/2302.12986",
        "llava_output": "1. Bar Chart: The image shows a bar chart that compares the scale variation of the same person on PRW and CUHK-SYSU datasets. The chart includes two bars, one for each dataset, and a percentage value for each bar.\n\n2. Pie Chart: The image does not",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data for the pie charts\nlabels = ['1.0-1.5', '1.5-3.0', '3.0-5.0', '5.0-10.0', '>10.0']\nsizes_prw = [9.43, 28.74, 31.63, 17.01, 13.19]\nsizes_cuhk_sysu = [2.69, 2.98, 39.81, 12.68, 41.85]\n\ncolors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0']\n\n# PRW pie chart\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].pie(sizes_prw, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)\naxs[0].set_title('PRW')\n\n# CUHK-SYSU pie chart\naxs[1].pie(sizes_cuhk_sysu, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)\naxs[1].set_title('CUHK-SYSU')\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig(\"plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Step 1: Define the data\nlabels = ['1.0-1.5', '1.5-3.0', '3.0-5.0', '5.0-10.0', '>10.0']\nsizes_prw = [9.43, 28.74, 31.63, 17.01, 13.19]\nsizes_cuhk_sysu = [2.69, 2.98, 39.81, 12.68, 41.85]\ncolors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#c2c2f0']\n\n# Step 2: Initialize the figure and axes\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Step 3: Plot PRW pie chart\naxs[0].pie(sizes_prw, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)\naxs[0].set_title('PRW')\n\n# Step 4: Plot CUHK-SYSU pie chart\naxs[1].pie(sizes_cuhk_sysu, labels=labels, colors=colors, autopct='%1.2f%%', startangle=140)\naxs[1].set_title('CUHK-SYSU')\n\n# Step 5: Save the figure as PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"plot.png\", dpi=300)\nplt.close(fig)\n",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2302.12986/dataset-scale-invariant_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 4,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.08308/searchspace_comparison2.png",
        "caption": "Best searched INT8 models with comparison to state-of-the-art NAS search spaces. Our searched spaces are proven to be the most quantization-friendly for the target device.",
        "source": "iccv_1/2303.08308/eval.tex",
        "arxiv_id": "iccv_1/2303.08308",
        "llava_output": "3 Line Charts are presented in the image, each showing the performance of different models.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Step 1: Define the data\nx = [5, 10, 15, 20, 25, 30, 35]\n\ny_SpaceEvo_CPU = [74.7, 76, 77.8, 78.7, 79.4, 79.95, 80.1]\ny_SpaceEvo_Pixel4 = [74.4, 76, 76.9, 78, 78.5, 79.5, 79.9]\ny_AttentiveNAS_space = [75, 76.6, 77.6, 78.3, 79, 79.9, 80]\ny_Proxylet_space = [74.9, 76, 76.8, 77.6, 78.3, 79.5, 79.95]\ny_MobileNetV3_space = [74, 76, 76.9, 77.8, 78.6, 79.5, 79.9]\ny_ResNet_space = [74.2, 75.9, 76.8, 77.5, 78.2, 79.1, 79.75]\n\n# Step 2: Initialize the figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Step 3: Plot data for Intel CPU\nax1.plot(x, y_SpaceEvo_CPU, 'o-', label='SpaceEvo@CPU')\nax1.plot(x, y_AttentiveNAS_space, 's-', label='AttentiveNAS space')\nax1.plot(x, y_Proxylet_space, 'D-', label='Proxylet space')\nax1.plot(x, y_MobileNetV3_space, '*-', label='MobileNetV3 space')\nax1.plot(x, y_ResNet_space, 'x-', label='ResNet space')\nax1.set_xlabel('INT8 latency on the Intel CPU (ms)')\nax1.set_ylabel('Top-1 INT8 Accuracy (%)')\nax1.legend()\n\n# Step 4: Plot data for Pixel 4\nax2.plot(x, y_SpaceEvo_Pixel4, 'o-', label='SpaceEvo@Pixel4')\nax2.plot(x, y_AttentiveNAS_space, 's-', label='AttentiveNAS space')\nax2.plot(x, y_Proxylet_space, 'D-', label='Proxylet space')\nax2.plot(x, y_MobileNetV3_space, '*-', label='MobileNetV3 space')\nax2.plot(x, y_ResNet_space, 'x-', label='ResNet space')\nax2.set_xlabel('INT8 latency on the Pixel 4 (ms)')\nax2.legend()\n\n# Step 5: Save the figure as PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"comparison_plot.png\", dpi=300)\nplt.close(fig)\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Step 1: Define the data\nx = [5, 10, 15, 20, 25, 30, 35]\n\ny_SpaceEvo_CPU = [74.7, 76, 77.8, 78.7, 79.4, 79.95, 80.1]\ny_SpaceEvo_Pixel4 = [74.4, 76, 76.9, 78, 78.5, 79.5, 79.9]\ny_AttentiveNAS_space = [75, 76.6, 77.6, 78.3, 79, 79.9, 80]\ny_Proxylet_space = [74.9, 76, 76.8, 77.6, 78.3, 79.5, 79.95]\ny_MobileNetV3_space = [74, 76, 76.9, 77.8, 78.6, 79.5, 79.9]\ny_ResNet_space = [74.2, 75.9, 76.8, 77.5, 78.2, 79.1, 79.75]\n\n# Step 2: Initialize the figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Step 3: Plot data for Intel CPU\nax1.plot(x, y_SpaceEvo_CPU, 'o-', label='SpaceEvo@CPU')\nax1.plot(x, y_AttentiveNAS_space, 's-', label='AttentiveNAS space')\nax1.plot(x, y_Proxylet_space, 'D-', label='Proxylet space')\nax1.plot(x, y_MobileNetV3_space, '*-', label='MobileNetV3 space')\nax1.plot(x, y_ResNet_space, 'x-', label='ResNet space')\nax1.set_xlabel('INT8 latency on the Intel CPU (ms)')\nax1.set_ylabel('Top-1 INT8 Accuracy (%)')\nax1.legend()\n\n# Step 4: Plot data for Pixel 4\nax2.plot(x, y_SpaceEvo_Pixel4, 'o-', label='SpaceEvo@Pixel4')\nax2.plot(x, y_AttentiveNAS_space, 's-', label='AttentiveNAS space')\nax2.plot(x, y_Proxylet_space, 'D-', label='Proxylet space')\nax2.plot(x, y_MobileNetV3_space, '*-', label='MobileNetV3 space')\nax2.plot(x, y_ResNet_space, 'x-', label='ResNet space')\nax2.set_xlabel('INT8 latency on the Pixel 4 (ms)')\nax2.legend()\n\n# Step 5: Save the figure as PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"comparison_plot.png\", dpi=300)\nplt.close(fig)\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.08308/searchspace_comparison2_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 5,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.08308/searchcost.png",
        "caption": "Search cost measured on 8 Nvidia V100 GPUs.",
        "source": "iccv_1/2303.08308/eval.tex",
        "arxiv_id": "iccv_1/2303.08308",
        "llava_output": "The image is a line chart that shows the search cost in terms of GPU hours for different search algorithms.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a new figure and axis\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Draw bars\nax.barh('Train from scratch', 120000, color='#1f77b4')\nax.barh('SpaceEvo', 25, color='#1f77b4')\n\n# Draw arrow\narrowprops = dict(arrowstyle='->', lw=2)\nax.annotate('', xy=(120000, 0.5), xytext=(25, 0.5), arrowprops=arrowprops)\n\n# Annotate 48000x\nax.text(60000, 0.5, '48000x', va='center', ha='center', color='white', fontsize=10)\n\n# Set x-axis label\nax.set_xlabel('Total search cost (GPU hours)')\n\n# Remove y-axis ticks\nax.yaxis.tick_right()\n\n# Set the range of x-axis\nax.set_xlim(0, 130000)\n\n# Remove spines\nfor spine in ['top', 'bottom', 'left', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Bars\nax.barh('Train from scratch', 120000, color='#1f77b4')\nax.barh('SpaceEvo', 25, color='#1f77b4')\n\n# Arrow\narrow = patches.FancyArrowPatch((25, 0), (120000, 0), mutation_scale=15, color='black')\nax.add_patch(arrow)\n\n# Annotations\nax.text(60000, 0, '48000x', va='center', ha='center', color='white', fontsize=10)\n\n# X-axis label\nax.set_xlabel('Total search cost (GPU hours)')\n\n# Remove y-axis left ticks\nax.yaxis.tick_right()\n\n# Set x-axis limit\nax.set_xlim(0, 130000)\n\n# Hide spines\nfor spine in ['top', 'bottom', 'left', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# Save the plot\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)\nplt.close()\n",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.08308/searchcost_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.08308/spaceranking1.png",
        "caption": "Q-T score effectiveness (Kendall's $\\tau$) on ranking search space quality. We achieve a high space ranking correlation.",
        "source": "iccv_1/2303.08308/eval.tex",
        "arxiv_id": "iccv_1/2303.08308",
        "llava_output": "The image is a line chart showing the Kendall's $\\tau$ score effectiveness on ranking search space quality. The chart has a blue line and a red line, with the blue line representing the Kendall's $\\tau$ score and the red line representing the per-interconnect latency.",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(9, 5))\n\n# Data points\nx = [57, 58, 59, 60, 61, 62, 62.5]\ny = [1.6, 1.7, 1.8, 1.85, 1.9, 1.95, 2.0]\nax.scatter(x, y, color='blue', s=100)\n\n# Regression line\nax.plot([56.5, 63], [1.58, 2.02], color='blue')\n\n# Kendall's tau annotation\nax.text(61.5, 1.92, \"Kendall's \u03c4=0.78\", fontsize=10)\n\n# X and Y axis labels\nax.set_xlabel('Avg. accuracy of the Pareto-frontier models (%)')\nax.set_ylabel('Q-T score')\n\n# Inset table\ncolumns = ['8ms', '10ms', '15ms', '20ms', '25ms']\nrows = ['']\ncell_text = [['0.76', '0.80', '0.79', '0.81', '0.83']]\nax.table(cellText=cell_text, colLabels=columns, cellLoc = 'center', loc='center', bbox=[0.7, 0.15, 0.25, 0.2])\nax.text(0.84, 0.36, 'Table: Per-latency Kendalltau', transform=ax.transAxes, fontsize=10)\n\n# Axis limits\nax.set_xlim(56.5, 63)\nax.set_ylim(1.58, 2.02)\n\n# Hide spines\nfor spine in ['top', 'right']:\n    ax.spines[spine].set_visible(False)\n\n# Save the plot\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)\nplt.close()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(9, 5))\n\n# Plot scatter points and regression line\nx_values = list(range(57, 64))\ny_values = [val/100 + 1.55 for val in x_values]  # example regression values based on the image\nax.scatter(x_values, y_values, color='blue', s=100)\nax.plot([57, 63], [1.6, 2.0], color='blue')\n\n# Set Kendall's tau annotation\nax.text(61.5, 1.92, \"Kendall's \u03c4=0.78\", fontsize=10)\n\n# Set the x and y axis labels\nax.set_xlabel('Avg. accuracy of the Pareto-frontier models (%)')\nax.set_ylabel('Q-T score')\n\n# Create an inset table for Per-latency Kendalltau\ncolumns = ['8ms', '10ms', '15ms', '20ms', '25ms']\ntable_data = [['0.76', '0.80', '0.79', '0.81', '0.83']]\nax.table(cellText=table_data, colLabels=columns, cellLoc='center', loc='center', bbox=[0.7, 0.15, 0.25, 0.2])\nax.text(0.84, 0.36, 'Table: Per-latency Kendalltau', transform=ax.transAxes, fontsize=10)\n\n# Set axis limits\nax.set_xlim(56.5, 63)\nax.set_ylim(1.58, 2.02)\n\n# Hide unwanted spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('plot.png', dpi=300)\nplt.close()\n",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.08308/spaceranking1_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 5,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.13396/text_sim.png",
        "caption": "empirical experiment on text similarity using CLIP's text encoder and Sentence-Bert",
        "source": "iccv_1/2303.13396/old_eval.tex",
        "arxiv_id": "iccv_1/2303.13396",
        "llava_output": "The image is a scatter plot, which shows the similarity between text encodings using CLIP's text encoder and Sentence-Bert. The x-axis represents the similarity scores, while the y-axis represents the text encodings. The plot displays multiple data points, with each point representing",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data\nlabels = ['car', 'mercedes', 'toyota', 'van', 'dog', 'labrador', 'cat']\nclip_data = [\n    [1, 0.8, 0.84, 0.85, 0.85, 0.67, 0.85],\n    [0.8, 1, 0.81, 0.78, 0.77, 0.66, 0.76],\n    [0.84, 0.81, 1, 0.79, 0.78, 0.68, 0.77],\n    [0.85, 0.78, 0.79, 1, 0.8, 0.67, 0.79],\n    [0.85, 0.77, 0.78, 0.8, 1, 0.76, 0.91],\n    [0.67, 0.66, 0.68, 0.67, 0.76, 1, 0.69],\n    [0.85, 0.76, 0.77, 0.79, 0.91, 0.69, 1]\n]\n\nsentence_bert_data = [\n    [1, 0.61, 0.59, 0.47, 0.39, 0.2, 0.34],\n    [0.61, 1, 0.65, 0.44, 0.22, 0.23, 0.19],\n    [0.59, 0.65, 1, 0.45, 0.24, 0.23, 0.18],\n    [0.47, 0.44, 0.45, 1, 0.29, 0.21, 0.24],\n    [0.39, 0.22, 0.24, 0.29, 1, 0.64, 0.61],\n    [0.2, 0.23, 0.23, 0.21, 0.64, 1, 0.45],\n    [0.34, 0.19, 0.18, 0.24, 0.61, 0.45, 1]\n]\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\n# Plotting CLIP data\ncax1 = ax1.matshow(clip_data, cmap='hot', vmin=0.65, vmax=1)\nax1.set_xticks(np.arange(len(labels)))\nax1.set_yticks(np.arange(len(labels)))\nax1.set_xticklabels(labels)\nax1.set_yticklabels(labels)\nplt.setp(ax1.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nax1.set_title('CLIP', pad=20)\n\n# Plotting Sentence-Bert data\ncax2 = ax2.matshow(sentence_bert_data, cmap='hot', vmin=0.15, vmax=1)\nax2.set_xticks(np.arange(len(labels)))\nax2.set_yticks(np.arange(len(labels)))\nax2.set_xticklabels(labels)\nax2.set_yticklabels(labels)\nplt.setp(ax2.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nax2.set_title('Sentence-Bert', pad=20)\n\n# Adjust layout and add colorbars\nplt.tight_layout()\nfig.colorbar(cax1, ax=ax1, orientation='vertical', fraction=0.046, pad=0.04)\nfig.colorbar(cax2, ax=ax2, orientation='vertical', fraction=0.046, pad=0.04)\n\n# Save the figure\nplt.savefig('text_similarity_comparison.png', dpi=300)\nplt.close()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data for heatmaps\nclip_matrix = [\n    [1, 0.8, 0.84, 0.85, 0.85, 0.67, 0.85],\n    [0.8, 1, 0.81, 0.78, 0.77, 0.66, 0.76],\n    [0.84, 0.81, 1, 0.79, 0.78, 0.68, 0.77],\n    [0.85, 0.78, 0.79, 1, 0.8, 0.67, 0.79],\n    [0.85, 0.77, 0.78, 0.8, 1, 0.76, 0.91],\n    [0.67, 0.66, 0.68, 0.67, 0.76, 1, 0.69],\n    [0.85, 0.76, 0.77, 0.79, 0.91, 0.69, 1]\n]\n\nsentence_bert_matrix = [\n    [1, 0.61, 0.59, 0.47, 0.39, 0.2, 0.34],\n    [0.61, 1, 0.65, 0.44, 0.22, 0.23, 0.19],\n    [0.59, 0.65, 1, 0.45, 0.24, 0.23, 0.18],\n    [0.47, 0.44, 0.45, 1, 0.29, 0.21, 0.24],\n    [0.39, 0.22, 0.24, 0.29, 1, 0.64, 0.61],\n    [0.2, 0.23, 0.23, 0.21, 0.64, 1, 0.45],\n    [0.34, 0.19, 0.18, 0.24, 0.61, 0.45, 1]\n]\n\nlabels = ['car', 'mercedes', 'toyota', 'van', 'dog', 'labrador', 'cat']\n\nfig, axs = plt.subplots(2, 1, figsize=(7, 12))\n\n# CLIP heatmap\nc1 = axs[0].imshow(clip_matrix, cmap=\"hot\", vmin=0.65, vmax=1)\naxs[0].set_title(\"CLIP\")\naxs[0].set_xticks(np.arange(len(labels)))\naxs[0].set_yticks(np.arange(len(labels)))\naxs[0].set_xticklabels(labels)\naxs[0].set_yticklabels(labels)\nplt.setp(axs[0].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nfig.colorbar(c1, ax=axs[0])\n\n# Sentence-Bert heatmap\nc2 = axs[1].imshow(sentence_bert_matrix, cmap=\"hot\", vmin=0.15, vmax=1)\naxs[1].set_title(\"Sentence-Bert\")\naxs[1].set_xticks(np.arange(len(labels)))\naxs[1].set_yticks(np.arange(len(labels)))\naxs[1].set_xticklabels(labels)\naxs[1].set_yticklabels(labels)\nplt.setp(axs[1].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\nfig.colorbar(c2, ax=axs[1])\n\nplt.tight_layout()\nplt.savefig(\"similarity_heatmaps.png\", dpi=300)\nplt.close()\n",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.13396/text_sim_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.13593/Hist_error_1000_std_nonstd_2_cam_new.png",
        "caption": "Triangulation of $p=5$ point correspondences incident to a line for $m=2$ views with complete visibility. The triangulation of this setting was done 1000 times, and each histogram shows the frequency both for the average relative error and the running time of the triangulation.",
        "source": "iccv_1/2303.13593/main.tex",
        "arxiv_id": "iccv_1/2303.13593",
        "llava_output": "The image is a histogram showing the frequency of relative error and running time for triangulation of $p=5$ point correspondences incident to a line for $m=2$ views with complete visibility.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some synthetic data for the histograms\nnp.random.seed(42)\nL1_0 = np.random.normal(0.5, 1, 1000)\nL1_1_std = np.random.normal(2, 1, 1000)\nL1_3 = np.random.normal(3, 0.7, 1000)\nL1_3_std = np.random.normal(1.5, 0.8, 1000)\n\n# Plotting\nfig, ax = plt.subplots(figsize=(15, 5))\nax.hist(L1_0, bins=40, range=(-2, 7), alpha=0.7, label=\"(L1,1) 0\", color=\"blue\")\nax.hist(L1_1_std, bins=40, range=(-2, 7), alpha=0.7, label=\"(L1,1) std\", color=\"purple\")\nax.hist(L1_3, bins=40, range=(-2, 7), alpha=0.7, label=\"(L1,3)\", color=\"yellow\")\nax.hist(L1_3_std, bins=40, range=(-2, 7), alpha=0.7, label=\"(L1,3) std\", color=\"pink\")\n\nax.set_title(\"Accuracy: 2 cameras, 5 point correspondences\")\nax.set_xlabel(\"Relative error\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nax.grid(axis=\"y\")\n\nplt.tight_layout()\nplt.savefig(\"triangulation_accuracy.png\", dpi=300)\nplt.close()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Create synthetic data for the histograms\nnp.random.seed(42)\nL1_0_data = np.random.normal(0.5, 1, 1000)\nL1_1_std_data = np.random.normal(2, 1, 1000)\nL1_3_data = np.random.normal(3, 0.7, 1000)\nL1_3_std_data = np.random.normal(1.5, 0.8, 1000)\n\n# Step 2: Setup the plotting\nfig, ax = plt.subplots(figsize=(15, 5))\n\n# Step 3: Plot the histograms\nbins = np.linspace(-2, 7, 50)  # Define the bin edges\nax.hist(L1_0_data, bins=bins, alpha=0.7, label=\"(L1,1) 0\", color=\"blue\")\nax.hist(L1_1_std_data, bins=bins, alpha=0.7, label=\"(L1,1) std\", color=\"purple\")\nax.hist(L1_3_data, bins=bins, alpha=0.7, label=\"(L1,3)\", color=\"yellow\")\nax.hist(L1_3_std_data, bins=bins, alpha=0.7, label=\"(L1,3) std\", color=\"pink\")\n\n# Step 4: Set title, labels, legend, and grid\nax.set_title(\"Accuracy: 2 cameras, 5 point correspondences\")\nax.set_xlabel(\"Relative error\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nax.grid(axis=\"y\")\n\n# Step 5: Save the figure as a PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"triangulation_accuracy.png\", dpi=300)\nplt.close()\n",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.13593/Hist_error_1000_std_nonstd_2_cam_new_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.13593/Hist_error_1000_01234_3_cam_newnew.png",
        "caption": "Triangulation of $p=5$ point correspondences incident to a line for $m=3$ views with complete visibility. The triangulation of this setting was done 1000 times, and each histogram shows the frequency both for the relative error and the running time of the depicted triangulation methods.",
        "source": "iccv_1/2303.13593/main.tex",
        "arxiv_id": "iccv_1/2303.13593",
        "llava_output": "The image is a histogram showing the relative error and running time of triangulation methods for a specific setting.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Create synthetic data for the histograms\nnp.random.seed(42)\nL1_0_data = np.random.normal(0.5, 1, 1000)\nL1_1_data = np.random.normal(1.5, 0.8, 1000)\nL1_2_data = np.random.normal(2.5, 0.7, 1000)\nL1_3_data = np.random.normal(3.5, 0.6, 1000)\nL1_4_data = np.random.normal(4.5, 0.5, 1000)\n\n# Step 2: Setup the plotting\nfig, ax = plt.subplots(figsize=(15, 5))\n\n# Step 3: Plot the histograms\nbins = np.linspace(-2, 7, 50)  # Define the bin edges\nax.hist(L1_0_data, bins=bins, alpha=0.7, label=\"(L1,1) 0\", color=\"blue\")\nax.hist(L1_1_data, bins=bins, alpha=0.7, label=\"(L1,1) 1\", color=\"green\")\nax.hist(L1_2_data, bins=bins, alpha=0.7, label=\"(L1,1) 2\", color=\"yellow\")\nax.hist(L1_3_data, bins=bins, alpha=0.7, label=\"(L1,1) 3\", color=\"orange\")\nax.hist(L1_4_data, bins=bins, alpha=0.7, label=\"(L1,1) 4\", color=\"purple\")\n\n# Step 4: Set title, labels, legend, and grid\nax.set_title(\"Accuracy: 3 cameras, 5 point correspondences\")\nax.set_xlabel(\"Relative error\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nax.grid(axis=\"y\")\n\n# Step 5: Save the figure as a PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"triangulation_accuracy_3_cameras.png\", dpi=300)\nplt.close()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Create synthetic data for the histograms.\nnp.random.seed(42)\nL1_0_data = np.random.normal(0.5, 1, 1000)\nL1_1_data = np.random.normal(1.5, 0.8, 1000)\nL1_2_data = np.random.normal(2.5, 0.7, 1000)\nL1_3_data = np.random.normal(3.5, 0.6, 1000)\nL1_4_data = np.random.normal(4.5, 0.5, 1000)\n\n# Step 2: Setup the plotting environment.\nfig, ax = plt.subplots(figsize=(15, 5))\n\n# Step 3: Define the bins and plot the histograms.\nbins = np.linspace(-2, 7, 50)\ncolors = ['blue', 'green', 'yellow', 'orange', 'purple']\nlabels = [\"(L1,1) 0\", \"(L1,1) 1\", \"(L1,1) 2\", \"(L1,1) 3\", \"(L1,1) 4\"]\ndata = [L1_0_data, L1_1_data, L1_2_data, L1_3_data, L1_4_data]\n\nfor i in range(5):\n    ax.hist(data[i], bins=bins, alpha=0.7, label=labels[i], color=colors[i])\n\n# Step 4: Set title, labels, and other plotting parameters.\nax.set_title(\"Accuracy: 3 cameras, 5 point correspondences\")\nax.set_xlabel(\"Relative error\")\nax.set_ylabel(\"Frequency\")\nax.legend(loc='upper right')\nax.grid(axis=\"y\")\n\n# Step 5: Save the plot as a PNG file with 300dpi resolution.\nplt.tight_layout()\nplt.savefig(\"triangulation_plot.png\", dpi=300)\nplt.show()\n",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.13593/Hist_error_1000_01234_3_cam_newnew_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.13593/Hist_error_100_01234_4_cam.png",
        "caption": "Triangulation of $p=5$ point correspondences incident to a line for $m=4$ views with complete visibility. The triangulation of this setting was done 100 times, and each histogram shows the frequency both for the relative error and the running time of the depicted triangulation methods.",
        "source": "iccv_1/2303.13593/main.tex",
        "arxiv_id": "iccv_1/2303.13593",
        "llava_output": "The image is a histogram showing the frequency of relative error and running time for a triangulation method.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create synthetic data for the histograms.\nnp.random.seed(42)\nL1_0_data = np.random.normal(0.2, 0.8, 100)\nL1_1_data = np.random.normal(1, 0.9, 100)\nL1_2_data = np.random.normal(2, 0.7, 100)\nL1_3_data = np.random.normal(3, 0.5, 100)\nL1_4_data = np.random.normal(4, 0.4, 100)\n\n# Setup the plotting environment.\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# Define the bins and plot the histograms.\nbins = np.linspace(-2, 6, 40)\ncolors = ['blue', 'green', 'yellow', 'orange', 'purple']\nlabels = [\"(L1,1) 0\", \"(L1,1) 1\", \"(L1,1) 2\", \"(L1,1) 3\", \"(L1,1) 4\"]\ndata = [L1_0_data, L1_1_data, L1_2_data, L1_3_data, L1_4_data]\n\nfor i in range(5):\n    ax.hist(data[i], bins=bins, alpha=0.7, label=labels[i], color=colors[i], rwidth=0.8)\n\n# Set title, labels, and other plotting parameters.\nax.set_title(\"Accuracy: 4 cameras, 5 point correspondences\")\nax.set_xlabel(\"Relative error\")\nax.set_ylabel(\"Frequency\")\nax.legend(loc='upper right')\nax.grid(axis=\"y\")\n\n# Save the plot as a PNG file with 300dpi resolution.\nplt.tight_layout()\nplt.savefig(\"triangulation_4cameras_plot.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate mock data to replicate the histograms\nnp.random.seed(0)  # for reproducibility\nL1_0_data = np.random.normal(0, 0.5, 100)\nL1_1_data = np.random.normal(1, 0.7, 100)\nL1_2_data = np.random.normal(2, 0.6, 100)\nL1_3_data = np.random.normal(3, 0.4, 100)\nL1_4_data = np.random.normal(4, 0.5, 100)\n\ndata = [L1_0_data, L1_1_data, L1_2_data, L1_3_data, L1_4_data]\n\n# Step 2: Setup the plotting parameters\ncolors = ['blue', 'green', 'yellow', 'orange', 'purple']\nlabels = [\"(L1,1) 0\", \"(L1,1) 1\", \"(L1,1) 2\", \"(L1,1) 3\", \"(L1,1) 4\"]\nbins = np.linspace(-2, 6, 50)\n\n# Step 3: Create the plot\nfig, ax = plt.subplots(figsize=(8, 5))\nfor i in range(5):\n    ax.hist(data[i], bins=bins, alpha=0.7, color=colors[i], label=labels[i], rwidth=0.8)\n\nax.set_title(\"Accuracy: 4 cameras, 5 point correspondences\")\nax.set_xlabel(\"Relative error\")\nax.set_ylabel(\"Frequency\")\nax.legend(loc='upper right')\nax.grid(True, axis='y')\n\n# Step 4: Save the figure as a PNG file with 300dpi\nplt.tight_layout()\nplt.savefig(\"accuracy_4cameras_5points.png\", dpi=300)\nplt.show()\n",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.13593/Hist_error_100_01234_4_cam_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 2,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2303.14027/cifar_fgsm_8_16_32_resnet_32_v4.png",
        "caption": "Adversarial attack results with running statistics for Euclidean models. The running statistics make the Euclidean models perform slightly better, but significantly more susceptible to adversarial attacks.",
        "source": "iccv_1/2303.14027/appendix.tex",
        "arxiv_id": "iccv_1/2303.14027",
        "llava_output": "The image is a Line Chart that presents the experiment results for adversarial attack results with running statistics for Euclidean models.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Step 1: Prepare the data\nx = [0.8/255, 1.6/255, 3.2/255]\neuclidean = [78, 73, 80]\nw_hnn = [58, 53, 35]\npoincare = [70, 60, 58]\n\n# Step 2: Initialize the plot\nfig, ax = plt.subplots()\n\n# Step 3: Plot the data\nax.plot(x, euclidean, 'o-', label='Euclidean', color='darkblue', linestyle='dashed')\nax.plot(x, w_hnn, 'o-', label='w/ HNN++', color='grey', linestyle='dashed')\nax.plot(x, poincare, 'o-', label='Poincar\u00e9', color='blue', linestyle='dashed')\n\n# Step 4: Set labels, title, and grid\nax.set_xlabel(\"\")\nax.set_ylabel(\"Accuracy (%)\")\nax.set_title(\"\")\nax.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Step 5: Display the legend\nax.legend()\n\n# Step 6: Save the figure as PNG with 300dpi\nplt.tight_layout()\nplt.savefig(\"adversarial_attack_results.png\", dpi=300)\n\n# Step 7: Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Step 2: Define the data\nx_values = [0.8/255, 1.6/255, 3.2/255]\neuclidean_values = [80, 75, 80]\nwhnn_values = [45, 40, 35]\npoincare_values = [70, 60, 55]\n\n# Step 3: Create the plot\nfig, ax = plt.subplots()\n\n# Plot the lines with markers\nax.plot(x_values, euclidean_values, 'o-', color='darkblue', linestyle='dashed', label='Euclidean')\nax.plot(x_values, whnn_values, 'o-', color='grey', linestyle='dashed', label='w/ HNN++')\nax.plot(x_values, poincare_values, 'o-', color='blue', linestyle='dashed', label='Poincar\u00e9')\n\n# Set x-tick labels\nax.set_xticks(x_values)\nax.set_xticklabels(['0.8/255', '1.6/255', '3.2/255'])\n\n# Other styling and labels\nax.set_ylabel('Accuracy (%)')\nax.grid(True, which='both', linestyle='--', linewidth=0.5)\nax.legend(loc='upper right')\n\n# Step 4: Save the plot as a PNG file with 300 dpi\nplt.savefig('adversarial_attack_results.png', dpi=300)\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2303.14027/cifar_fgsm_8_16_32_resnet_32_v4_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 3,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2304.10700/geogpt-long.png",
        "caption": "GeoGPT",
        "source": "iccv_1/2304.10700/appendix.tex",
        "arxiv_id": "iccv_1/2304.10700",
        "llava_output": "1. The image is a bar chart that displays the results of an experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the data\nT_error = [1.00, 1.43, 1.86, 2.29, 2.71, 3.14, 3.57, 4.00]\nT_matches = [5, 10, 15, 20, 25, 30, 35]\nvalues = [\n    [1.12, 4.94, 9.55, 12.93, 14.92, 16.07, 16.72, 17.12],\n    [1.12, 4.94, 9.49, 12.82, 14.79, 15.91, 16.53, 16.92],\n    [1.06, 4.70, 9.04, 12.13, 13.89, 14.84, 15.34, 15.63],\n    [0.98, 4.38, 8.29, 10.98, 12.35, 13.05, 13.40, 13.59],\n    [0.89, 3.91, 7.20, 9.24, 10.23, 10.70, 10.90, 11.01],\n    [0.80, 3.35, 5.94, 7.37, 8.00, 8.27, 8.37, 8.43]\n]\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(8,6))\ncax = ax.matshow(values, cmap=\"plasma\")\nfig.colorbar(cax)\n\n# Set the x-tick labels\nax.set_xticks(np.arange(len(T_error)))\nax.set_yticks(np.arange(len(T_matches)))\nax.set_xticklabels(T_error)\nax.set_yticklabels(T_matches)\n\n# Place text on heatmap\nfor i in range(len(T_matches)):\n    for j in range(len(T_error)):\n        ax.text(j, i, f\"{values[i][j]:.2f}\", ha='center', va='center', color='w', fontsize=8)\n\nax.set_xlabel('Terror(pixels)')\nax.set_ylabel('Tmatches')\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('GeoGPT_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Define the data\nT_error = [1.00, 1.43, 1.86, 2.29, 2.71, 3.14, 3.57, 4.00]\nT_matches = [5, 10, 15, 20, 25, 30, 35]\nvalues = [\n    [1.12, 4.94, 9.55, 12.93, 14.92, 16.07, 16.72, 17.12],\n    [1.12, 4.94, 9.49, 12.82, 14.79, 15.91, 16.53, 16.92],\n    [1.06, 4.70, 9.04, 12.13, 13.89, 14.84, 15.34, 15.63],\n    [0.98, 4.38, 8.29, 10.98, 12.35, 13.05, 13.40, 13.59],\n    [0.89, 3.91, 7.20, 9.24, 10.23, 10.70, 10.90, 11.01],\n    [0.80, 3.35, 5.94, 7.37, 8.00, 8.27, 8.37, 8.43]\n]\n\n# Step 2: Create the plot\nfig, ax = plt.subplots(figsize=(8,6))\ncax = ax.matshow(values, cmap=\"plasma\")\nfig.colorbar(cax)\n\n# Step 3: Set tick labels and place text on heatmap\nax.set_xticks(np.arange(len(T_error)))\nax.set_yticks(np.arange(len(T_matches)))\nax.set_xticklabels(T_error)\nax.set_yticklabels(T_matches)\nfor i in range(len(T_matches)):\n    for j in range(len(T_error)):\n        ax.text(j, i, f\"{values[i][j]:.2f}\", ha='center', va='center', color='w', fontsize=8)\n\n# Step 4: Label axes\nax.set_xlabel('Terror(pixels)')\nax.set_ylabel('Tmatches')\n\n# Step 5: Save the plot as PNG with 300 dpi\nplt.savefig('GeoGPT_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2304.10700/geogpt-long_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2304.10700/geogpt-orbit.png",
        "caption": "GeoGPT",
        "source": "iccv_1/2304.10700/appendix.tex",
        "arxiv_id": "iccv_1/2304.10700",
        "llava_output": "The image is a bar chart that presents the results of an experiment. The chart has a total of 12 bars, each representing a different time period. The bars are arranged in a vertical manner, with the first bar on the left and the last bar on the right. The bars are labeled with numbers",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Define the data\nT_error = [1.00, 1.43, 1.86, 2.29, 2.71, 3.14, 3.57, 4.00]\nT_matches = [5, 10, 15, 20, 25, 30, 35]\nvalues = [\n    [0.34, 1.60, 3.08, 4.22, 4.99, 5.50, 5.84, 6.07],\n    [0.34, 1.60, 3.08, 4.22, 4.99, 5.50, 5.84, 5.81],\n    [0.32, 1.55, 2.49, 4.09, 4.82, 5.29, 5.61, 5.81],\n    [0.26, 1.30, 2.51, 3.38, 3.89, 4.20, 4.39, 4.49],\n    [0.20, 0.99, 1.87, 2.49, 2.80, 2.95, 3.04, 3.09],\n    [0.14, 0.66, 1.19, 1.57, 1.73, 1.80, 1.85, 1.87],\n    [0.08, 0.41, 0.69, 0.87, 0.95, 0.97, 0.99, 1.00]\n]\n\n# Step 2: Create the plot\nfig, ax = plt.subplots(figsize=(8,6))\ncax = ax.matshow(values, cmap=\"plasma\")\nfig.colorbar(cax)\n\n# Step 3: Set tick labels and place text on heatmap\nax.set_xticks(np.arange(len(T_error)))\nax.set_yticks(np.arange(len(T_matches)))\nax.set_xticklabels(T_error)\nax.set_yticklabels(T_matches)\nfor i in range(len(T_matches)):\n    for j in range(len(T_error)):\n        ax.text(j, i, f\"{values[i][j]:.2f}\", ha='center', va='center', color='w', fontsize=8)\n\n# Step 4: Label axes\nax.set_xlabel('Terror(pixels)')\nax.set_ylabel('Tmatches')\n\n# Step 5: Save the plot as PNG with 300 dpi\nplt.savefig('GeoGPT_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nT_error = [1.00, 1.43, 1.86, 2.29, 2.71, 3.14, 3.57, 4.00]\nT_matches = [5, 10, 15, 20, 25, 30, 35]\nvalues = [\n    [0.34, 1.60, 3.08, 4.22, 4.99, 5.50, 5.84, 6.07],\n    [0.34, 1.60, 3.08, 4.22, 4.99, 5.50, 5.84, 5.81],\n    [0.32, 1.55, 2.49, 4.09, 4.82, 5.29, 5.61, 5.81],\n    [0.26, 1.30, 2.51, 3.38, 3.89, 4.20, 4.39, 4.49],\n    [0.20, 0.99, 1.87, 2.49, 2.80, 2.95, 3.04, 3.09],\n    [0.14, 0.66, 1.19, 1.57, 1.73, 1.80, 1.85, 1.87],\n    [0.08, 0.41, 0.69, 0.87, 0.95, 0.97, 0.99, 1.00]\n]\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(8, 6))\ncax = ax.matshow(values, cmap=\"plasma\")\nfig.colorbar(cax)\n\n# Set tick labels and place text on heatmap\nax.set_xticks(np.arange(len(T_error)))\nax.set_yticks(np.arange(len(T_matches)))\nax.set_xticklabels(T_error)\nax.set_yticklabels(T_matches)\nfor i in range(len(T_matches)):\n    for j in range(len(T_error)):\n        ax.text(j, i, f\"{values[i][j]:.2f}\", ha='center', va='center', color='w', fontsize=8)\n\n# Label axes\nax.set_xlabel('Terror(pixels)')\nax.set_ylabel('Tmatches')\n\n# Save the plot\nplt.savefig('GeoGPT_plot.png', dpi=300, bbox_inches='tight')\nplt.show()\n",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2304.10700/geogpt-orbit_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2304.10700/geogpt-hop.png",
        "caption": "GeoGPT",
        "source": "iccv_1/2304.10700/appendix.tex",
        "arxiv_id": "iccv_1/2304.10700",
        "llava_output": "The image is a bar chart that presents the results of an experiment. The chart has a vertical axis and a horizontal axis, with the horizontal axis showing the different time intervals and the vertical axis showing the corresponding temperature values. The bars are labeled with the corresponding time intervals and temperature values, making it easy to understand",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nx = [1.00, 1.43, 1.86, 2.29, 2.71, 3.14, 3.57, 4.00]\ny = [5, 10, 15, 20, 25, 30, 35]\nz = [\n    [0.03, 0.69, 1.39, 2.10, 2.80, 3.40, 3.85, 4.00],\n    [0.03, 0.69, 1.39, 2.10, 2.80, 3.40, 3.85, 4.00],\n    [0.03, 0.20, 0.67, 1.35, 2.03, 2.69, 3.25, 3.67],\n    [0.02, 0.16, 0.54, 1.08, 1.58, 2.06, 2.47, 2.71],\n    [0.02, 0.11, 0.38, 0.75, 1.11, 1.40, 1.62, 1.74],\n    [0.01, 0.07, 0.24, 0.46, 0.68, 0.85, 0.97, 1.02],\n    [0.01, 0.04, 0.14, 0.25, 0.37, 0.45, 0.50, 0.52]\n]\n\nfig, ax = plt.subplots()\n\n# Create the heatmap\nc = ax.pcolormesh(x, y, z, shading='auto', cmap='magma')\n\n# Set the limits\nax.set_xlim(1.00, 4.00)\nax.set_ylim(5, 35)\n\n# Label the axes\nax.set_xlabel('Terror(pixels)')\nax.set_ylabel('Traces')\n\n# Colorbar\ncbar = fig.colorbar(c, ax=ax)\ncbar.set_label('Value')\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('heatmap.png', dpi=300)\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nx_labels = [1.00, 1.43, 1.86, 2.29, 2.71, 3.14, 3.57, 4.00]\ny_labels = [5, 10, 15, 20, 25, 30, 35]\ndata = [\n    [0.03, 0.20, 0.67, 1.35, 2.03, 2.69, 3.25, 3.67],\n    [0.02, 0.16, 0.54, 1.08, 1.58, 2.06, 2.47, 2.71],\n    [0.02, 0.11, 0.38, 0.75, 1.11, 1.40, 1.62, 1.74],\n    [0.01, 0.07, 0.24, 0.46, 0.68, 0.85, 0.97, 1.02],\n    [0.01, 0.04, 0.14, 0.25, 0.37, 0.45, 0.50, 0.52],\n    [0.01, 0.04, 0.14, 0.25, 0.37, 0.45, 0.50, 0.52],\n    [0.01, 0.04, 0.14, 0.25, 0.37, 0.45, 0.50, 0.52]\n]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Create heatmap\nc = ax.imshow(data, interpolation='nearest', cmap='magma', origin='lower', aspect='auto', extent=[1.00, 4.00, 5, 35])\n\n# Set tick positions and labels\nax.set_xticks(x_labels)\nax.set_yticks(y_labels)\nax.set_xticklabels(x_labels)\nax.set_yticklabels(y_labels)\n\n# Set labels for x and y axes\nax.set_xlabel('Terror(pixels)')\nax.set_ylabel('Traces')\n\n# Add colorbar\ncbar = plt.colorbar(c, ax=ax)\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('heatmap.png', dpi=300)\nplt.close()\n",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2304.10700/geogpt-hop_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2306.15925/Weight_Norm.png",
        "caption": "Change in weight norm of the linear classifier based on the representations trained by SBCL on CIFAR-100-LT with imbalance ratio 100 during training.",
        "source": "iccv_1/2306.15925/04-exp.tex",
        "arxiv_id": "iccv_1/2306.15925",
        "llava_output": "The image is a line chart showing the change in weight norm of the linear classifier based on the representations trained by SBCL on CIFAR-100-LT with an imbalance ratio of 100 during training.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data (just for visualization)\nx = np.linspace(0, 100, 100)\ny1 = 5 + 3 * np.sin(0.2 * x) + np.random.normal(0, 0.3, 100)  # Weight norm at epoch 200\ny2 = 5 + 3 * np.sin(0.2 * x + 1) + np.random.normal(0, 0.3, 100)  # Weight norm at epoch 600\ny3 = 5 + 3 * np.sin(0.2 * x + 2) + np.random.normal(0, 0.3, 100)  # Weight norm at epoch 1000\n\nplt.figure(figsize=(8, 6))\n\n# Plotting the data\nplt.plot(x, y1, color='green', label='Weight norm at epoch 200')\nplt.plot(x, y2, color='red', label='Weight norm at epoch 600')\nplt.plot(x, y3, color='blue', label='Weight norm at epoch 1000')\n\n# Setting the labels and title\nplt.xlabel('Class index')\nplt.ylabel('Per-class weight norms')\nplt.title('Change in weight norm of the linear classifier based on the representations trained by SBCL on CIFAR-100-LT with imbalance ratio 100 during training')\n\n# Displaying the legend\nplt.legend()\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('weight_norms_plot.png', dpi=300)\nplt.close()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Set up data. This is mock data since the exact data values are not available.\nx = np.arange(0, 101)\ny1 = 5 + 2 * np.sin(0.1 * x) + 0.5 * np.random.normal(size=x.shape)  # Weight norm at epoch 200\ny2 = 5 + 2 * np.sin(0.1 * x + 2) + 0.5 * np.random.normal(size=x.shape)  # Weight norm at epoch 600\ny3 = 5 + 2 * np.sin(0.1 * x + 4) + 0.5 * np.random.normal(size=x.shape)  # Weight norm at epoch 1000\n\n# Step 2: Create the plot\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y1, color='green', label='Weight norm at epoch 200')\nax.plot(x, y2, color='red', label='Weight norm at epoch 600')\nax.plot(x, y3, color='blue', label='Weight norm at epoch 1000')\n\n# Step 3: Set labels, title, and legend\nax.set_xlabel('Class index')\nax.set_ylabel('Per-class weight norms')\nax.legend()\n\n# Step 4: Save the figure\nplt.tight_layout()\nplt.savefig('weight_norms_plot.png', dpi=300)\nplt.close()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2306.15925/Weight_Norm_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.00398/Abl1.png",
        "caption": "Uncertainty increases with increased masking of the input images (Left) and texts (Right). Results with three vision encoders and one language encoder from CLIP.",
        "source": "iccv_1/2307.00398/ProbVLM_arxiv.tex",
        "arxiv_id": "iccv_1/2307.00398",
        "llava_output": "1. Line Chart: The image shows a line chart with two lines, one for image and one for text. The chart illustrates the uncertainty of the input images and texts as the masking increases.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Set up data. This is mock data since the exact data values are not available.\nx = np.linspace(0, 0.8, 50)\ny1 = np.exp(2 * x) - 1  # ResNet 50\ny2 = np.exp(2.2 * x) - 1.2  # ViT-B16\ny3 = np.exp(2.5 * x) - 1.5  # ViT-B32\ny4 = 1.2 * x  # CLIP Text Encoder\n\n# Step 2: Create the plot\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n# Left plot for Image encoders\naxs[0].plot(x, y1, color='green', label='ResNet 50')\naxs[0].fill_between(x, y1 - 0.05, y1 + 0.05, color='green', alpha=0.2)\naxs[0].plot(x, y2, color='orange', label='ViT-B16')\naxs[0].fill_between(x, y2 - 0.04, y2 + 0.04, color='orange', alpha=0.2)\naxs[0].plot(x, y3, color='purple', label='ViT-B32')\naxs[0].fill_between(x, y3 - 0.03, y3 + 0.03, color='purple', alpha=0.2)\n\n# Right plot for Text encoders\naxs[1].plot(x, y4, color='magenta', label='CLIP Text Encoder')\naxs[1].fill_between(x, y4 - 0.05, y4 + 0.05, color='magenta', alpha=0.2)\n\n# Step 3: Set labels, title, and legend\nfor ax in axs:\n    ax.set_xlabel('Masked Fraction')\n    ax.set_ylabel('Uncertainty Score')\n    ax.legend()\n\naxs[0].set_title('Image')\naxs[1].set_title('Text')\n\n# Step 4: Save the figure\nplt.tight_layout()\nplt.savefig('uncertainty_vs_masking.png', dpi=300)\nplt.close()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Set up mock data since the exact data values are not available.\nx = np.linspace(0, 0.8, 100)\ny_resnet = 50 * x**2\ny_vit_b16 = 40 * x**2 + 10\ny_vit_b32 = 35 * x**2 + 15\ny_clip = 1.2 * x\n\n# For the shading, we generate mock data, considering a small variation.\nvariation = 0.05\ny_resnet_var = y_resnet * np.random.uniform(1 - variation, 1 + variation, y_resnet.shape[0])\ny_vit_b16_var = y_vit_b16 * np.random.uniform(1 - variation, 1 + variation, y_vit_b16.shape[0])\ny_vit_b32_var = y_vit_b32 * np.random.uniform(1 - variation, 1 + variation, y_vit_b32.shape[0])\ny_clip_var = y_clip * np.random.uniform(1 - variation, 1 + variation, y_clip.shape[0])\n\n# Step 2: Create subplots for \"Image\" (left) and \"Text\" (right) visualizations.\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Left plot: Image\naxs[0].plot(x, y_resnet, label='ResNet 50', color='green')\naxs[0].fill_between(x, y_resnet, y_resnet_var, color='green', alpha=0.3)\naxs[0].plot(x, y_vit_b16, label='ViT-B16', color='orange')\naxs[0].fill_between(x, y_vit_b16, y_vit_b16_var, color='orange', alpha=0.3)\naxs[0].plot(x, y_vit_b32, label='ViT-B32', color='purple')\naxs[0].fill_between(x, y_vit_b32, y_vit_b32_var, color='purple', alpha=0.3)\naxs[0].set_title(\"Image\")\naxs[0].legend()\n\n# Right plot: Text\naxs[1].plot(x, y_clip, label='CLIP Text Encoder', color='magenta')\naxs[1].fill_between(x, y_clip, y_clip_var, color='magenta', alpha=0.3)\naxs[1].set_title(\"Text\")\naxs[1].legend()\n\n# Common settings\nfor ax in axs:\n    ax.set_xlabel('Masked Fraction')\n    ax.set_ylabel('Uncertainty Score')\n\n# Step 3: Save the figure.\nplt.tight_layout()\nplt.savefig('uncertainty_plot.png', dpi=300)\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.00398/Abl1_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.00398/activeL2.png",
        "caption": "Results for active learning, with different vision encoders and varying training budgets. For a given encoder, uncertainty-based sampling outperforms random sampling.",
        "source": "iccv_1/2307.00398/ProbVLM_arxiv.tex",
        "arxiv_id": "iccv_1/2307.00398",
        "llava_output": "The image is a line chart showing the results of an experiment with different vision encoders and varying training budgets. The chart is titled \"Results for active learning\" and displays the performance of the encoders in terms of accuracy. The chart shows that for a given encoder, uncertainty-based sampling outper",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Set up mock data\nx = np.linspace(0, 3000, 100)\n\n# Data for each of the lines in the plot\ny_vit_b32_uncertainty = 0.8 - 0.2 * np.exp(-x/2000)\ny_vit_b32_random = 0.75 - 0.25 * np.exp(-x/2000)\ny_vit_b16_uncertainty = 0.78 - 0.2 * np.exp(-x/2500)\ny_vit_b16_random = 0.73 - 0.2 * np.exp(-x/2500)\ny_resnet50_uncertainty = 0.77 - 0.25 * np.exp(-x/2300)\ny_resnet50_random = 0.72 - 0.2 * np.exp(-x/2400)\ny_vit_b32_ft = 0.76 - 0.25 * np.exp(-x/2200)\ny_resnet50_zs = 0.54 + 0.1 * np.exp(-x/1800)\ny_vit_b16_ft = 0.74 - 0.23 * np.exp(-x/2100)\ny_vit_b16_zs = 0.53 + 0.11 * np.exp(-x/1900)\n\n# Step 2: Plot data\nplt.figure(figsize=(10, 6))\n\n# Plot each line\nplt.plot(x, y_vit_b32_uncertainty, label='ViT-B32 (Uncertainty)', color='blue')\nplt.plot(x, y_vit_b32_random, '--', label='ViT-B32 (Random)', color='blue')\nplt.plot(x, y_vit_b16_uncertainty, label='ViT-B16 (Uncertainty)', color='orange')\nplt.plot(x, y_vit_b16_random, '--', label='ViT-B16 (Random)', color='orange')\nplt.plot(x, y_resnet50_uncertainty, label='ResNet50 (Uncertainty)', color='green')\nplt.plot(x, y_resnet50_random, '--', label='ResNet50 (Random)', color='green')\nplt.plot(x, y_vit_b32_ft, '-.', label='ViT-B32 (FT)', color='blue')\nplt.plot(x, y_resnet50_zs, ':', label='ResNet50 (ZS)', color='green')\nplt.plot(x, y_vit_b16_ft, '-.', label='ViT-B16 (FT)', color='orange')\nplt.plot(x, y_vit_b16_zs, ':', label='ViT-B16 (ZS)', color='orange')\n\n# Step 3: Set title, labels, legend, and other plot properties\nplt.xlabel('Number of Samples (in the Training Budget)')\nplt.ylabel('R@1')\nplt.title('Results for active learning, with different vision encoders and varying training budgets')\nplt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.tight_layout()\n\n# Step 4: Save the figure\nplt.savefig('active_learning_results.png', dpi=300)\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Set up mock data\nx = np.linspace(0, 3000, 100)\n\n# Mock data based on the visual trend in the image\ny_data = {\n    'ViT-B32 (Uncertainty)': 0.8 - 0.2 * np.exp(-x/2000),\n    'ViT-B32 (Random)': 0.75 - 0.25 * np.exp(-x/2000),\n    'ViT-B16 (Uncertainty)': 0.78 - 0.2 * np.exp(-x/2500),\n    'ViT-B16 (Random)': 0.73 - 0.2 * np.exp(-x/2500),\n    'ResNet50 (Uncertainty)': 0.77 - 0.25 * np.exp(-x/2300),\n    'ResNet50 (Random)': 0.72 - 0.2 * np.exp(-x/2400),\n    'ViT-B32 (FT)': 0.76 - 0.25 * np.exp(-x/2200),\n    'ResNet50 (ZS)': 0.54 + 0.1 * np.exp(-x/1800),\n    'ViT-B16 (FT)': 0.74 - 0.23 * np.exp(-x/2100),\n    'ViT-B16 (ZS)': 0.53 + 0.11 * np.exp(-x/1900)\n}\n\ncolors = {\n    'ViT-B32 (Uncertainty)': 'blue',\n    'ViT-B32 (Random)': 'blue',\n    'ViT-B16 (Uncertainty)': 'orange',\n    'ViT-B16 (Random)': 'orange',\n    'ResNet50 (Uncertainty)': 'green',\n    'ResNet50 (Random)': 'green',\n    'ViT-B32 (FT)': 'blue',\n    'ResNet50 (ZS)': 'green',\n    'ViT-B16 (FT)': 'orange',\n    'ViT-B16 (ZS)': 'orange'\n}\n\nlinestyles = {\n    'ViT-B32 (Uncertainty)': '-',\n    'ViT-B32 (Random)': '--',\n    'ViT-B16 (Uncertainty)': '-',\n    'ViT-B16 (Random)': '--',\n    'ResNet50 (Uncertainty)': '-',\n    'ResNet50 (Random)': '--',\n    'ViT-B32 (FT)': '-.',\n    'ResNet50 (ZS)': ':',\n    'ViT-B16 (FT)': '-.',\n    'ViT-B16 (ZS)': ':'\n}\n\n# Step 2: Plot data\nplt.figure(figsize=(10, 6))\n\nfor key, value in y_data.items():\n    plt.plot(x, value, label=key, color=colors[key], linestyle=linestyles[key])\n\n# Step 3: Set labels, legend, and other plot properties\nplt.xlabel('Number of Samples (in the Training Budget)')\nplt.ylabel('R@1')\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1))\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Step 4: Save the figure\nplt.tight_layout()\nplt.savefig('active_learning_results.png', dpi=300)\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.00398/activeL2_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 4,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.08300/ranking.png",
        "caption": "The ranking correlation of architecture generator.",
        "source": "iccv_1/2307.08300/iccv.tex",
        "arxiv_id": "iccv_1/2307.08300",
        "llava_output": "The image is a scatter plot showing the ranking correlation of architecture generator.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Set up mock data\nx = np.linspace(45, 75, 15)\n\n# Mock data based on the visual trend in the image\ny_all = 0.2 * x + 50\ny_good = 0.1 * x + 60\ny_bad = -0.15 * x + 87\n\n# Scatter points\nx_points = [48, 51, 53, 56, 58, 61, 63, 66, 68, 70, 73, 50, 54, 57, 60, 64, 67, 71, 74]\ny_points = [70, 72, 73, 69, 71, 73.5, 74, 70.5, 72.5, 74.5, 75.5, 68.5, 69.5, 71.5, 69, 70.5, 72, 73, 75]\n\n# Step 2: Plot data\nplt.figure(figsize=(7, 5))\n\n# Plot scatter points\nplt.scatter(x_points, y_points, color='blue', s=50)\n\n# Plot lines\nplt.plot(x, y_all, label='Correlation with all subnets', color='blue', linewidth=2)\nplt.plot(x, y_good, label='Correlation with good subnets', color='orange', linewidth=2)\nplt.plot(x, y_bad, label='Correlation with bad subnets', color='green', linewidth=2)\n\n# Step 3: Set labels, legend, and other plot properties\nplt.xlabel('Inherited Top1-Acc(%)')\nplt.ylabel('Inferred Top1-Acc(%)')\nplt.legend()\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Step 4: Save the figure\nplt.tight_layout()\nplt.savefig('correlation_plot.png', dpi=300)\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Define the data points\ninherited = [48, 51, 53, 56, 58, 61, 63, 66, 68, 70, 73, 50, 54, 57, 60, 64, 67, 71, 74]\ninferred = [70, 72, 73, 69, 71, 73.5, 74, 70.5, 72.5, 74.5, 75.5, 68.5, 69.5, 71.5, 69, 70.5, 72, 73, 75]\n\n# Step 2: Plot the data points\nplt.scatter(inherited, inferred, color='blue')\n\n# Step 3: Define the regression lines\nx = np.linspace(45, 75, 100)\ny_all_subnets = 0.2 * x + 50\ny_good_subnets = 0.1 * x + 60\ny_bad_subnets = -0.15 * x + 87\n\n# Step 4: Plot the regression lines\nplt.plot(x, y_all_subnets, label='Correlation with all subnets', color='blue')\nplt.plot(x, y_good_subnets, label='Correlation with good subnets', color='orange')\nplt.plot(x, y_bad_subnets, label='Correlation with bad subnets', color='green')\n\n# Step 5: Set the title, labels, and legend\nplt.xlabel('Inherited Top1-Acc(%)')\nplt.ylabel('Inferred Top1-Acc(%)')\nplt.legend()\n\n# Step 6: Save the plot as a PNG file with 300dpi\nplt.tight_layout()\nplt.savefig('correlation_plot.png', dpi=300)\n\nplt.show()\n",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.08300/ranking_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.08300/step.png",
        "caption": "FLOPs/Accuracy tradeoffs of ShiftNAS with different steps",
        "source": "iccv_1/2307.08300/iccv.tex",
        "arxiv_id": "iccv_1/2307.08300",
        "llava_output": "The image is a line chart that shows the FLOPs/Accuracy tradeoffs of ShiftNAS with different steps.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data\ngflops = [1.3, 1.4, 1.5, 1.6, 1.7, 1.8]\naccuracy_005 = [75.8, 76.2, 76.6, 77.0, 77.2, 77.5]\naccuracy_01 = [75.8, 76.3, 76.8, 77.1, 77.3, 77.6]\naccuracy_02 = [75.9, 76.5, 76.9, 77.2, 77.4, 77.6]\n\n# Plotting\nplt.plot(gflops, accuracy_005, marker='o', label='step=0.05', color='blue')\nplt.plot(gflops, accuracy_01, marker='o', label='step=0.1', color='orange')\nplt.plot(gflops, accuracy_02, marker='o', label='step=0.2', color='green')\n\n# Setting labels, title, and legend\nplt.xlabel('GFLOPs')\nplt.ylabel('Top1-Acc(%)')\nplt.legend()\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('flops_accuracy_tradeoffs.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Step-by-step approach\n\n# 1. Define data points\ngflops = [1.3, 1.4, 1.5, 1.6, 1.7, 1.8]\nstep_005 = [75.8, 76.2, 76.6, 77.0, 77.2, 77.5]\nstep_01 = [75.8, 76.3, 76.8, 77.1, 77.3, 77.6]\nstep_02 = [75.9, 76.5, 76.9, 77.2, 77.4, 77.6]\n\n# 2. Create a new figure with size\nplt.figure(figsize=(6, 4))\n\n# 3. Plot the data for each step size\nplt.plot(gflops, step_005, color=\"blue\", marker=\"o\", linestyle=\"-\", label=\"step=0.05\")\nplt.plot(gflops, step_01, color=\"orange\", marker=\"o\", linestyle=\"-\", label=\"step=0.1\")\nplt.plot(gflops, step_02, color=\"green\", marker=\"o\", linestyle=\"-\", label=\"step=0.2\")\n\n# 4. Set labels, title, legend, and grid\nplt.xlabel(\"GFLOPs\")\nplt.ylabel(\"Top1-Acc(%)\")\nplt.legend()\nplt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n\n# 5. Adjust plot limits and ticks if necessary\nplt.ylim(75.5, 77.7)\n\n# 6. Save the figure with 300 dpi\nplt.tight_layout()\nplt.savefig(\"flops_accuracy_tradeoffs.png\", dpi=300)\n\n# 7. Show the plot\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.08300/step_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.08504/vqa_throughput_pa_3.png",
        "caption": "VQA performance and throughput of \\modelname on different selection ratio of Text-Guided Patch Abstraction.",
        "source": "iccv_1/2307.08504/main.tex",
        "arxiv_id": "iccv_1/2307.08504",
        "llava_output": "The image is a line chart that presents the VQA performance and throughput of a model on different selection ratios of Text-Guided Patch Abstraction.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data\nselection_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\nvqa_result = [77.76, 77.85, 77.82, 77.84, 77.83, 77.89, 77.82]\nthroughput = [-376.23, -360.23, -342.32, -320.23, -289.23, -269.21, -256.42]\n\n# Create a figure and a set of subplots\nfig, ax1 = plt.subplots(figsize=(7, 4))\n\n# Plot VQA-Result\nax1.plot(selection_ratio, vqa_result, color=\"orange\", marker=\"o\", linestyle=\"-\", label=\"VQA-Result\")\nax1.set_xlabel(\"Selection Ratio\")\nax1.set_ylabel(\"ACC\", color=\"orange\")\nax1.tick_params(axis='y', labelcolor=\"orange\")\n\n# Create a second y-axis that shares the same x-axis\nax2 = ax1.twinx()\nax2.plot(selection_ratio, throughput, color=\"red\", marker=\"s\", linestyle=\"-\", label=\"Throughput\")\nax2.set_ylabel(\"Num\", color=\"red\")\nax2.tick_params(axis='y', labelcolor=\"red\")\n\n# Set the limits of the y-axis based on data\nax1.set_ylim(77.60, 77.95)\nax2.set_ylim(-400, -240)\n\n# Annotate points with their respective values\nfor i, txt in enumerate(vqa_result):\n    ax1.annotate(f\"{txt:.2f}\", (selection_ratio[i], vqa_result[i] + 0.01), fontsize=9, ha='center')\nfor i, txt in enumerate(throughput):\n    ax2.annotate(f\"{txt:.2f}\", (selection_ratio[i], throughput[i] - 10), fontsize=9, ha='center', color=\"red\")\n\n# Save the figure with 300 dpi\nplt.tight_layout()\nplt.savefig(\"vqa_performance_throughput.png\", dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\n\n# Step 1: Define the data\nselection_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\nvqa_result = [77.76, 77.85, 77.82, 77.84, 77.83, 77.89, 77.82]\nthroughput = [376.23, 360.23, 342.32, 320.23, 289.23, 269.21, 256.42]\n\n# Step 2: Create a figure and axis\nfig, ax1 = plt.subplots(figsize=(8, 5))\n\n# Step 3: Plot VQA-Result\nax1.set_xlabel(\"Selection Ratio\")\nax1.set_ylabel(\"ACC\", color=\"orange\")\nax1.plot(selection_ratio, vqa_result, 'o-', color=\"orange\", label=\"VQA-Result\")\nax1.tick_params(axis='y', labelcolor=\"orange\")\n\n# Step 4: Instantiate a second y-axis\nax2 = ax1.twinx()\nax2.set_ylabel(\"Num\", color=\"red\")\nax2.plot(selection_ratio, throughput, 's-', color=\"red\", label=\"Throughput\")\nax2.tick_params(axis='y', labelcolor=\"red\")\n\n# Step 5: Annotate the data points\nfor (x, y1, y2) in zip(selection_ratio, vqa_result, throughput):\n    ax1.text(x, y1, f\"{y1:.2f}\", ha=\"center\", va=\"bottom\", color=\"orange\")\n    ax2.text(x, y2, f\"{y2:.2f}\", ha=\"center\", va=\"top\", color=\"red\")\n\n# Step 6: Save the plot as a .png with 300dpi\nplt.tight_layout()\nplt.savefig(\"vqa_performance_throughput.png\", dpi=300)\n\n# Step 7: Display the plot\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.08504/vqa_throughput_pa_3_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.09004/n626.jpg",
        "caption": "Performances of SORD, POE, and Ord2Seq (PVT) for each category on the DR dataset, showing the proportions of samples that truly belong to one level and are predicted to the correct, adjacent, and other levels. Although overall performance is still limited on unbalanced categories, Ord2Seq significantly improves the Accuracy performance in distinguishing adjacent categories.",
        "source": "iccv_1/2307.09004/egbib.tex",
        "arxiv_id": "iccv_1/2307.09004",
        "llava_output": "100%</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Define the data\nlevels = ['1', '2', '3', '4', '5']\nmodels = ['M1', 'M2', 'M3']\n\n# Proportions for Correct, Adjacent, and Other for each model in each level\ndata = {\n    'M1': {\n        'Correct': [98, 92, 15, 52, 5],\n        'Adjacent': [2, 8, 50, 34, 55],\n        'Other': [0, 0, 35, 14, 40]\n    },\n    'M2': {\n        'Correct': [96, 84, 22, 47, 28],\n        'Adjacent': [3, 16, 58, 40, 42],\n        'Other': [1, 0, 20, 13, 30]\n    },\n    'M3': {\n        'Correct': [92, 84, 11, 28, 13],\n        'Adjacent': [6, 8, 16, 29, 58],\n        'Other': [2, 8, 73, 43, 29]\n    }\n}\n\ncolors = {'Correct': 'red', 'Adjacent': 'blue', 'Other': 'gray'}\n\n# Step 2: Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Step 3: Plot the stacked bar chart\nbar_width = 0.25\nindex = np.arange(len(levels))\nfor i, model in enumerate(models):\n    bottom = np.zeros(len(levels))\n    for category in ['Correct', 'Adjacent', 'Other']:\n        ax.bar(index + i*bar_width, data[model][category], bar_width, color=colors[category], label=f'{model} {category}' if 'M1' in model else \"\")\n        for j, (level, value) in enumerate(zip(levels, data[model][category])):\n            ax.text(j + i*bar_width, bottom[j] + value/2, f\"{value}%\", ha=\"center\", va=\"center\", fontsize=9, color=\"white\")\n            bottom[j] += value\n\n# Step 4: Adjust axis and labels\nax.set_xlabel('Level')\nax.set_ylabel('Proportion (%)')\nax.set_title('Performances of SORD, POE, and Ord2Seq (PVT) for each category on the DR dataset')\nax.set_xticks(index + bar_width)\nax.set_xticklabels(levels)\nax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n\n# Step 5: Save the plot as a .png with 300dpi\nplt.tight_layout()\nplt.savefig(\"performance_comparison.png\", dpi=300, bbox_inches=\"tight\")\n\n# Step 6: Display the plot\nplt.show()\n",
        "code_cot": "",
        "type": "Others",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.09004/n626_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.11077/tsne_perplexity-15.0_person-dog-apple.png",
        "caption": "t-SNE visualization of ground truth annotations. AlignDet pre-training results in better class separation.",
        "source": "iccv_1/2307.11077/egpaper_final.tex",
        "arxiv_id": "iccv_1/2307.11077",
        "llava_output": "This image is a scatter plot showing the results of a clustering algorithm. The scatter plot is divided into two sections, with one section displaying the results of the clustering algorithm and the other section displaying the ground truth annotations. The scatter plot is used to visualize the clustering performance and compare it to the actual",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Random data generation (this would ideally be your t-SNE data)\nnp.random.seed(0)\nn_points = 500\nperson_random = np.random.multivariate_normal([0, 0], [[30, 15], [15, 30]], n_points)\ndog_random = np.random.multivariate_normal([15, 15], [[30, 15], [15, 30]], n_points)\napple_random = np.random.multivariate_normal([7.5, -7.5], [[30, 15], [15, 30]], n_points)\n\nperson_ours = np.random.multivariate_normal([0, 0], [[15, 10], [10, 20]], n_points)\ndog_ours = np.random.multivariate_normal([15, 10], [[15, 5], [5, 15]], n_points)\napple_ours = np.random.multivariate_normal([7.5, -10], [[20, 10], [10, 20]], n_points)\n\n# Plotting\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nfor ax, title, person_data, dog_data, apple_data in zip(axes, ['Random Initialization', 'Ours'], \n                                                        [person_random, person_ours], \n                                                        [dog_random, dog_ours], \n                                                        [apple_random, apple_ours]):\n    ax.scatter(person_data[:, 0], person_data[:, 1], color='blue', label='Person')\n    ax.scatter(dog_data[:, 0], dog_data[:, 1], color='orange', label='Dog')\n    ax.scatter(apple_data[:, 0], apple_data[:, 1], color='green', label='Apple')\n    ax.set_title(title)\n    ax.legend()\n\n# Save figure\nplt.tight_layout()\nplt.savefig(\"t-SNE_visualization.png\", dpi=300)\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random t-SNE data\nnp.random.seed(0)\nperson_random = np.random.multivariate_normal([0, 0], [[30, 15], [15, 30]], 300)\ndog_random = np.random.multivariate_normal([15, 15], [[30, 15], [15, 30]], 300)\napple_random = np.random.multivariate_normal([7.5, -7.5], [[30, 15], [15, 30]], 300)\n\nperson_ours = np.random.multivariate_normal([0, 0], [[15, 10], [10, 20]], 300)\ndog_ours = np.random.multivariate_normal([15, 10], [[15, 5], [5, 15]], 300)\napple_ours = np.random.multivariate_normal([7.5, -10], [[20, 10], [10, 20]], 300)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Random Initialization\nax1.scatter(person_random[:, 0], person_random[:, 1], color='blue', label='Person')\nax1.scatter(dog_random[:, 0], dog_random[:, 1], color='orange', label='Dog')\nax1.scatter(apple_random[:, 0], apple_random[:, 1], color='green', label='Apple')\nax1.set_title('Random Initialization')\nax1.legend()\n\n# Ours\nax2.scatter(person_ours[:, 0], person_ours[:, 1], color='blue', label='Person')\nax2.scatter(dog_ours[:, 0], dog_ours[:, 1], color='orange', label='Dog')\nax2.scatter(apple_ours[:, 0], apple_ours[:, 1], color='green', label='Apple')\nax2.set_title('Ours')\nax2.legend()\n\n# Save as PNG with 300dpi\nplt.tight_layout()\nplt.savefig('t-SNE_plot.png', dpi=300)\nplt.show()\n",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.11077/tsne_perplexity-15.0_person-dog-apple_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.11077/retinanet_loss.png",
        "caption": "Fine-tuning losses of RetinaNet on COCO train 2017.",
        "source": "iccv_1/2307.11077/egpaper_final.tex",
        "arxiv_id": "iccv_1/2307.11077",
        "llava_output": "The image is a scatter plot showing the fine-tuning losses of RetinaNet on COCO train 2017.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the x-axis represents 'iterations' and the y-axis represents 'loss'\niterations = np.linspace(0, 80000, 800)  # Simulating 80000 iterations\n# Simulated loss values for 'ours' and 'random initialization'\n# Since I don't have the exact data, I'm using an exponential decay function to simulate the loss\nloss_cls_ours = np.exp(-iterations / 20000) * 0.6 + 0.2\nloss_cls_random_init = loss_cls_ours * (1 + np.random.normal(0, 0.01, size=loss_cls_ours.shape))\nloss_bbox_ours = np.exp(-iterations / 20000) * 0.3 + 0.2\nloss_bbox_random_init = loss_bbox_ours * (1 + np.random.normal(0, 0.01, size=loss_bbox_ours.shape))\n\n# Create figure and axis objects\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))  # The figure size is adjusted for visual similarity\n\n# Plotting for classification loss\naxs[0].plot(iterations, loss_cls_ours, label='Ours', color='blue')\naxs[0].plot(iterations, loss_cls_random_init, label='Random Initialization', color='red', linestyle='dashed')\naxs[0].set_title('loss_cls')\naxs[0].set_xlabel('Iterations')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n\n# Plotting for bounding box loss\naxs[1].plot(iterations, loss_bbox_ours, label='Ours', color='blue')\naxs[1].plot(iterations, loss_bbox_random_init, label='Random Initialization', color='red', linestyle='dashed')\naxs[1].set_title('loss_bbox')\naxs[1].set_xlabel('Iterations')\naxs[1].legend()\n\n# Tight layout to ensure no overlapping of plots\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/loss_plot.png', dpi=300)\n\n# Output the path to the saved file\n'/mnt/data/loss_plot.png'\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Given the context, it is likely that these are loss values during training iterations.\n# We'll simulate a typical loss curve that decreases over time with some noise.\n\n# Create a figure and two subplots, side by side.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Generate some data that looks like the original plot\niterations = np.arange(0, 80000, 100)  # Iterations from 0 to 80k\nloss_cls = 0.8 * np.exp(-iterations / 20000) + 0.2 + np.random.normal(0, 0.01, len(iterations))\nloss_bbox = 0.5 * np.exp(-iterations / 20000) + 0.3 + np.random.normal(0, 0.01, len(iterations))\n\n# Assuming 'Ours' and 'Random Initialization' are almost similar, we'll use the same values for both\n# Plotting loss_cls\nax1.plot(iterations, loss_cls, 'r', label='Ours')\nax1.plot(iterations, loss_cls + np.random.normal(0, 0.005, len(iterations)), 'b--', label='Random Initialization')\nax1.set_title('loss_cls')\nax1.set_xlabel('Iterations')\nax1.set_ylabel('Loss')\nax1.legend()\n\n# Plotting loss_bbox\nax2.plot(iterations, loss_bbox, 'r', label='Ours')\nax2.plot(iterations, loss_bbox + np.random.normal(0, 0.005, len(iterations)), 'b--', label='Random Initialization')\nax2.set_title('loss_bbox')\nax2.set_xlabel('Iterations')\nax2.legend()\n\n# Set the layout to tight for both subplots\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/fine_tuning_losses.png', dpi=300)\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.11077/retinanet_loss_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.12101/IoU_vis_1.jpg",
        "caption": "Bag quality (IoU of proposals with GT) of construction in SSD-Det. B.S. (blue) means neighborhood sampler. SPSD I (orange) denotes single SPSD adopted. SPSD II (yellow) is two SPSD and interactive structure adopted. SPSD II significantly improves the quality.",
        "source": "iccv_1/2307.12101/paper_supple.tex",
        "arxiv_id": "iccv_1/2307.12101",
        "llava_output": "The image is a set of five bar charts, each showing the bag quality (IoU of proposals with GT) of construction in SSD-Det. B.S. (blue) means neighborhood sampler. SPSD I (orange) denotes single SPSD adopted. SPS",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming we have the data for IoU mean, max, and top10 mean for each method\n# As actual data is not provided, I'll create dummy data that resembles the plot\nnp.random.seed(0)\n\n# Creating data\niou_values = np.random.rand(3, 1000)\niou_values[1] *= np.linspace(0.9, 1.1, 1000)  # Adjust to make SPSD I a bit higher\niou_values[2] *= np.linspace(1.1, 1.3, 1000)  # Adjust to make SPSD II significantly higher\n\n# Creating figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Titles\ntitles = ['IoU mean', 'IoU max', 'IoU top10 mean']\n\n# Colors\ncolors = ['blue', 'orange', 'yellow']\n\n# Method labels\nmethods = ['B.S.', 'SPSD I', 'SPSD II']\n\n# Box plot data\nbox_data = [np.mean(iou_values, axis=1), np.max(iou_values, axis=1), iou_values[:, :10].mean(axis=1)]\n\nfor i in range(3):\n    # Histograms\n    axs[i].hist(iou_values, bins=30, color=colors, label=methods, stacked=True)\n    axs[i].set_title(titles[i])\n    \n    # Box plots\n    bp = axs[i].boxplot(box_data[i], vert=False, patch_artist=True, positions=[0.25], widths=[0.1])\n    for patch, color in zip(bp['boxes'], colors):\n        patch.set_facecolor(color)\n    \n    # Setting labels for x-axis\n    axs[i].set_xlabel('Values')\n\n# Setting the legend for the first plot\naxs[0].legend(loc='upper left')\n\n# Adjust layout\nplt.tight_layout()\n\n",
        "code_cot": "",
        "type": "Others",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.12101/IoU_vis_1_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.13539/Ablation_Eta.png",
        "caption": "Learning Rate ($\\eta$)",
        "source": "iccv_1/2307.13539/Appendix.tex",
        "arxiv_id": "iccv_1/2307.13539",
        "llava_output": "The image is a line chart showing the learning rate ($\\eta$) for a neural network.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming we have the data for ECE and OE for the given learning rates\n# As actual data is not provided, I'll create dummy data that resembles the plot\nlearning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]\nece_values = [2.0, 1.8, 1.6, 1.4, 1.2]  # Dummy ECE values\noe_values = [1.5, 1.3, 1.1, 0.9, 0.7]   # Dummy OE values\n\n# Creating figure\nfig, ax = plt.subplots()\n\n# Plotting the ECE and OE values\nax.scatter(learning_rates, ece_values, color='blue', label='ECE')\nax.scatter(learning_rates, oe_values, color='red', label='OE', marker='^')\n\n# Setting the x-axis to log scale\nax.set_xscale('log')\n\n# Adding legend\nax.legend()\n\n# Adding title and labels\nax.set_title('ECE/OE (%)')\nax.set_xlabel('Learning Rate ($\\\\eta$)')\n\n# Save the figure\nplt.savefig('/mnt/data/learning_rate_plot.png', dpi=300)\n",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.13539/Ablation_Eta_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.15539/detection_bias.png",
        "caption": "Illustration of detection bias. The experiment is conducted under Blend attack on CIFAR-10, using an SSL pre-trained ResNet-18 and LN detected samples.",
        "source": "iccv_1/2307.15539/appendix.tex",
        "arxiv_id": "iccv_1/2307.15539",
        "llava_output": "2 Line Charts are presented in the image, showing the detection bias of the experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming we have the data for CA and ASR over epochs\n# As actual data is not provided, I'll create dummy data that resembles the plot\nepochs = list(range(101))\nca_values = 86 + np.random.rand(101) * 6  # Dummy CA values\nasr_values = np.abs(np.random.rand(101) * 30 - np.linspace(0, 20, 101))  # Dummy ASR values\n\n# Creating figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plotting the CA values\nax1.plot(epochs, ca_values, color='blue')\nax1.set_title('CA')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('CA')\nax1.set_ylim(86, 92)\n\n# Plotting the ASR values\nax2.plot(epochs, asr_values, color='red')\nax2.set_title('ASR')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('ASR')\nax2.set_ylim(0, 40)\n\n# Adjust layout to prevent overlap\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/detection_bias_plot.png', dpi=300)\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.15539/detection_bias_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.15539/process.png",
        "caption": "Illustration of training process using NAB under BadNets attack. LGA and NC are adopted for detection and relabeling, respectively.",
        "source": "iccv_1/2307.15539/appendix.tex",
        "arxiv_id": "iccv_1/2307.15539",
        "llava_output": "1. The image is a bar chart showing the accuracy of the model under different attacks.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming we have the data for CA, BA, ASR, clean loss, poison loss and poison+stamp loss over epochs\n# As actual data is not provided, I'll create dummy data that resembles the plot\nepochs = list(range(101))\nca_values = 80 - np.random.rand(101) * 3  # Dummy CA values, decreasing\nba_values = 80 - np.random.rand(101) * 2  # Dummy BA values, decreasing slower than CA\nasr_values = np.random.rand(101) * 80  # Dummy ASR values, random\n\n# Loss values are typically decreasing, so we'll use an exponential decay to simulate this\nclean_loss = np.exp(-np.array(epochs) / 50) * 2\npoison_loss = clean_loss - np.random.rand(101) * 0.1\npoison_stamp_loss = clean_loss - np.random.rand(101) * 0.2\n\n# Creating figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plotting the accuracy values\nax1.plot(epochs, ca_values, color='blue', label='CA')\nax1.plot(epochs, ba_values, color='red', label='BA')\nax1.plot(epochs, asr_values, color='gray', label='ASR')\nax1.set_title('Accuracy')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Accuracy')\nax1.legend()\n\n# Plotting the loss values\nax2.plot(epochs, clean_loss, color='blue', label='clean')\nax2.plot(epochs, poison_loss, color='red', label='poison')\nax2.plot(epochs, poison_stamp_loss, color='gray', label='poison+stamp')\nax2.set_title('Loss')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Loss')\nax2.legend()\n\n# Adjust layout to prevent overlap\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/training_process_plot.png', dpi=300)\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.15539/process_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.15539/representations.png",
        "caption": "Representations under the effect of adversarial backdoor (AB) and non-adversarial backdoor (NAB), which are injected by attackers and defenders respectively. ``Stamp'' is the trigger pattern for NAB. (a) Clean samples are not influenced by backdoor. (b) AB changes model behavior on poisoned samples. (c) NAB is not triggered on clean samples. (d) NAB suppresses the effectiveness of AB on poisoned samples.",
        "source": "iccv_1/2307.15539/main.tex",
        "arxiv_id": "iccv_1/2307.15539",
        "llava_output": "The image is a set of four different colored images, each with a different label. The labels are \"Clean\", \"Poison\", \"Clean + Stamp\", and \"Poison + Stamp\". The colors of the images are blue, orange, red, and yellow. The images are likely meant to",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming we have the data for the scatter plots\n# As actual data is not provided, I'll create dummy data that resembles the plot\nnp.random.seed(0)  # For reproducibility\nn_samples = 1000\nn_features = 2\n\n# Generating random 2D points for each subplot\ndata_clean = np.random.randn(n_samples, n_features)\ndata_poison = np.random.randn(n_samples, n_features) + np.array([5, 0])  # Offset for visual distinction\n\n# Creating subplots\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\n# Plot (a)\naxs[0, 0].scatter(data_clean[:, 0], data_clean[:, 1], c=np.random.rand(n_samples), alpha=0.5)\naxs[0, 0].set_title('(a)')\n\n# Plot (b)\naxs[0, 1].scatter(data_clean[:, 0], data_clean[:, 1], c='blue', alpha=0.5, label='clean')\naxs[0, 1].scatter(data_poison[:, 0], data_poison[:, 1], c='red', alpha=0.5, label='poison')\naxs[0, 1].legend(loc='lower right')\naxs[0, 1].set_title('(b)')\n\n# Plot (c)\naxs[1, 0].scatter(data_clean[:, 0], data_clean[:, 1], c='blue', alpha=0.5, label='clean')\naxs[1, 0].scatter(data_clean[:, 0] + 1, data_clean[:, 1] + 1, c='darkblue', alpha=0.5, label='clean+stamp')\naxs[1, 0].legend(loc='lower right')\naxs[1, 0].set_title('(c)')\n\n# Plot (d)\naxs[1, 1].scatter(data_clean[:, 0], data_clean[:, 1], c='blue', alpha=0.5, label='clean')\naxs[1, 1].scatter(data_poison[:, 0], data_poison[:, 1], c='orange', alpha=0.5, label='poison+stamp')\naxs[1, 1].legend(loc='lower right')\naxs[1, 1].set_title('(d)')\n\n# Adjust layout to prevent overlap\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/backdoor_effects_plot.png', dpi=300)\n",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.15539/representations_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2307.16634/different_backbones.png",
        "caption": "Quality of pseudo labels using different backbones for CLIP's image encoder",
        "source": "iccv_1/2307.16634/main_final.tex",
        "arxiv_id": "iccv_1/2307.16634",
        "llava_output": "The image is a bar chart showing the quality of pseudo labels using different backbones for CLIP's image encoder. The chart has three bars, each representing a different backbone. The bars are labeled with the corresponding backbone names, and the quality of the pseudo labels is shown on the y-",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data for the plot\nbackbones = ['ViT-B_32', 'RN101', 'RN50x16', 'RN50x64']\nglobal_mAP = [83, 85, 86, 89]\nlocal_mAP = [84, 86, 87, 91]\n\nx = range(len(backbones))\n\n# Create bar plot\nplt.figure(figsize=(8, 4))\nplt.bar(x, global_mAP, width=0.4, label='global', color='blue', align='center')\nplt.bar(x, local_mAP, width=0.4, label='local', color='orange', align='edge')\nplt.xlabel('Different image encoder\\'s backbones for CLIP')\nplt.ylabel('mAP (%)')\nplt.title('Quality of pseudo labels using different backbones for CLIP\\'s image encoder')\nplt.xticks(x, backbones)\nplt.legend()\nplt.grid(axis='y', linestyle='--')\n\n# Save the figure\nplt.savefig('/mnt/data/clip_backbones_mAP.png', dpi=300)\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2307.16634/different_backbones_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.01236/perplexity_val.png",
        "caption": "Sentence perplexity distributions on the validation set. Only the perplexities ranging from 0 to 105 are shown for the ease of reading.",
        "source": "iccv_1/2308.01236/egpaper_final.tex",
        "arxiv_id": "iccv_1/2308.01236",
        "llava_output": "1. Histogram</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\ncategories = ['[0,5)', '[10,15)', '[20,25)', '[30,35)', '[40,45)', '[50,55)', '[60,65)', '[70,75)', '[80,85)', '[90,95)', '[100,105)']\nmatched = [0.4, 0.35, 0.2, 0.05, 0, 0, 0, 0, 0, 0, 0]  # Dummy data for matched expressions\nmismatched = [0.3, 0.25, 0.15, 0.05, 0, 0, 0, 0, 0, 0, 0]  # Dummy data for mismatched expressions\n\nx = np.arange(len(categories))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\n\n# Plotting data\nrects1 = ax.bar(x - width/2, matched, width, label='in matched expressions', color='blue')\nrects2 = ax.bar(x + width/2, mismatched, width, label='in mismatched expressions', color='orange')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Frequency')\nax.set_title('Perplexity of sentences')\nax.set_xticks(x)\nax.set_xticklabels(categories, rotation=45, ha=\"right\")\nax.legend()\n\n# Save the figure\nplt.savefig('/mnt/data/perplexity_histogram.png', dpi=300)\n",
        "code_cot": "",
        "type": "Histogram",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.01236/perplexity_val_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.04829/cocoline.png",
        "caption": "On COCO, MixReorg's ablation study on the number of progressive mixings and the number of images for the contextual mixing operation. (a) Yellow line: Ablation study on the number $P$ of the progressive mixing modules. We replace one progressive mixing module with one transformer layer to maintain the model size. (b) Red line: Ablation study on the number $M$ of images for each contextual mixing operation.",
        "source": "iccv_1/2308.04829/paper.tex",
        "arxiv_id": "iccv_1/2308.04829",
        "llava_output": "The image is a line chart that presents the results of an ablation study on the number of progressive mixings and the number of images for the contextual mixing operation. The chart shows the number of progressive mixings (P) and the number of images for each contextual mixing operation (M) on the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data for the plot\nx_values = range(0, 34, 2)  # Assuming these are the x-axis intervals given the graph\ny_values_p = [11.5, 13.2, 14.9, 17, 20.1, 20.5]  # Dummy data for the yellow line (Progressive Mixing Modules)\ny_values_m = [18.2, 18.2, 18.2, 18.2, 18.2, 18.2]  # Dummy data for the red line (Images for Contextual Mixing)\n\n# Ensuring the length of x_values matches y_values\nx_values = x_values[:len(y_values_p)]\n\n# Plotting the lines\nplt.figure(figsize=(8, 4))\nplt.plot(x_values, y_values_p, marker='o', color='orange', label='The Number P of Progressive Mixing Modules')\nplt.plot(x_values, y_values_m, marker='s', color='red', label='The Number M of Images for Contextual Mixing')\n\n# Annotating the points\nfor i, txt in enumerate(y_values_p):\n    plt.annotate(f'{txt}', (x_values[i], y_values_p[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\nfor i, txt in enumerate(y_values_m):\n    plt.annotate(f'{txt}', (x_values[i], y_values_m[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n\n# Setting labels and legend\nplt.xlabel('Number of Modules/Images')\nplt.ylabel('mIoU (%)')\nplt.legend()\n\n# Saving the figure\nplt.savefig('/mnt/data/mixreorg_ablation_study.png', dpi=300)\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.04829/cocoline_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.07009/Fig.5.Attack_Comparison_Graph-Active.png",
        "caption": "Attack comparison on different camera poses. Values are AP@0.5 of the car averaged from all models.",
        "source": "iccv_1/2308.07009/main.tex",
        "arxiv_id": "iccv_1/2308.07009",
        "llava_output": "The image is a line chart that shows the attack comparison on different camera poses. The values are AP@0.5 of the car averaged from all models.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Dummy data to replicate the graph's appearance\ndistances = ['[5,7]', '[7,9]', '[9,11]', '[11,13]', '[13,15]', '[15,17]', '[17,19]', '[19,21]']\nangles = ['[-10,0]', '[0,10]', '[10,20]', '[20,30]', '[30,40]', '[40,50]']\npitches = ['[-15,-5]', '[-5,5]', '[5,15]']\n\n# AP values for the different methods across distances, angles, and pitches\nap_normal = np.random.uniform(low=0.6, high=1.0, size=(len(distances) + len(angles) + len(pitches)))\nap_random = np.random.uniform(low=0.5, high=0.9, size=(len(distances) + len(angles) + len(pitches)))\nap_naive = np.random.uniform(low=0.4, high=0.8, size=(len(distances) + len(angles) + len(pitches)))\nap_das = np.random.uniform(low=0.5, high=1.0, size=(len(distances) + len(angles) + len(pitches)))\nap_fca = np.random.uniform(low=0.6, high=1.0, size=(len(distances) + len(angles) + len(pitches)))\nap_camou = np.random.uniform(low=0.4, high=0.9, size=(len(distances) + len(angles) + len(pitches)))\nap_er = np.random.uniform(low=0.5, high=0.9, size=(len(distances) + len(angles) + len(pitches)))\nap_dta = np.random.uniform(low=0.5, high=1.0, size=(len(distances) + len(angles) + len(pitches)))\nap_ours = np.random.uniform(low=0.7, high=1.0, size=(len(distances) + len(angles) + len(pitches)))\n\n# X axis values\nx_values = list(range(len(distances) + len(angles) + len(pitches)))\n\n# Plotting the lines\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(x_values, ap_normal, 'o-', label='Normal')\nax.plot(x_values, ap_random, 'v-', label='Random')\nax.plot(x_values, ap_naive, 's-', label='Naive')\nax.plot(x_values, ap_das, 'p-', label='DAS')\nax.plot(x_values, ap_fca, '*-', label='FCA')\nax.plot(x_values, ap_camou, 'x-', label='CAMOU')\nax.plot(x_values, ap_er, 'd-', label='ER')\nax.plot(x_values, ap_dta, '^-', label='DTA')\nax.plot(x_values, ap_ours, 'h-', label='Ours')\n\n# Setting the x-axis labels\nax.set_xticks(x_values)\nax.set_xticklabels(distances + angles + pitches, rotation=45, ha=\"right\")\n\n# Adding grid, legend, and labels\nax.grid(True)\nax.legend()\nax.set_ylabel('AP@0.5')\nax.set_xlabel('Distances (m) / Angles (\u00b0) / Pitches (\u00b0)')\n\n# Title of the plot\nax.set_title('Attack comparison on different camera poses')\n\n# Saving the figure\nplt.tight_layout()\nplt.savefig('/mnt/data/attack_comparison_camera_poses.png', dpi=300)\n",
        "code_cot": "",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.07009/Fig.5.Attack_Comparison_Graph-Active_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.07009/Fig.7.Different_Class_Comparison-Active.png",
        "caption": "Transferability to different class (Truck and Bus).",
        "source": "iccv_1/2308.07009/main.tex",
        "arxiv_id": "iccv_1/2308.07009",
        "llava_output": "The image is a line chart that shows the transferability of different classes of vehicles, including trucks and buses. The chart has a horizontal axis and a vertical axis, with the horizontal axis representing the different classes of vehicles and the vertical axis representing the transferability. The chart displays the transferability of each class",
        "is_data_visual": "Yes",
        "code": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# This function will be used to load the data, split it and run the models.\ndef run_spam_detection():\n    # Load the dataset\n    df = pd.read_csv('/mnt/data/SPAM.csv')\n    \n    # Split the dataset into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.15, random_state=42)\n    \n    # We will use CountVectorizer to convert text messages into a matrix of token counts\n    from sklearn.feature_extraction.text import CountVectorizer\n    vect = CountVectorizer()\n    X_train_dtm = vect.fit_transform(X_train)\n    X_test_dtm = vect.transform(X_test)\n    \n    # Train and evaluate Logistic Regression\n    lr = LogisticRegression(max_iter=1000)\n    lr.fit(X_train_dtm, y_train)\n    y_pred_lr = lr.predict(X_test_dtm)\n    lr_acc = accuracy_score(y_test, y_pred_lr)\n    \n    # Train and evaluate Random Forest\n    rf = RandomForestClassifier(random_state=42)\n    rf.fit(X_train_dtm, y_train)\n    y_pred_rf = rf.predict(X_test_dtm)\n    rf_acc = accuracy_score(y_test, y_pred_rf)\n    \n    # Print the evaluation results\n    print(\"Logistic Regression Accuracy: \", lr_acc)\n    print(\"Random Forest Accuracy: \", rf_acc)\n    print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n    print(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n\n    return lr_acc, rf_acc\n\n# Call the function to run the spam detection\nrun_spam_detection()\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.07009/Fig.7.Different_Class_Comparison-Active_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "No",
            "text_elements_rating": 5,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 5,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.07009/Fig.17.smooth_camouflage_performance.png",
        "caption": "Performance graph for various smooth loss $\\beta$ and camouflage loss $\\gamma$.",
        "source": "iccv_1/2308.07009/main.tex",
        "arxiv_id": "iccv_1/2308.07009",
        "llava_output": "The image is a line chart showing the average precision for different smooth loss $\\beta$ and camouflage loss $\\gamma$ values.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data to mimic the plots in the provided image\nbeta_values = ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0']\ngamma_values = ['0.1', '0.2', '0.3', '0.4', '0.5']\naverage_precision_beta = np.random.rand(10) * 0.35  # Random data simulating average precision for beta\naverage_precision_gamma = np.random.rand(5) * 0.35  # Random data simulating average precision for gamma\naverage_precision = np.concatenate((average_precision_beta, average_precision_gamma, average_precision_gamma))\n\n# Creating figure and axes\nfig, axs = plt.subplots(3, 1, figsize=(8, 6))\n\n# First plot\naxs[0].bar(beta_values + gamma_values, average_precision, color=['blue', 'orange', 'green', 'red', 'purple']*3)\naxs[0].set_title('Average Precision @0.5 of Textures with Different Smooth \u03b2 and Camouflage \u03b3')\naxs[0].set_ylabel('Average Precision @0.5')\n\n# Second plot\naxs[1].bar(beta_values, average_precision_beta, color=['blue', 'orange', 'green', 'red', 'purple']*2)\naxs[1].set_title('Average Precision @0.5 Grouped by Smooth \u03b2')\naxs[1].set_ylabel('Average Precision @0.5')\n\n# Third plot\naxs[2].bar(gamma_values, average_precision_gamma, color=['grey', 'silver', 'darkgrey', 'black', 'lightgrey'])\naxs[2].set_title('Average Precision @0.5 Grouped by Camouflage \u03b3')\naxs[2].set_ylabel('Average Precision @0.5')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/performance_graph.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.07009/Fig.17.smooth_camouflage_performance_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.08428/histogram_similarity.jpg",
        "caption": "We conduct a statistical analysis of raw text and synthetic caption on YFCC15M. (a) is the image-text/caption similarity distribution; (b) is the token number distribution of the raw texts and synthetic captions.",
        "source": "iccv_1/2308.08428/iccv23_ALIP.tex",
        "arxiv_id": "iccv_1/2308.08428",
        "llava_output": "The image is a histogram showing the similarity distribution of raw text and synthetic captions.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data to mimic the plots in the provided image\nbeta_values = ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.0']\ngamma_values = ['0.1', '0.2', '0.3', '0.4', '0.5']\naverage_precision_beta = np.random.rand(10) * 0.35  # Random data simulating average precision for beta\naverage_precision_gamma = np.random.rand(5) * 0.35  # Random data simulating average precision for gamma\naverage_precision = np.concatenate((average_precision_beta, average_precision_gamma, average_precision_gamma))\n\n# Creating figure and axes\nfig, axs = plt.subplots(3, 1, figsize=(8, 6))\n\n# First plot\naxs[0].bar(beta_values + gamma_values, average_precision, color=['blue', 'orange', 'green', 'red', 'purple']*3)\naxs[0].set_title('Average Precision @0.5 of Textures with Different Smooth \u03b2 and Camouflage \u03b3')\naxs[0].set_ylabel('Average Precision @0.5')\n\n# Second plot\naxs[1].bar(beta_values, average_precision_beta, color=['blue', 'orange', 'green', 'red', 'purple']*2)\naxs[1].set_title('Average Precision @0.5 Grouped by Smooth \u03b2')\naxs[1].set_ylabel('Average Precision @0.5')\n\n# Third plot\naxs[2].bar(gamma_values, average_precision_gamma, color=['grey', 'silver', 'darkgrey', 'black', 'lightgrey'])\naxs[2].set_title('Average Precision @0.5 Grouped by Camouflage \u03b3')\naxs[2].set_ylabel('Average Precision @0.5')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/performance_graph.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "",
        "type": "Histogram",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.08428/histogram_similarity_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.09534/avg_size_dis.png",
        "caption": "Size distribution of instances in (a) SODA-D and (b) SODA-A, where the absolute size corresponds to the square root of the object area.",
        "source": "iccv_1/2308.09534/0817.tex",
        "arxiv_id": "iccv_1/2308.09534",
        "llava_output": "The image is a histogram showing the size distribution of instances in (a) SODA-D and (b) SODA-A. The absolute size corresponds to the square root of the object area.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the data for the histograms\n# These are placeholders and should be replaced with actual values\nsoda_d_sizes = np.random.normal(loc=20, scale=5, size=50000)\nsoda_a_sizes = np.random.normal(loc=15, scale=10, size=50000)\n\nfig, axs = plt.subplots(1, 2, figsize=(6.39, 3.47))  # Size adjusted to match the aspect ratio\n\n# SODA-D histogram\naxs[0].hist(soda_d_sizes, bins=30, color='teal', edgecolor='black')\naxs[0].set_title('(a) SODA-D')\naxs[0].set_xlabel('Absolute Size')\naxs[0].set_ylabel('#Instances')\naxs[0].set_yscale('log')\n\n# SODA-A histogram\naxs[1].hist(soda_a_sizes, bins=30, color='teal', edgecolor='black')\naxs[1].set_title('(b) SODA-A')\naxs[1].set_xlabel('Absolute Size')\naxs[1].set_yscale('log')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 dpi\nplt.savefig('/mnt/data/soda_size_distribution.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "",
        "type": "Histogram",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.09534/avg_size_dis_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2308.10447/freq_and_instance_num.png",
        "caption": "Instance frequency distribution (a) and distribution of the number of instances in a scene (b).",
        "source": "iccv_1/2308.10447/dataset.tex",
        "arxiv_id": "iccv_1/2308.10447",
        "llava_output": "The image presents a pie chart and a bar chart, both displaying information about the frequency of instances in a scene. The pie chart is labeled \"a) Instance frequency distribution\" and shows the distribution of instances in a scene. The bar chart, labeled \"b) Instance counts in a scene,\"",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Assuming the data for the pie chart and the bar chart\n# These are placeholders and should be replaced with actual data\n# Pie chart data\nlabels = ['1', '2', '3', '4', '5', '>5']\nsizes = [50, 20, 15, 10, 3, 2]\ncolors = ['blue', 'green', 'red', 'orange', 'purple', 'yellow']\n\n# Bar chart data\nx = [3, 4, 5, 6, 7]\ny = [1500, 3000, 5000, 2000, 1000]\nbar_colors = ['yellow', 'green', 'blue', 'orange', 'red']\n\nfig, axs = plt.subplots(1, 2, figsize=(6.53, 2.72))\n\n# Pie chart\naxs[0].pie(sizes, labels=labels, colors=colors, startangle=90, autopct='%1.1f%%')\naxs[0].set_title('(a) instance frequency')\n\n# Bar chart\naxs[1].bar(x, y, color=bar_colors)\naxs[1].set_title('(b) instance counts in a scene')\naxs[1].set_xlabel('Scene')\naxs[1].set_ylabel('Scene')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 dpi\nplt.savefig('/mnt/data/instance_distribution.png', dpi=300)\n\nplt.show()\n",
        "code_cot": "",
        "type": "Others",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2308.10447/freq_and_instance_num_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.00796/user-study.png",
        "caption": "User perceptual study. The motion generated by our method is the one that best matches the textual description from the user's perspective.",
        "source": "iccv_1/2309.00796/main.tex",
        "arxiv_id": "iccv_1/2309.00796",
        "llava_output": "The image is a bar chart that presents the results of a user perceptual study. The chart compares the motion generated by different methods and shows which method best matches the user's perception. The chart has five bars, each representing a different method, and the results are displayed in terms of user ratings.",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\nmodels = ['Std', 'T2M-GPT', 'Motiondiffuse', 'Ours w/o BPS', 'Ours w/o GA', 'Ours w/o LA', 'Ours']\nuser_ratings = [4.09, 4.49, 4.26, 3.18, 3.53, 0.43, 4.88]\nerror = [1.06, 1.07, 0.90, 0.88, 0.91, 0.43, 0.43]\n\nx = np.arange(len(models))  # the label locations\n\n# Plot setup\nfig, ax = plt.subplots()\nrects1 = ax.bar(x, user_ratings, yerr=error, capsize=5, color=['red', 'blue', 'orange', 'green', 'teal', 'darkblue', 'cyan'])\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('User rating')\nax.set_title('User perceptual study')\nax.set_xticks(x)\nax.set_xticklabels(models)\nax.legend()\n\n# Attach a text label above each bar in *rects*, displaying its height.\nfor rect in rects1:\n    height = rect.get_height()\n    ax.annotate('{}'.format(height),\n                xy=(rect.get_x() + rect.get_width() / 2, height),\n                xytext=(0, 3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/user_perceptual_study.png', dpi=300)\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.00796/user-study_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.02120/dataset_v2.png",
        "caption": "Distribution of the 20 classes in the easy-EPIC Aff dataset, showing a significant class imbalance.",
        "source": "iccv_1/2309.02120/egpaper_final.tex",
        "arxiv_id": "iccv_1/2309.02120",
        "llava_output": "1. Bar Chart</s>",
        "is_data_visual": "Yes",
        "code": "# Data for plotting\nclasses = ['open', 'put', 'close', 'empty', 'wear', 'turn-on', 'turn-off', 'cut', 'stir', 'pour', 'remove', 'mix', 'turn', 'peel', 'shake', 'throw', 'spray', 'dry']\nfrequencies = [14.0, 13.7, 9.5, 9.0, 7.3, 5.8, 5.6, 3.7, 2.9, 2.4, 2.2, 2.1, 2.0, 1.1, 0.8, 0.7, 0.7, 0.4, 0.3]\n\nx = np.arange(len(classes))  # the label locations\n\n# Plot setup\nfig, ax = plt.subplots()\nbars = ax.bar(x, frequencies, color=plt.cm.tab20.colors)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('% Frequency')\nax.set_title('Distribution of the 20 classes in the easy-EPIC Aff dataset')\nax.set_xticks(x)\nax.set_xticklabels(classes, rotation=90)  # Rotate labels to fit\nax.set_ylim(0, 16)  # Extend y-axis to fit the text\n\n# Attach a text label above each bar displaying its height.\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate('{:.1f}'.format(height),\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/class_distribution_easy_epic_aff.png', dpi=300)\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.02120/dataset_v2_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.05148/cfd_skin_color.png",
        "caption": "Skin color representation",
        "source": "iccv_1/2309.05148/supp.tex",
        "arxiv_id": "iccv_1/2309.05148",
        "llava_output": "The image is a scatter plot showing the distribution of skin colors.</s>",
        "is_data_visual": "Yes",
        "code": "# Data for plotting\nclasses = ['open', 'put', 'close', 'empty', 'wear', 'turn-on', 'turn-off', 'cut', 'stir', 'pour', 'remove', 'mix', 'turn', 'peel', 'shake', 'throw', 'spray', 'dry']\nfrequencies = [14.0, 13.7, 9.5, 9.0, 7.3, 5.8, 5.6, 3.7, 2.9, 2.4, 2.2, 2.1, 2.0, 1.1, 0.8, 0.7, 0.7, 0.4, 0.3]\n\nx = np.arange(len(classes))  # the label locations\n\n# Plot setup\nfig, ax = plt.subplots()\nbars = ax.bar(x, frequencies, color=plt.cm.tab20.colors)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('% Frequency')\nax.set_title('Distribution of the 20 classes in the easy-EPIC Aff dataset')\nax.set_xticks(x)\nax.set_xticklabels(classes, rotation=90)  # Rotate labels to fit\nax.set_ylim(0, 16)  # Extend y-axis to fit the text\n\n# Attach a text label above each bar displaying its height.\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate('{:.1f}'.format(height),\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('/mnt/data/class_distribution_easy_epic_aff.png', dpi=300)\n",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.05148/cfd_skin_color_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "No",
            "text_elements_rating": 5,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 5,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.07499/teaser_latest.png",
        "caption": "ImageNet-C accuracy v/s training time comparison. Our method is on the pareto-front (achieves better robust accuracy in much lesser time) compared to the state-of-the-art methods Augmix based Complete fine-tuning and WISE-complete fine-tuning. The data points labelled with suffix \"C\" correspond to CLIP models.",
        "source": "iccv_1/2309.07499/latest_arxiv.tex",
        "arxiv_id": "iccv_1/2309.07499",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the graph\nmethods = ['AugMix-FT', 'WISE-FT', 'Ours']\ntimes = [1, 2, 3, 4, 5]\naccuracies = [54, 56, 58, 60, 62, 64, 66]\n\n# AugMix-FT data\naugmix_ft_time = [1, 2, 3, 5]\naugmix_ft_accuracy = [56, 62, 64, 66]\naugmix_ft_labels = ['RN-50', 'RN-151', 'VIT-B', 'VIT-L14C']\n\n# WISE-FT data\nwise_ft_time = [1, 3, 4]\nwise_ft_accuracy = [58, 62, 64]\nwise_ft_labels = ['RN-50C', 'VIT-BC', 'VIT-L14C']\n\n# Ours data\nours_time = [1, 2, 3]\nours_accuracy = [60, 64, 66]\nours_labels = ['RN-50', 'RN-151', 'VIT-L14C']\n\n# Plotting the data\nfig, ax = plt.subplots()\n\n# AugMix-FT plot\nax.scatter(augmix_ft_time, augmix_ft_accuracy, color='red', label=methods[0], marker='D')\nfor i, txt in enumerate(augmix_ft_labels):\n    ax.annotate(txt, (augmix_ft_time[i], augmix_ft_accuracy[i]))\n\n# WISE-FT plot\nax.scatter(wise_ft_time, wise_ft_accuracy, color='blue', label=methods[1], marker='o')\nfor i, txt in enumerate(wise_ft_labels):\n    ax.annotate(txt, (wise_ft_time[i], wise_ft_accuracy[i]))\n\n# Ours plot\nax.scatter(ours_time, ours_accuracy, color='purple', label=methods[2], marker='*')\nfor i, txt in enumerate(ours_labels):\n    ax.annotate(txt, (ours_time[i], ours_accuracy[i]))\n\n# Labels & Legend\nax.set_xlabel('Fine-tuning time (relative)')\nax.set_ylabel('Accuracy')\nax.set_title('ImageNet-C accuracy v/s training time comparison')\nax.legend()\n\n# Save the figure\nplt.savefig('/mnt/data/imagenet_c_accuracy_comparison.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.07499/teaser_latest_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.07499/supp_fraction.png",
        "caption": "Ablation on amount of distillation data v/s accuracy. The x-axis shows the fraction of augmented data used in distillation and the y-axis shows the accuracy achieved for each fraction by the APT baseline and our method on the ImageNet-C dataset. Here, ResNet-101 is used as the student network, updated using a ResNet-34 teacher, both being single modal.",
        "source": "iccv_1/2309.07499/latest_arxiv.tex",
        "arxiv_id": "iccv_1/2309.07499",
        "llava_output": "The image is a line chart that shows the ablation on the amount of distillation data v/s accuracy. The x-axis represents the fraction of augmented data used in distillation, and the y-axis shows the accuracy achieved for each fraction by the APT baseline and the proposed method on",
        "is_data_visual": "Yes",
        "code": "import numpy as np\n\n# Data from the graph\nfractions = [0.2, 0.4, 0.6, 0.8, 1.0]\nbaseline_accuracy = [48, 50, 52, 54, 56]\nours_accuracy = [49, 53, 55, 57, 58]\n\nbar_width = 0.35  # Width of the bars\nindex = np.arange(len(fractions))\n\n# Plotting the data\nfig, ax = plt.subplots()\n\n# Baseline bars\nbaseline_bars = ax.bar(index, baseline_accuracy, bar_width, label='Baseline', color='blue', hatch='//')\n\n# Our method bars\nours_bars = ax.bar(index + bar_width, ours_accuracy, bar_width, label='Ours', color='magenta', hatch='..')\n\n# Labels & Legend\nax.set_xlabel('Fraction of Data')\nax.set_ylabel('Accuracy')\nax.set_title('Ablation on amount of distillation data v/s accuracy')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(fractions)\nax.legend()\n\n# Save the figure\nplt.savefig('/mnt/data/ablation_distillation_data_accuracy.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.07499/supp_fraction_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.08690/fundamental_rotation_iters_fixed_3.jpg",
        "caption": "\\it Results for a fixed number of iterations, \\ie, without stopping criterion. We vary the number of iterations between 1000 and 10000.",
        "source": "iccv_1/2309.08690/results.tex",
        "arxiv_id": "iccv_1/2309.08690",
        "llava_output": "The image is a line chart that shows the results of an experiment. The chart has different colored lines, each representing a different number of iterations. The number of iterations ranges from 1000 to 10000, and the chart shows how the results change with the increase in the number of",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the graph\niterations = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\nmAA_blue = [0.81, 0.82, 0.825, 0.83, 0.833, 0.835, 0.836, 0.837, 0.838, 0.839]\nmAA_gray = [0.82, 0.83, 0.832, 0.834, 0.835, 0.836, 0.837, 0.838, 0.839, 0.84]\nmAA_green = [0.83, 0.84, 0.842, 0.844, 0.845, 0.846, 0.847, 0.848, 0.849, 0.85]\nmAA_orange = [0.84, 0.85, 0.852, 0.854, 0.855, 0.856, 0.857, 0.858, 0.859, 0.86]\n\n# Plotting the data\nfig, ax = plt.subplots()\n\nax.plot(iterations, mAA_blue, 'o-', color='blue')\nax.plot(iterations, mAA_gray, 'o-', color='gray')\nax.plot(iterations, mAA_green, 'o-', color='green')\nax.plot(iterations, mAA_orange, 'o-', color='orange')\n\n# Styling the plot to match the original as closely as possible\nax.set_facecolor('lightgrey')\nax.grid(True)\nax.set_xlim(1000, 10000)\nax.set_ylim(0.81, 0.85)\nax.set_xticks(iterations)\nax.set_yticks([0.81, 0.82, 0.83, 0.84, 0.85])\nax.set_xlabel('Number of iterations')\nax.set_ylabel('Rotation mAA (10\u00b0)')\nax.set_title('Results for a fixed number of iterations, i.e., without stopping criterion')\n\n# Save the figure\nplt.savefig('/mnt/data/rotation_mAA_iterations.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.08690/fundamental_rotation_iters_fixed_3_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "iccv_figures_1/2309.08690/sintetic_data_experiments_quadratic.jpg",
        "caption": "\\it Experimental results for the curve (left) and circle (right) fitting. We compare RANSAC, BaySAC, and BANSAC based on the number of iterations and RMS error for different inlier rates.",
        "source": "iccv_1/2309.08690/supp_mat.tex",
        "arxiv_id": "iccv_1/2309.08690",
        "llava_output": "The image is a line chart showing the number of iterations and RMS error for different inlier rates for RANSAC, BaySAC, and BANSAC. The chart is divided into three sections, one for each algorithm, and each section has a different color. The chart is labeled with",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from the graph\ninlier_rates = [10, 15, 20, 25, 30, 35, 40, 45, 50]\nransac_iterations = [2500, 2000, 1500, 1000, 800, 600, 400, 200, 100]\nbaysac_iterations = [2200, 1800, 1400, 900, 700, 500, 300, 150, 75]\nbansac_iterations = [3000, 2500, 2000, 1500, 1200, 900, 600, 300, 150]\nransac_rmse = [0.014, 0.013, 0.012, 0.011, 0.01, 0.009, 0.008, 0.007, 0.006]\nbaysac_rmse = [0.013, 0.012, 0.011, 0.01, 0.009, 0.008, 0.007, 0.006, 0.005]\nbansac_rmse = [0.012, 0.011, 0.01, 0.009, 0.008, 0.007, 0.006, 0.005, 0.004]\n\nfig, ax1 = plt.subplots()\n\n# First axis for the number of iterations\nax1.set_xlabel('Inlier rate [%]')\nax1.set_ylabel('Number of iterations', color='black')\nax1.bar(np.array(inlier_rates) - 1.5, ransac_iterations, width=1, label='RANSAC', color='blue')\nax1.bar(np.array(inlier_rates) - 0.5, baysac_iterations, width=1, label='BaySAC', color='green')\nax1.bar(np.array(inlier_rates) + 0.5, bansac_iterations, width=1, label='BANSAC', color='red')\nax1.tick_params(axis='y', labelcolor='black')\nax1.set_title('Curve fitting')\n\n# Second axis for RMSE\nax2 = ax1.twinx()\nax2.set_ylabel('RMSE', color='black') \nax2.plot(inlier_rates, ransac_rmse, color='blue', marker='o')\nax2.plot(inlier_rates, baysac_rmse, color='green', marker='o')\nax2.plot(inlier_rates, bansac_rmse, color='red', marker='o')\nax2.tick_params(axis='y', labelcolor='black')\n\nfig.tight_layout()  # To ensure the right y-label is not slightly clipped\nax1.legend()\n\n# Save the figure\nplt.savefig('/mnt/data/curve_fitting.png', dpi=300)\n\n# Display the plot\nplt.show()\n",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/iccv_1/2309.08690/sintetic_data_experiments_quadratic_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 5,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/1810.10801/chip_calibration_fig.png",
        "caption": "Firing rate of spiking neurons on the ROLLS chip when each neuron is stimulated with 200Hz, ordered.",
        "source": "icra/1810.10801/root.tex",
        "arxiv_id": "icra/1810.10801",
        "llava_output": "The image is a line chart that shows the firing rate of spiking neurons on the ROLLS chip when each neuron is stimulated with 200Hz, ordered.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# This is placeholder data. Replace it with your actual data points.\nneuron_firing_rates = np.random.rand(100) * 100\nneuron_indices = np.arange(len(neuron_firing_rates))\nmean_firing_rate = np.mean(neuron_firing_rates)\n\n# Creating the bar plot\nplt.figure(figsize=(20,5)) # You can adjust the size to match the aspect ratio of your plot\nplt.bar(neuron_indices, neuron_firing_rates, width=1.0, color=plt.cm.tab20c.colors)\n\n# Adding the mean line and text\nplt.axhline(mean_firing_rate, color='blue', linewidth=2)\nplt.text(x=np.max(neuron_indices)*0.5, y=mean_firing_rate, s=f'mean = {mean_firing_rate:.1f}', \n         color='blue', va='bottom', ha='center')\n\n# Setting the axis labels\nplt.xlabel('Neuron index')\nplt.ylabel('Output firing rate, Hz')\n\n# Optionally set x and y ranges to match the plot in the image\nplt.ylim(0, max(neuron_firing_rates) + 10)\nplt.xlim(0, len(neuron_indices) - 1)\n\n# Saving the figure with 300 DPI\nplt.savefig('neuron_firing_rates.png', dpi=300)\n\n# Show plot for visualization\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/1810.10801/chip_calibration_fig_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/1810.10801/Learning_allCMD_fig.jpg",
        "caption": "Time plots of activity on the ROLLS chip during testing of the learned feedforward controller for all five commands.",
        "source": "icra/1810.10801/root.tex",
        "arxiv_id": "icra/1810.10801",
        "llava_output": "The image is a line chart showing the results of the experiment.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming dummy data since actual data is not provided\ntime = np.linspace(0, 100000, 1000)\nimu_signal = np.sin(time / 8000) * 1500 + np.random.normal(0, 200, size=time.shape)\nfiltered_imu_signal = np.sin(time / 8000) * 1500\n\ndelta_plus = np.random.randint(0, 5, size=time.shape)\ndelta_minus = np.random.randint(0, 5, size=time.shape)\nlearning_command_neurons = np.random.randint(0, 2, size=time.shape)\nmotor_space_neurons = np.random.randint(0, 5, size=time.shape)\nmotor_rate_neurons = np.sin(time / 8000) * 30 + 10\n\n# You would replace these background regions with the actual command periods\nlearn_cmd_regions = [(10000, 20000), (40000, 50000), (70000, 80000)]\ntest_cmd_regions = [(20000, 40000), (50000, 70000), (80000, 100000)]\n\nfig, axs = plt.subplots(5, 1, figsize=(10, 15), sharex=True)\n\n# Plotting dummy IMU signals\naxs[0].plot(time, imu_signal, label='IMU-signal')\naxs[0].plot(time, filtered_imu_signal, label='Filtered IMU-signal')\naxs[0].legend()\n\n# Plotting dummy delta neurons\naxs[1].scatter(time, delta_plus, color='blue', label='Delta+ neurons', s=1)\naxs[1].scatter(time, delta_minus, color='orange', label='Delta- neurons', s=1)\naxs[1].legend()\n\n# Plotting dummy learning command neurons\naxs[2].scatter(time, learning_command_neurons, color='blue', s=1)\n\n# Plotting dummy motor space neurons\naxs[3].scatter(time, motor_space_neurons, color='blue', s=1)\n\n# Plotting dummy motor rate neurons\naxs[4].plot(time, motor_rate_neurons, color='red')\naxs[4].set_xlabel('time in ms')\n\nfor ax in axs:\n    # Indicate learn cmd regions\n    for start, end in learn_cmd_regions:\n        ax.axvspan(start, end, color='orange', alpha=0.3)\n\n    # Indicate test cmd regions\n    for start, end in test_cmd_regions:\n        ax.axvspan(start, end, color='blue', alpha=0.3)\n\n# Adjust spacing\nplt.tight_layout()\n\n# Save plot as a png file\nplt.savefig('activity_plot.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/1810.10801/Learning_allCMD_fig_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2002.03197/all_train_loss.png",
        "caption": "Training (a) and validation (b) losses of networks in RNN pretrain (epochs 1-50) and in DeltaRNN retrain (c,d) (epochs 51-60) with $M=32,64,128,256$ and $T=100$ time steps",
        "source": "icra/2002.03197/root.tex",
        "arxiv_id": "icra/2002.03197",
        "llava_output": "The image is a bar chart that presents the training and validation losses of networks in RNN pretrain and DeltaRNN retrain. The chart shows the evolution of the losses over time, with different colors representing the different training and validation stages. The chart is divided into two sections, one for RNN pretrain",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulate some data\nepochs_pre = np.arange(1, 51)\nepochs_re = np.arange(51, 61)\nloss_pre_32 = np.exp(-epochs_pre / 20) + np.random.normal(0, 0.01, size=epochs_pre.shape)\nloss_pre_64 = np.exp(-epochs_pre / 25) + np.random.normal(0, 0.01, size=epochs_pre.shape)\nloss_pre_128 = np.exp(-epochs_pre / 30) + np.random.normal(0, 0.01, size=epochs_pre.shape)\nloss_pre_256 = np.exp(-epochs_pre / 35) + np.random.normal(0, 0.01, size=epochs_pre.shape)\nloss_re_32 = np.exp(-epochs_re / 20) + np.random.normal(0, 0.02, size=epochs_re.shape)\nloss_re_64 = np.exp(-epochs_re / 25) + np.random.normal(0, 0.02, size=epochs_re.shape)\nloss_re_128 = np.exp(-epochs_re / 30) + np.random.normal(0, 0.02, size=epochs_re.shape)\nloss_re_256 = np.exp(-epochs_re / 35) + np.random.normal(0, 0.02, size=epochs_re.shape)\n\n# Create subplots\nfig, axs = plt.subplots(1, 4, figsize=(18, 5), sharey=True)\n\n# Plot data for pre training and validation\naxs[0].plot(epochs_pre, loss_pre_32, 'k-o', label='2L-32H-GRU')\naxs[0].plot(epochs_pre, loss_pre_64, 'r-o', label='2L-64H-GRU')\naxs[0].plot(epochs_pre, loss_pre_128, 'g-o', label='2L-128H-GRU')\naxs[0].plot(epochs_pre, loss_pre_256, 'b-o', label='2L-256H-GRU')\naxs[0].set_title('(a) Pre. Training')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Loss')\n\naxs[1].plot(epochs_pre, loss_pre_32, 'k-o')\naxs[1].plot(epochs_pre, loss_pre_64, 'r-o')\naxs[1].plot(epochs_pre, loss_pre_128, 'g-o')\naxs[1].plot(epochs_pre, loss_pre_256, 'b-o')\naxs[1].set_title('(b) Pre. Validation')\naxs[1].set_xlabel('Epoch')\n\n# Plot data for re training and validation\naxs[2].plot(epochs_re, loss_re_32, 'k-o', label='2L-32H-DeltaGRU')\naxs[2].plot(epochs_re, loss_re_64, 'r-o', label='2L-64H-DeltaGRU')\naxs[2].plot(epochs_re, loss_re_128, 'g-o', label='2L-128H-DeltaGRU')\naxs[2].plot(epochs_re, loss_re_256, 'b-o', label='2L-256H-DeltaGRU')\naxs[2].set_title('(c) Re. Training')\naxs[2].set_xlabel('Epoch')\n\naxs[3].plot(epochs_re, loss_re_32, 'k-o')\naxs[3].plot(epochs_re, loss_re_64, 'r-o')\naxs[3].plot(epochs_re, loss_re_128, 'g-o')\naxs[3].plot(epochs_re, loss_re_256, 'b-o')\naxs[3].set_title('(d) Re. Validation')\naxs[3].set_xlabel('Epoch')\n\n# Add legend\nhandles, labels = axs[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper right')\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0, 1, 0.95])\n\n# Save as PNG file with 300dpi\nplt.savefig('loss_plot.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2002.03197/all_train_loss_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2002.09425/MC_sim_h_time2.png",
        "caption": "Altitude time series of a set of Monte-Carlo simulation including 200 flights initialized from random attitude and angular velocities with different flight control methods. a.) The benchmark method. b.) The proposed method but using P1 allocation. c.) The proposed method.",
        "source": "icra/2002.09425/main.tex",
        "arxiv_id": "icra/2002.09425",
        "llava_output": "The image is a line chart showing the altitude time series of a set of Monte-Carlo simulation including 200 flights initialized from random attitude and angular velocities with different flight control methods.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# The actual data would be replaced here with your dataset\n# I'm using random data for demonstration purposes.\ntime = np.linspace(0, 10, 500)  # Assuming the time goes from 0 to 10 seconds\nnp.random.seed(0)  # For reproducibility\n\n# Generate sample data for 200 flights\ndata_a = np.exp(-time[:, None] * (1 + 0.5 * np.random.rand(1, 200))) * 60\ndata_b = np.exp(-time[:, None] * (1 + 0.2 * np.random.rand(1, 200))) * 60\ndata_c = np.exp(-time[:, None] * (1 + 0.1 * np.random.rand(1, 200))) * 60\n\n# Creating the plot with three subplots\nfig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n\n# Plotting the data for each flight method\nfor i in range(200):\n    axs[0].plot(time, data_a[:, i], color='gray', linewidth=0.5)\n    axs[1].plot(time, data_b[:, i], color='gray', linewidth=0.5)\n    axs[2].plot(time, data_c[:, i], color='gray', linewidth=0.5)\n\n# Fixing the labels and titles\naxs[0].set_title('a.) The benchmark method')\naxs[1].set_title('b.) The proposed method but using P1 allocation')\naxs[2].set_title('c.) The proposed method')\n\n# Labeling the axes\nfor ax in axs:\n    ax.set_ylabel('height [m]')\n    ax.set_xlabel('t [s]')\n\n# Saving the plot to a PNG file\nplt.tight_layout()\nplt.savefig('altitude_time_series.png', dpi=300)\n\n# Show the plot (optional, depending on whether you want to see it in a window or not)\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2002.09425/MC_sim_h_time2_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2002.09425/MC_sim_initial_conditions.png",
        "caption": "Scatter plot of the initial conditions of the Monte-Carlo simulation with colors showing the maximum height drop. The crashed flights are shown in red cross markers. a.) The benchmark method. b.) The proposed method but using P1 allocation. c.) The proposed method.",
        "source": "icra/2002.09425/main.tex",
        "arxiv_id": "icra/2002.09425",
        "llava_output": "The image is a scatter plot, showing the initial conditions of the Monte-Carlo simulation with colors representing the maximum height drop. The crashed flights are marked with red crosses.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# This is mocked data for illustration purposes\n# Replace this with your actual data\nn_points = 100\nomega_x_1 = np.random.uniform(-10, 10, n_points)\nomega_z_1 = np.random.uniform(-10, 10, n_points)\nnz_1 = np.random.uniform(-1, 1, n_points)\nheight_drop_1 = np.random.uniform(0, 30, n_points)\ncrash_1 = np.random.choice([True, False], size=n_points, p=[0.3, 0.7])\n\nomega_x_2 = np.random.uniform(-10, 10, n_points)\nomega_z_2 = np.random.uniform(-10, 10, n_points)\nnz_2 = np.random.uniform(-1, 1, n_points)\nheight_drop_2 = np.random.uniform(0, 30, n_points)\ncrash_2 = np.random.choice([True, False], size=n_points, p=[0.3, 0.7])\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15)) # Specify the number of rows and columns of plots\n\nfor row, (omega_x, omega_z, nz, height_drop, crash) in enumerate(zip([omega_x_1, omega_x_2], [omega_z_1, omega_z_2], [nz_1, nz_2], [height_drop_1, height_drop_2], [crash_1, crash_2])):\n    sc = axes[row, 0].scatter(omega_x[~crash], omega_z[~crash], c=height_drop[~crash], cmap='viridis', label='recovered')\n    cr = axes[row, 0].scatter(omega_x[crash], omega_z[crash], c='r', marker='x', label='crashed')\n    axes[row, 0].set_xlabel('\u03c9x [rad/s]')\n    axes[row, 0].set_ylabel('\u03c9y [rad/s]')\n    axes[row, 0].set_title(f'a{row+1}.)')\n\n    sc = axes[row, 1].scatter(nz[~crash], omega_z[~crash], c=height_drop[~crash], cmap='viridis')\n    cr = axes[row, 1].scatter(nz[crash], omega_z[crash], c='r', marker='x')\n    axes[row, 1].set_xlabel('nz')\n    axes[row, 1].set_ylabel('\u03c9z [rad/s]')\n    axes[row, 1].set_title(f'b{row+1}.)')\n\n# Last row showing the proposed method without the red cross markers\nsc = axes[2, 0].scatter(omega_x_2, omega_z_2, c=height_drop_2, cmap='viridis', label='recovered')\naxes[2, 0].set_xlabel('\u03c9x [rad/s]')\naxes[2, 0].set_ylabel('\u03c9y [rad/s]')\naxes[2, 0].set_title(f'c1.)')\n\nsc = axes[2, 1].scatter(nz_2, omega_z_2, c=height_drop_2, cmap='viridis')\naxes[2, 1].set_xlabel('nz')\naxes[2, 1].set_ylabel('\u03c9z [rad/s]')\naxes[2, 1].set_title(f'c2.)')\n\n# Color bar\ncbar = fig.colorbar(sc, ax=axes.ravel().tolist(), orientation='horizontal', pad=0.02)\ncbar.set_label('height drop')\n\n# Legend\naxes[0, 0].legend()\n\nplt.tight_layout()\nplt.savefig('monte_carlo_scatter.png', dpi=300)\nplt.show()",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2002.09425/MC_sim_initial_conditions_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2009.08876/ablation.png",
        "caption": "Ablation studies: the test results of individual gating network after 2.1 step of training when (a) The Main Gating Network is trained end-to-end together with the experts. (b) The gating mechanism is trained without the sparsity loss.",
        "source": "icra/2009.08876/IEEEexample.tex",
        "arxiv_id": "icra/2009.08876",
        "llava_output": "The image is a line chart showing the test results of individual gating network after 2.1 step of training when (a) The Main Gating Network is trained end-to-end together with the experts and (b) The gating mechanism is trained without the sparsity loss.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have your data in arrays similar to these placeholders\nframe_count = 250\nframes = np.arange(frame_count)\ndata_placeholder = {\n    'left_cam': np.random.rand(frame_count),\n    'center_cam': np.random.rand(frame_count),\n    'right_cam': np.random.rand(frame_count),\n    'lidar': np.random.rand(frame_count),\n    'ground_truth': np.random.randn(frame_count),\n    'predicted': np.random.randn(frame_count)\n}\n\n# Create a figure with 6 rows and 2 columns of subplots\nfig, axs = plt.subplots(7, 2, figsize=(15, 15), sharex=True)\n\n# Plot the placeholder data in its respective subplot\nfor i in range(6):\n    for j in range(2):\n        if i < 5:\n            axs[i, j].plot(frames, data_placeholder['left_cam'], label='left cam')\n            axs[i, j].plot(frames, data_placeholder['center_cam'], label='center cam')\n            axs[i, j].plot(frames, data_placeholder['right_cam'], label='right cam')\n            axs[i, j].plot(frames, data_placeholder['lidar'], label='lidar', linestyle='--')\n            axs[i, j].set_ylim(0, 1) # adjust the y-axis limits to match the plot's range\n        else:\n            axs[i, j].plot(frames, data_placeholder['ground_truth'], label='ground truth')\n            axs[i, j].plot(frames, data_placeholder['predicted'], label='predicted', linestyle='--')\n        # axs[i, j].legend() # Uncomment and customize this if you want to display a legend\n\n# Set common labels\nfor ax in axs.flat:\n    ax.set(xlabel='frames', ylabel='weights')\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()\n\n# Adjust spacing between plots\nplt.subplots_adjust(hspace=0.3)\n\n# Save the figure\nplt.savefig('my_figure.png', dpi=300)",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2009.08876/ablation_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2011.00756/benchmark-1-cp.png",
        "caption": "Learning curves with different observation set. Benchmark environments: Hopper, Walker2D, HalfCheetah, InvertedDoublePendulum.",
        "source": "icra/2011.00756/3_Benchmark_cp.tex",
        "arxiv_id": "icra/2011.00756",
        "llava_output": "This image is a set of four different graphs, each representing a different environment for the experiment. The graphs are labeled with the names of the environments, such as Hopper, Walker2D, HalfCheetah, and InvertedDoublePendulum. Each graph shows the learning curve of the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some dummy data to simulate the actual data from the plots\ndef generate_dummy_data(steps, seed):\n    np.random.seed(seed)\n    return np.cumsum(np.random.randn(steps) * 15 + np.random.rand(steps) * 10)\n\n# Generate dummy steps (x-axis data) and rewards (y-axis data)\nsteps_hopper = np.linspace(0, 0.8e6, 200)\nrewards_hopper = [generate_dummy_data(len(steps_hopper), seed) for seed in range(6)]\n\nsteps_walker = np.linspace(0, 0.8e6, 200)\nrewards_walker = [generate_dummy_data(len(steps_walker), seed) for seed in range(6, 12)]\n\nsteps_halfcheetah = np.linspace(0, 1e6, 200)\nrewards_halfcheetah = [generate_dummy_data(len(steps_halfcheetah), seed) for seed in range(12, 18)]\n\nsteps_invertedpendulum = np.linspace(0, 200e3, 200)\nrewards_invertedpendulum = [generate_dummy_data(len(steps_invertedpendulum), seed) for seed in range(18, 24)]\n\n# Create a figure with four subplots, one for each environment\nfig, axs = plt.subplots(2, 2, figsize=(15, 10))\n(ax1, ax2), (ax3, ax4) = axs\n\ncolors = ['r', 'b', 'g', 'c', 'm', 'y']\n\nfor i in range(6):\n    ax1.plot(steps_hopper, rewards_hopper[i], color=colors[i], label=f'Method {i+1}')\n    ax2.plot(steps_walker, rewards_walker[i], color=colors[i], label=f'Method {i+1}')\n    ax3.plot(steps_halfcheetah, rewards_halfcheetah[i], color=colors[i], label=f'Method {i+1}')\n    ax4.plot(steps_invertedpendulum, rewards_invertedpendulum[i], color=colors[i], label=f'Method {i+1}')\n\n# Set plot titles\nax1.set_title('Hopper-v2')\nax2.set_title('Walker2d-v2')\nax3.set_title('HalfCheetah-v2')\nax4.set_title('InvertedDoublePendulum-v2')\n\n# Set x and y labels\nfor ax in axs.flat:\n    ax.set(xlabel='Step', ylabel='Reward')\n\n# Display the legend\nax1.legend()\nax2.legend()\nax3.legend()\nax4.legend()\n\n# Set the x-axis limits and labels for all plots\nax1.set_xlim(0, 0.8e6)\nax1.set_xticks(np.linspace(0, 0.8e6, 5))\nax1.set_xticklabels(['0', '0.2M', '0.4M', '0.6M', '0.8M'])\n\nax2.set_xlim(0, 0.8e6)\nax2.set_xticks(np.linspace(0, 0.8e6, 5))\nax2.set_xticklabels(['0', '0.2M', '0.4M', '0.6M', '0.8M'])\n\nax3.set_xlim(0, 1e6)\nax3.set_xticks(np.linspace(0, 1e6, 3))\nax3.set_xticklabels(['0', '0.5M', '1M'])\n\nax4.set_xlim(0, 200e3)\nax4.set_xticks(np.linspace(0, 200e3, 5))\nax4.set_xticklabels(['0', '50k', '100k', '150k', '200k'])\n\n# Tight layout adjusts the subplots to fit into the figure area.\nplt.tight_layout()\n\n# Save the figure with a resolution of 300 dpi\nplt.savefig('learning_curves.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2011.00756/benchmark-1-cp_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2011.00756/benchmark-2-1-2-dash.png",
        "caption": "Learning curves on Hopper-v2. (a) Three observation spaces (RS, OAI, and Ours) with and without a short history. (b) Ours set with different additional information, which drops significantly when we add $x$ to Ours.",
        "source": "icra/2011.00756/3_Benchmark_cp.tex",
        "arxiv_id": "icra/2011.00756",
        "llava_output": "The image is a line chart showing the learning curves on Hopper-v2. The chart has three observation spaces (RS, OAI, and Ours) with and without a short history. The chart also shows the Ours set with different additional information, which drops significantly when we add $x$ to",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some synthetic data to mimic the learning curves\nsteps = np.linspace(0, 800000, 500)  # Generate steps from 0 to 0.8M\nnp.random.seed(0)  # For reproducibility\n\n# Synthetic reward data; using random noise to simulate variability in training\nrs = np.random.normal(loc=1000, scale=200, size=len(steps)).cumsum() + 500\nrs_history = np.random.normal(loc=1100, scale=220, size=len(steps)).cumsum() + 600\n\noai = np.random.normal(loc=1400, scale=250, size=len(steps)).cumsum() + 800\noai_history = np.random.normal(loc=1500, scale=270, size=len(steps)).cumsum() + 900\n\nours = np.random.normal(loc=1700, scale=300, size=len(steps)).cumsum() + 1000\nours_history = np.random.normal(loc=1800, scale=330, size=len(steps)).cumsum() + 1100\n\n# Smoothing the data just for illustration\ndef smooth(y, box_pts):\n    box = np.ones(box_pts)/box_pts\n    y_smooth = np.convolve(y, box, mode='same')\n    return y_smooth\n\n# Apply the smoothing function to the synthetic data\nrs = smooth(rs, 20)\nrs_history = smooth(rs_history, 20)\noai = smooth(oai, 20)\noai_history = smooth(oai_history, 20)\nours = smooth(ours, 20)\nours_history = smooth(ours_history, 20)\n\n# Start plotting\nplt.figure(figsize=(6, 5))\nplt.plot(steps, rs, label='RS', color='blue')\nplt.plot(steps, rs_history, label='RS+history', linestyle='--', color='blue')\nplt.plot(steps, oai, label='OAI', color='green')\nplt.plot(steps, oai_history, label='OAI+history', linestyle='--', color='green')\nplt.plot(steps, ours, label='Ours', color='black')\nplt.plot(steps, ours_history, label='Ours+history', linestyle='--', color='black')\n\nplt.xlabel('Step')\nplt.ylabel('Reward')\nplt.title('(a) History (N=2)')\nplt.legend()\n\n# Adjust the x-axis labels to match the desired format\nplt.xticks(\n    np.arange(0, 800001, 200000),\n    ['0', '0.2M', '0.4M', '0.6M', '0.8M']\n)\n\n# Save the figure as a png file with 300dpi\nplt.savefig('learning_curves.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2011.00756/benchmark-2-1-2-dash_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2011.00756/exp-result.png",
        "caption": "Learning curves comparing optimized observation set (Search) to initial observation set (RS) and OpenAI benchmarks observation (OAI).",
        "source": "icra/2011.00756/5_Experiments.tex",
        "arxiv_id": "icra/2011.00756",
        "llava_output": "4 graphs showing the results of different experiments.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - replace with actual data.\n# For example purposes, we will use random data.\nnp.random.seed(0)\nx = np.linspace(0, 1, 100)\ny1 = np.random.normal(loc=1000, scale=500, size=x.shape).cumsum()\ny2 = np.random.normal(loc=1200, scale=500, size=x.shape).cumsum()\ny3 = np.random.normal(loc=1500, scale=500, size=x.shape).cumsum()\n\n# Function to plot one subplot\ndef plot_subplot(ax, title, x, y1, y2, y3):\n    ax.plot(x, y1, label='RS')\n    ax.plot(x, y2, label='OAI')\n    ax.plot(x, y3, label='Search')\n    ax.set_title(title)\n    ax.set_xlabel('Step')\n    ax.set_ylabel('Reward')\n    ax.legend()\n\n# Create figure and subplots\nfig, axs = plt.subplots(2, 2, figsize=(15, 10))\nplot_subplot(axs[0, 0], 'Hopper-v2', x, y1, y2, y3)\nplot_subplot(axs[0, 1], 'Walker2d-v2', x, y1, y2, y3)\nplot_subplot(axs[1, 0], 'HalfCheetah-v2', x, y1, y2, y3)\nplot_subplot(axs[1, 1], 'InvertedDoublePendulum-v2', x, y1, y2, y3)\n\n# Fine-tune figure; make sure subplots do not overlap\nfig.tight_layout()\n\n# Save the figure\nplt.savefig('learning_curves.png', dpi=300)\n\n# Show plot for visual confirmation\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2011.00756/exp-result_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2011.00756/exp-abl-1a-evaluation.png",
        "caption": "Comparison of design choices. The plots show learning curves of the optimized observation spaces. We evaluate four different hyperparameters, (a) evaluation metric, (b) training time, (c) grouping scheme, and (d) dropout layer.",
        "source": "icra/2011.00756/5_Experiments.tex",
        "arxiv_id": "icra/2011.00756",
        "llava_output": "The image is a Line Chart that presents the experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical data (replace these with your actual data)\nsteps = np.linspace(0, 800000, 100)  # Steps from 0 to 800k\ntest_data = np.random.rand(100) * 1500  # Random data for \"Test\"\nall_data = np.random.rand(100) * 1500 + 200  # Random data for \"All\", slightly higher\nhalf_data = np.random.rand(100) * 1500 + 300  # Random data for \"Half\", even higher\n\n# Plotting the data\nplt.figure()\nplt.plot(steps, test_data, label='Test', color='green')\nplt.plot(steps, all_data, label='All', color='orange')\nplt.plot(steps, half_data, label='Half', color='blue')\n\n# Adding titles and labels\nplt.title('(a) Evaluation Metric')\nplt.xlabel('Step')\nplt.ylabel('Reward')\n\n# Formatting x-axis to display '200k', '400k', etc.\nplt.xticks([0, 200000, 400000, 600000, 800000], ['0', '200k', '400k', '600k', '800k'])\n\n# Adding legend\nplt.legend()\n\n# Saving the plot with 300 DPI\nplt.savefig('evaluation_metric.png', dpi=300)\n\n# Showing the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2011.00756/exp-abl-1a-evaluation_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2011.03790/eval_curves1.png",
        "caption": "Evaluation of the generated sparse keypoint-based model. We measure the average errors in estimated keypoint position with respect to the total no. of keypoints in (a) and total no. of scenes in (b).",
        "source": "icra/2011.03790/root.tex",
        "arxiv_id": "icra/2011.03790",
        "llava_output": "The image is a scatter plot that presents the evaluation of the generated sparse keypoint-based model. The scatter plot shows the average errors in estimated keypoint position with respect to the total number of keypoints in the image. The x-axis represents the total number of keypoints, while the y-axis represents",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Example data points\ntotal_keypoints = [8, 9, 10, 11, 12]\nerror_obj1_a = [0.006, 0.005, 0.008, 0.004, 0.007]\nerror_obj2_a = [0.007, 0.006, 0.009, 0.005, 0.008]\nerror_obj3_a = [0.004, 0.003, 0.006, 0.003, 0.005]\n\ntotal_scenes = [3, 4, 5, 6, 7, 8]\nerror_obj1_b = [0.010, 0.008, 0.009, 0.007, 0.006, 0.005]\nerror_obj2_b = [0.011, 0.009, 0.008, 0.007, 0.007, 0.006]\nerror_obj3_b = [0.008, 0.007, 0.006, 0.006, 0.005, 0.005]\n\n# Create figure and axes\nfig, axs = plt.subplots(1, 2, figsize=(14, 5), constrained_layout=True)\n\n# Plotting the left graph (a)\naxs[0].plot(total_keypoints, error_obj1_a, 'r-o', label='OBJ1')\naxs[0].plot(total_keypoints, error_obj2_a, 'b-o', label='OBJ2')\naxs[0].plot(total_keypoints, error_obj3_a, 'g-o', label='OBJ3')\naxs[0].set_xlabel('Total number of defined keypoints')\naxs[0].set_ylabel('Mean error (in meters)')\naxs[0].set_title('Mean error vs Num. of Total Keypoints')\naxs[0].legend(loc='upper right')\naxs[0].text(10, 0.003, 'scenes=6', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n\n# Plotting the right graph (b)\naxs[1].plot(total_scenes, error_obj1_b, 'r-o', label='OBJ1')\naxs[1].plot(total_scenes, error_obj2_b, 'b-o', label='OBJ2')\naxs[1].plot(total_scenes, error_obj3_b, 'g-o', label='OBJ3')\naxs[1].set_xlabel('Total number of scenes')\naxs[1].set_ylabel('Mean error (in meters)')\naxs[1].set_title('Mean error vs Num. of Scenes')\naxs[1].legend(loc='upper right')\naxs[1].text(5, 0.003, 'keypoints=9', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2011.03790/eval_curves1_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2102.05414/manipVSerr.png",
        "caption": "Scenario A: Comparison of manipulability and position error for the circle trajectory on the 3 UR5 robot arms.",
        "source": "icra/2102.05414/main.tex",
        "arxiv_id": "icra/2102.05414",
        "llava_output": "The image is a line chart showing the manipulability and position error for the circle trajectory on the 3 UR5 robot arms.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate synthetic data\ntimesteps = np.arange(0, 1800)\narm1_manipulability = (np.sin(timesteps * np.pi / 150) + 1) * 0.1\narm2_manipulability = (np.sin(timesteps * np.pi / 150 + np.pi / 3) + 1) * 0.1\narm3_manipulability = (np.sin(timesteps * np.pi / 150 + 2 * np.pi / 3) + 1) * 0.1\n\narm1_pos_error = (np.cos(timesteps * np.pi / 150) + 1) * 0.01\narm2_pos_error = (np.cos(timesteps * np.pi / 150 + np.pi / 3) + 1) * 0.01\narm3_pos_error = (np.cos(timesteps * np.pi / 150 + 2 * np.pi / 3) + 1) * 0.01\n\n# Create two axes\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()\n\n# Plot manipulability on ax1\nax1.plot(timesteps, arm1_manipulability, 'b-', label='Arm 1 manipulability')\nax1.plot(timesteps, arm2_manipulability, 'g-', label='Arm 2 manipulability')\nax1.plot(timesteps, arm3_manipulability, 'r-', label='Arm 3 manipulability')\n\n# Plot position error on ax2\nax2.plot(timesteps, arm1_pos_error, 'c-', label='Arm 1 pos. error')\nax2.plot(timesteps, arm2_pos_error, 'm-', label='Arm 2 pos. error')\nax2.plot(timesteps, arm3_pos_error, 'y-', label='Arm 3 pos. error')\n\n# Label axes\nax1.set_xlabel('Timestep')\nax1.set_ylabel('Manipulability')\nax2.set_ylabel('Position Error [m]')\n\n# Legends\nax1.legend(loc='upper left')\nax2.legend(loc='upper right')\n\n# Title\nplt.title('Scenario A: Comparison of manipulability and position error for the circle trajectory on the 3 UR5 robot arms')\n\n# Save figure\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Show plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2102.05414/manipVSerr_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2102.05414/manipComparisonSquare.png",
        "caption": "Qualitative manipulability comparison for scenarios B and C of the square trajectory on the 3 UR5 robot arms",
        "source": "icra/2102.05414/main.tex",
        "arxiv_id": "icra/2102.05414",
        "llava_output": "The image is a scatter plot that shows the manipulability comparison for scenarios B and C of the square trajectory on the three UR5 robot arms. The plot displays the time steps on the x-axis and the manipulability on the y-axis. The data is represented by multiple points scattered across",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulating some data for demonstration purposes.\n# In your actual application, replace these with the real data.\ntime_steps = np.linspace(0, 1500, 1500)\ndata_arm_1 = 0.02 * np.cos(0.02 * np.pi * time_steps) + 0.05\ndata_arm_2 = 0.02 * np.sin(0.02 * np.pi * time_steps + 0.2) + 0.05\ndata_arm_3 = 0.015 * np.sin(0.02 * np.pi * time_steps + 0.4) + 0.055\n\n# Create subplots with 2 columns for Scenario B and C\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\n# Plot data for Scenario B\naxes[0].plot(time_steps, data_arm_1, label='UR5 arm 1', color='blue')\naxes[0].plot(time_steps, data_arm_2, label='UR5 arm 2', color='orange')\naxes[0].plot(time_steps, data_arm_3, label='UR5 arm 3', color='green')\naxes[0].set_title('Scenario B')\naxes[0].set_ylim(0, 0.10)\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Manipulability')\naxes[0].legend(loc='upper right')\n\n# Plot data for Scenario C\naxes[1].plot(time_steps, data_arm_1, label='UR5 arm 1', color='blue')\naxes[1].plot(time_steps, data_arm_2, label='UR5 arm 2', color='orange')\naxes[1].plot(time_steps, data_arm_3, label='UR5 arm 3', color='green')\naxes[1].set_title('Scenario C')\naxes[1].set_ylim(0, 0.10)\naxes[1].set_xlabel('Timestep')\naxes[1].set_ylabel('Manipulability')\naxes[1].legend(loc='upper right')\n\n# Layout adjustment\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('manipulability_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2102.05414/manipComparisonSquare_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2102.05414/manipComparisonVR.png",
        "caption": "Qualitative manipulability comparison for scenarios B and C of the teleoperation trajectory on the 3 UR5 robots",
        "source": "icra/2102.05414/main.tex",
        "arxiv_id": "icra/2102.05414",
        "llava_output": "The image is a set of three line charts, each representing a different scenario (A, B, and C) for the manipulability of the UR5 robots. The charts are labeled with the corresponding scenario and are displayed side by side. The graphs show the manipulability of the robots",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data - replace these with the actual data\ntime_steps = range(0, 1600)  # Placeholder for the x-axis data\nmanipulability_arm1_b = [0.05] * len(time_steps)  # Placeholder data for arm1 in Scenario B\nmanipulability_arm2_b = [0.06] * len(time_steps)  # Placeholder data for arm2 in Scenario B\nmanipulability_arm3_b = [0.04] * len(time_steps)  # Placeholder data for arm3 in Scenario B\nmanipulability_arm1_c = [0.05] * len(time_steps)  # Placeholder data for arm1 in Scenario C\nmanipulability_arm2_c = [0.07] * len(time_steps)  # Placeholder data for arm2 in Scenario C\nmanipulability_arm3_c = [0.04] * len(time_steps)  # Placeholder data for arm3 in Scenario C\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Scenario B\naxes[0].plot(time_steps, manipulability_arm1_b, label='UR5 arm 1', color='blue')\naxes[0].plot(time_steps, manipulability_arm2_b, label='UR5 arm 2', color='green')\naxes[0].plot(time_steps, manipulability_arm3_b, label='UR5 arm 3', color='orange')\naxes[0].set_title('Scenario B')\naxes[0].set_xlabel('Timestep')\naxes[0].set_ylabel('Manipulability')\naxes[0].set_ylim([0, 0.1])\naxes[0].legend()\n\n# Scenario C\naxes[1].plot(time_steps, manipulability_arm1_c, label='UR5 arm 1', color='blue')\naxes[1].plot(time_steps, manipulability_arm2_c, label='UR5 arm 2', color='green')\naxes[1].plot(time_steps, manipulability_arm3_c, label='UR5 arm 3', color='orange')\naxes[1].set_title('Scenario C')\naxes[1].set_xlabel('Timestep')\naxes[1].set_ylabel('Manipulability')\naxes[1].set_ylim([0, 0.1])\naxes[1].legend()\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure with 300 DPI\nplt.savefig('manipulability_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2102.05414/manipComparisonVR_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2102.05414/manipComparisonVRYuMi.png",
        "caption": "Qualitative manipulability comparison for scenarios B and C of the teleoperation trajectory on the 2 YuMi robots",
        "source": "icra/2102.05414/main.tex",
        "arxiv_id": "icra/2102.05414",
        "llava_output": "The image is a scatter plot that presents the results of the experiment comparing the manipulability of scenarios B and C for the teleoperation trajectory on the two YuMi robots. The x-axis represents the time step, and the y-axis represents the manipulability. The scatter plot shows the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some random data to simulate the actual data\ntimesteps = np.arange(0, 1600)\nyumi1_left_b = np.random.rand(1600) * 0.1\nyumi1_right_b = np.random.rand(1600) * 0.1\nyumi2_left_b = np.random.rand(1600) * 0.1\nyumi2_right_b = np.random.rand(1600) * 0.1\n\nyumi1_left_c = np.random.rand(1600) * 0.1\nyumi1_right_c = np.random.rand(1600) * 0.1\nyumi2_left_c = np.random.rand(1600) * 0.1\nyumi2_right_c = np.random.rand(1600) * 0.1\n\n# Create the figure and axes\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plotting data for Scenario B\nax1.plot(timesteps, yumi1_left_b, label='YuMi 1 left')\nax1.plot(timesteps, yumi1_right_b, label='YuMi 1 right')\nax1.plot(timesteps, yumi2_left_b, label='YuMi 2 left')\nax1.plot(timesteps, yumi2_right_b, label='YuMi 2 right')\nax1.set_title('Scenario B')\nax1.set_xlabel('Timestep')\nax1.set_ylabel('Manipulability')\nax1.legend()\n\n# Plotting data for Scenario C\nax2.plot(timesteps, yumi1_left_c, label='YuMi 1 left')\nax2.plot(timesteps, yumi1_right_c, label='YuMi 1 right')\nax2.plot(timesteps, yumi2_left_c, label='YuMi 2 left')\nax2.plot(timesteps, yumi2_right_c, label='YuMi 2 right')\nax2.set_title('Scenario C')\nax2.set_xlabel('Timestep')\nax2.set_ylabel('Manipulability')\nax2.legend()\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Save the figure as a PNG file with 300 DPI\nplt.savefig('manipulability_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2102.05414/manipComparisonVRYuMi_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2103.05225/ep_reward.png",
        "caption": "Learning curve of DQN on one of the environment",
        "source": "icra/2103.05225/Supplementary_first_submission.tex",
        "arxiv_id": "icra/2103.05225",
        "llava_output": "The image is a line chart showing the learning curve of a Deep Q-Network (DQN) on one of the environments.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# This is just example data, you should replace 'episode_returns' with your actual data list\nepisode_returns = [-100*x for x in range(100)]  # Replace with your data\n\n# Define the number of steps (substitute the 'range' with your actual steps)\nsteps = range(0, 100)\n\n# Create the plot\nplt.figure()\nplt.plot(steps, episode_returns)\n\n# Add labels and title (match these with your figure)\nplt.xlabel('Steps')\nplt.ylabel('Episode Return')\nplt.title('Learning curve of DQN on one of the environment')\n\n# Save the figure with 300 DPI\nplt.savefig('learning_curve.png', dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2103.05225/ep_reward_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2103.13282/ClusterNEW.jpg",
        "caption": "Dataset statistics A: Histogram of number of key points per annotated image, and range of cheetah sizes. B: Diversity of cheetah poses in annotated data seen by t-SNE clustering of the ground truth data, with example centroid posture from each kmeans cluster (cheetahs normalized for size and clustered by kmeans).",
        "source": "icra/2103.13282/arxivV1.tex",
        "arxiv_id": "icra/2103.13282",
        "llava_output": "The image is a scatter plot showing the number of key points per annotated image and the range of cheetah sizes.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\n# Random placeholder data for the histograms\nbounding_box_data = np.random.lognormal(mean=6.5, sigma=1.0, size=2000)\nnum_bodyparts_data = np.random.poisson(lam=10, size=2000)\n\n# Random placeholder data for the t-SNE clustering\npose_data = np.random.rand(100, 2)  # 100 samples with 2 features each (for simplicity)\ntsne = TSNE(n_components=2, random_state=0)\npose_tsne = tsne.fit_transform(pose_data)\n\n# KMeans clustering on the t-SNE data to identify centroids\nkmeans = KMeans(n_clusters=10, random_state=0)\nclusters = kmeans.fit_predict(pose_tsne)\ncentroids = kmeans.cluster_centers_\n\n# Create plots\nfig, axs = plt.subplots(1, 2, figsize=(20, 8))\n\n# Histogram plots\naxs[0].hist(bounding_box_data, bins=30, color='blue')\naxs[0].set_xlabel(\"Bounding box diagonal [pixels]\")\naxs[0].set_ylabel(\"count\")\naxs[0].set_title(\"A\")\n\naxs[0].hist(num_bodyparts_data, bins=30, color='blue', orientation='horizontal')\naxs[0].set_xlabel(\"count\")\naxs[0].set_ylabel(\"Number of labeled bodyparts\")\n\n# t-SNE and KMeans cluster illustration\ncolors = plt.cm.rainbow(np.linspace(0, 1, 10))\nfor idx, centroid in enumerate(centroids):\n    cluster_member_indices = np.where(clusters == idx)[0]\n    cluster_member_data = pose_tsne[cluster_member_indices]\n\n    # draw the keypoints and lines for each cluster - this is representational\n    for member in cluster_member_data:\n        axs[1].scatter(member[0], member[1], color=colors[idx])\n        axs[1].plot(centroid[0], centroid[1], 'kx', markersize=10)\n\n    # Drawing connections, purely illustrative\n    keypoints_x, keypoints_y = cluster_member_data.T\n    for i in range(keypoints_x.size - 1):\n        axs[1].plot(keypoints_x[i:i+2], keypoints_y[i:i+2], color=colors[idx])\n\n# Additional formatting for visibility\naxs[1].set_title(\"B\")\naxs[1].axis('off')\n\n# Save figure\nplt.tight_layout()\nplt.savefig('data_visualization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Histogram",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2103.13282/ClusterNEW_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2103.13282/figure5-trim.png",
        "caption": "Summary of simulation results of the three methods.",
        "source": "icra/2103.13282/arxivV1.tex",
        "arxiv_id": "icra/2103.13282",
        "llava_output": "The image is a collection of six different graphs, each displaying the noise levels of the three methods. The graphs are arranged in a row, with each graph showing the noise levels at different percentages. The graphs are labeled with the corresponding percentages, such as 0.0%, 25%,",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some mock data for demonstration purposes\nnp.random.seed(0)\ndata1 = [np.random.normal(0, std, 100) for std in range(1, 4)]\ndata2 = [np.random.normal(0, std, 100) for std in range(1, 4)]\ndata3 = [np.random.normal(0, std, 100) for std in range(1, 4)]\n\n# A function to create a boxplot at a specific grid position\ndef create_boxplot(data, position, title, color):\n    ax = plt.subplot(3, 3, position)\n    ax.boxplot(data, patch_artist=True, boxprops=dict(facecolor=color))\n    ax.set_title(title)\n    ax.set_xticklabels(['TRI', 'EKF', 'FTE'])\n\n# Set figure size and subplot layout\nplt.figure(figsize=(10, 10))\n\n# First row of plots\ncreate_boxplot(data1, 1, 'Noise \u03c3 = 0, 0% Outliers', 'orange')\ncreate_boxplot(data2, 2, 'Noise \u03c3 = 0, 2% Outliers', 'orange')\ncreate_boxplot(data3, 3, 'Noise \u03c3 = 0, 5% Outliers', 'orange')\n\n# Second row of plots\ncreate_boxplot(data1, 4, 'Noise \u03c3 = 5, 0% Outliers', 'blue')\ncreate_boxplot(data2, 5, 'Noise \u03c3 = 5, 2% Outliers', 'blue')\ncreate_boxplot(data3, 6, 'Noise \u03c3 = 5, 5% Outliers', 'blue')\n\n# Third row of plots\ncreate_boxplot(data1, 7, 'Noise \u03c3 = 10, 0% Outliers', 'green')\ncreate_boxplot(data2, 8, 'Noise \u03c3 = 10, 2% Outliers', 'green')\ncreate_boxplot(data3, 9, 'Noise \u03c3 = 10, 5% Outliers', 'green')\n\n# Adjust layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('boxplot_figure.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Box Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2103.13282/figure5-trim_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2103.15215/rvio_err.png",
        "caption": "Range-VIO position error",
        "source": "icra/2103.15215/root.tex",
        "arxiv_id": "icra/2103.15215",
        "llava_output": "The image is a line chart showing the position error of a VIO system.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data - replace these with your actual data\ntimes = np.linspace(380, 480, 1000)  # Replace with your actual time data\nx_error = np.sin(0.1*times)  # Replace with X position error data\ny_error = np.cos(0.1*times) + 1  # Replace with Y position error data\nz_error = np.sin(0.1*times) * np.cos(0.1*times)  # Replace with Z position error data\n\n# Create the plot\nplt.figure()\nplt.plot(times, x_error, label='X', color='blue')\nplt.plot(times, y_error, label='Y', color='orange')\nplt.plot(times, z_error, label='Z', color='green')\n\n# Label the axes\nplt.xlabel('Time [s]')\nplt.ylabel('Position error [m]')\n\n# Add the title\nplt.title('Range-VIO position error')\n\n# Add the legend\nplt.legend()\n\n# Save the figure\nplt.savefig('position_error_plot.png', dpi=300)\n\n# Show the figure\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2103.15215/rvio_err_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 5,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2104.03657/translational_error_average_boxplot_per_segment.png",
        "caption": "Relative trajectory errors for different segment lengths. Mean values are indicated by black bars.",
        "source": "icra/2104.03657/evaluation.tex",
        "arxiv_id": "icra/2104.03657",
        "llava_output": "3D MiniNet Based Removal:\n\n* No Removal: 0.00%\n* Occupancy Grid Based Removal: 0.00%\n* Translational Error: 0.00%\n* 1 Segment Length: 0.0",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (you'll need to replace this with the actual data)\nsegment_lengths = [0.5, 1, 2, 5, 10, 20]\nno_removal_data = [np.random.normal(10, 5, size=100) for _ in segment_lengths]\nmini_net_removal_data = [np.random.normal(6, 3, size=100) for _ in segment_lengths]\noccupancy_grid_removal_data = [np.random.normal(4, 2, size=100) for _ in segment_lengths]\n\n# Mean values for each method\nno_removal_means = [np.mean(data) for data in no_removal_data]\nmini_net_removal_means = [np.mean(data) for data in mini_net_removal_data]\noccupancy_grid_removal_means = [np.mean(data) for data in occupancy_grid_removal_data]\n\nfig, ax = plt.subplots()\n\n# Boxplot positions\npositions_no_removal = [x - 0.2 for x in range(len(segment_lengths))]\npositions_mini_net_removal = [x for x in range(len(segment_lengths))]\npositions_occupancy_grid_removal = [x + 0.2 for x in range(len(segment_lengths))]\n\n# Create boxplots\nbp_no_removal = ax.boxplot(no_removal_data, positions=positions_no_removal, widths=0.15, patch_artist=True, boxprops=dict(facecolor=\"grey\"))\nbp_mini_net_removal = ax.boxplot(mini_net_removal_data, positions=positions_mini_net_removal, widths=0.15, patch_artist=True, boxprops=dict(facecolor=\"red\"))\nbp_occupancy_grid_removal = ax.boxplot(occupancy_grid_removal_data, positions=positions_occupancy_grid_removal, widths=0.15, patch_artist=True, boxprops=dict(facecolor=\"blue\"))\n\n# Add mean value indicators (black bars)\nfor i in range(len(segment_lengths)):\n    ax.plot([positions_no_removal[i], positions_no_removal[i]], [no_removal_means[i], no_removal_means[i]], color='black', linewidth=2)\n    ax.plot([positions_mini_net_removal[i], positions_mini_net_removal[i]], [mini_net_removal_means[i], mini_net_removal_means[i]], color='black', linewidth=2)\n    ax.plot([positions_occupancy_grid_removal[i], positions_occupancy_grid_removal[i]], [occupancy_grid_removal_means[i], occupancy_grid_removal_means[i]], color='black', linewidth=2)\n\n# Customize the plot\nax.set_xticks(range(len(segment_lengths)))\nax.set_xticklabels(segment_lengths)\nax.set_xlabel('Segment Length [m]')\nax.set_ylabel('Translational Error [%]')\nax.set_title('Relative trajectory errors for different segment lengths')\n\n# Adding a legend\nax.legend([bp_no_removal[\"boxes\"][0], bp_mini_net_removal[\"boxes\"][0], bp_occupancy_grid_removal[\"boxes\"][0]], \n          ['No Removal', '3D-MiniNet Based Removal', 'Occupany Grid Based Removal'], loc='upper right')\n\n# Save the figure with 300 DPI\nplt.savefig('boxplot.png', dpi=300)\nplt.show()",
        "code_cot": "",
        "type": "Box Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2104.03657/translational_error_average_boxplot_per_segment_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "icra_figures/2107.05998/2DPosition_error.png",
        "caption": "Trajectory following results. The solid lines represent the trajectories. The dotted lines are the computed and real position error.",
        "source": "icra/2107.05998/text.tex",
        "arxiv_id": "icra/2107.05998",
        "llava_output": "40, 80, 120, 160, 200, 240, 280, 320, 360, 400, 440, 480, 520, 5",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some example data\n# You should replace these arrays with your actual data\ny_position = np.linspace(0, 160, 500)\nx_trajectory = np.sin(y_position / 10) * 20 + 20  # Just for illustration\nx_position_error_computed = np.sin(y_position / 15) * 5  # Just for illustration\nx_position_error_real = np.cos(y_position / 15) * 5  # Just for illustration\n\n# Plot trajectories (solid lines)\nplt.plot(y_position, x_trajectory, color='red', label='Trajectory')\n\n# Plot position errors (dotted lines)\nplt.plot(y_position, x_position_error_computed, 'b--', label='Computed position error')\nplt.plot(y_position, x_position_error_real, 'r:', label='Real position error')\n\n# Customize the plot\nplt.xlabel('Y position (mm)')\nplt.ylabel('X position / Position error (mm)')\nplt.title('Trajectory following results')\nplt.legend(loc='upper left')\n\n# Set the secondary y-axis for the position error\nax2 = plt.twinx()\nax2.set_ylabel('Position error (mm)')\nax2.set_ylim(plt.ylim())  # Ensure the secondary y-axis aligns with the primary y-axis\n\n# Set plot style and colors\nplt.style.use('dark_background')\nplt.grid(True)\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('trajectory_plot.png', dpi=300)\n\n# Alternatively, if you also want to view the plot on screen, use plt.show()\n# plt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2107.05998/2DPosition_error_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "icra_figures/2107.05998/compensation_error.png",
        "caption": "The absolute error of the movement compensation algorithm. Two different movement types (translation and rotation) are investigated separately.",
        "source": "icra/2107.05998/text.tex",
        "arxiv_id": "icra/2107.05998",
        "llava_output": "The image is a scatter plot showing the absolute error of the movement compensation algorithm for two different movement types (translation and rotation). The x-axis represents the translation error, while the y-axis represents the rotation error. The scatter plot consists of several points, some of which are clustered together, while others",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Creating some data for the two movement types (Translation and Rotation)\n# For demonstration purposes, random numbers are used.\nnp.random.seed(0)  # For reproducibility\ntranslation_data = np.random.normal(loc=3, scale=0.5, size=100)\nrotation_data = np.random.normal(loc=3, scale=0.5, size=100)\n\ndata = [translation_data, rotation_data]\n\n# Creating a boxplot\nfig, ax = plt.subplots()\n\n# Boxplot with blue boxes and red medians\nbp = ax.boxplot(data, patch_artist=True, medianprops=dict(color=\"red\"))\n\n# Changing the box color to blue\nfor box in bp['boxes']:\n    box.set(color='blue', linewidth=2)\n    box.set(facecolor='blue')\n\n# Adding labels on the x-axis for the movement types\nax.set_xticklabels(['Translation', 'Rotation'])\n\n# Setting the y-axis label\nax.set_ylabel('Error (mm)')\n\n# Setting the range for the y-axis\nax.set_ylim(2, 4.5)\n\n# Changing the background color to black and the ticks color to white\nax.set_facecolor('black')\nax.tick_params(axis='x', colors='white')\nax.tick_params(axis='y', colors='white')\n\n# Saving the figure with 300dpi\nplt.savefig('boxplot.png', dpi=300, facecolor='black')\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Box Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2107.05998/compensation_error_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2107.05998/2DPosition_error.png",
        "caption": "Trajectory following results. The solid lines represent the trajectories. The dotted lines are the computed and real position error.",
        "source": "icra/2107.05998/text_final.tex",
        "arxiv_id": "icra/2107.05998",
        "llava_output": "The image is a Line Chart displaying the trajectory of a moving object. The chart shows the position of the object over time, with the x-axis representing time and the y-axis representing position in meters. The chart also includes a dotted line representing the computed position error and a solid line representing the real position",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data (replace with your actual data)\ny_position = [i for i in range(170)]  # X-axis data\nx_position = [i + (i % 10) for i in range(170)]  # Y-axis trajectory data\nposition_error_computed = [(-1)**(i//10) * (i % 10) for i in range(170)]  # Y-axis computed error data\nposition_error_real = [(-1)**(i//15) * (i % 5) for i in range(170)]  # Y-axis real error data\n\n# Plot the trajectories\nplt.plot(y_position, x_position, 'r', label='Trajectory')\n\n# Plot the computed and real position error\nplt.plot(y_position, position_error_computed, 'b--', label='Computed Position Error')\nplt.plot(y_position, position_error_real, 'r:', label='Real Position Error')\n\n# Add labels, title, and legend\nplt.xlabel('Y position (mm)')\nplt.ylabel('X position (mm)')\nplt.title('Trajectory Following Results')\nplt.legend()\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('trajectory_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2107.05998/2DPosition_error_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "No",
            "text_elements_rating": 2,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "icra_figures/2107.05998/compensation_error.png",
        "caption": "Absolute error of the movement compensation algorithm. Two motion types (translation and rotation) over $10$ trials are displayed.",
        "source": "icra/2107.05998/text_final.tex",
        "arxiv_id": "icra/2107.05998",
        "llava_output": "The image is a scatter plot showing the absolute error of the movement compensation algorithm for two motion types (translation and rotation) over 10 trials.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data for demonstration purposes\ntranslation_errors = np.random.uniform(low=2.5, high=4.0, size=10)\nrotation_errors = np.random.uniform(low=2.5, high=4.0, size=10)\n\ndata = [translation_errors, rotation_errors]\n\n# Create the boxplot\nfig, ax = plt.subplots()\nbp = ax.boxplot(data, patch_artist=True)\n\n# Customizing the boxplot colors\nfor box in bp['boxes']:\n    # Change outline color\n    box.set(color='blue', linewidth=2)\n    # Change fill color\n    box.set(facecolor='white')\n\n# Changing the color and line width of the whiskers\nfor whisker in bp['whiskers']:\n    whisker.set(color='blue', linewidth=2)\n\n# Changing the color and linewidth of the caps\nfor cap in bp['caps']:\n    cap.set(color='blue', linewidth=2)\n\n# Changing the color and line width of the medians\nfor median in bp['medians']:\n    median.set(color='red', linewidth=2)\n\n# Changing the style of fliers and their fill\nfor flier in bp['fliers']:\n    flier.set(marker='o', color='blue', alpha=0.5)\n\n# Adding labels\nax.set_xticklabels(['Translation', 'Rotation'])\nax.set_ylabel('Error (mm)')\n\n# Set the colors of the axes background\nax.set_facecolor('black')\nfig.patch.set_facecolor('black')\n\n# Set the colors of the axes ticks\nax.tick_params(axis='x', colors='white')\nax.tick_params(axis='y', colors='white')\n\n# Save the figure\nplt.savefig('boxplot.png', dpi=300, facecolor=fig.get_facecolor())\n\nplt.show()",
        "code_cot": "",
        "type": "Box Plot",
        "if_annotate": true,
        "runnable": false,
        "output_figure_path": "chat-gpt-v/icra/2107.05998/compensation_error_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2108.04531/plot_avg.png",
        "caption": "Average latency between robots and the management system with varying number of connected robots 1, 10 and 100. Upstream latency is average value of continuous data acquisition for 10 minutes which contains two relay points on the path and end point. Downstream latency is average value of 10 commands while collecting data. Target turn around time (c) + (d) is within 1.0 second and could be cleared under those number of robots.",
        "source": "icra/2108.04531/root.tex",
        "arxiv_id": "icra/2108.04531",
        "llava_output": "The image is a line chart that shows the average latency between robots and the management system with varying number of connected robots. The chart has three lines, each representing a different number of robots: 1, 10, and 100. The chart also includes information about the target",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data placeholders (replace with actual data)\nrobots = [1, 10, 100]\nupstream_a = np.array([350, 350, 350]) # Replace with actual values\nupstream_b = np.array([500, 500, 500]) # Replace with actual values\nupstream_c = np.array([650, 650, 650]) # Replace with actual values\ndownstream_d = np.array([200, 250, 300]) # Replace with actual values\n\n# Set up the plot layout\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the upstream average latency\nax1.plot(robots, upstream_a, 'g:', label='(a) robot -> MQTT Broker')\nax1.plot(robots, upstream_b, 'y--', label='(b) robot -> Stream Out')\nax1.plot(robots, upstream_c, 'b-', label='(c) robot -> Stream In')\nax1.set_xscale('log')\nax1.set_xlabel('Number of robots')\nax1.set_ylabel('Latency [ms]')\nax1.set_title('Upstream average latency')\nax1.legend()\n\n# Plot the downstream average latency\nax2.plot(robots, downstream_d, 'b-', label='(d) Command')\nax2.set_xscale('log')\nax2.set_xlabel('Number of robots')\nax2.set_ylabel('Latency [ms]')\nax2.set_title('Downstream average latency')\nax2.legend()\n\n# Saving the figure with 300 DPI\nplt.savefig('latency_plot.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2108.04531/plot_avg_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2108.04531/plot_data.png",
        "caption": "10 minutes time-series latency when 100 robots (using simulator) connected. There is no interruption and the performance was stable. Some larger delay, however, were also recorded in some occasions; 5-10 times during a 10 minute trial.",
        "source": "icra/2108.04531/root.tex",
        "arxiv_id": "icra/2108.04531",
        "llava_output": "10 minutes time-series latency when 100 robots (using simulator) connected. There is no interruption and the performance was stable. Some larger delay, however, were also recorded in some occasions; 5-10 times during a 10 minute trial.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data (you should replace this with your actual data)\nnp.random.seed(0)\nx = np.arange(1, 601)\ny1 = np.random.normal(300, 100, size=600).cumsum() % 600\ny2 = np.random.normal(300, 100, size=600).cumsum() % 600\ny3 = np.random.normal(300, 100, size=600).cumsum() % 600\n\n# Smoothing the sample data to make it look like a time series\ny1 = np.convolve(y1, np.ones(10)/10, mode='same')\ny2 = np.convolve(y2, np.ones(10)/10, mode='same')\ny3 = np.convolve(y3, np.ones(10)/10, mode='same')\n\n# Plotting\nplt.figure(figsize=(10, 4))\nplt.plot(x, y1, 'c:', label='(a) robot -> MQTT Broker', lw=1)\nplt.plot(x, y2, 'm--', label='(b) robot -> Stream Out', lw=1)\nplt.plot(x, y3, 'g-', label='(c) robot -> Stream In', lw=2)\n\n# Decorations\nplt.title('Upstream latency (100 robots, 10 min)')\nplt.xlabel('Number of robots')\nplt.ylabel('Latency [ms]')\nplt.legend()\nplt.ylim(0, 1000)\nplt.grid(True)\n\n# Saving plot to a PNG file\nplt.savefig('upstream_latency.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2108.04531/plot_data_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2108.05297/VLy2.png",
        "caption": "Plot of value of Lyapunov function during numerical simulation.",
        "source": "icra/2108.05297/simStudy.tex",
        "arxiv_id": "icra/2108.05297",
        "llava_output": "The image is a line chart showing the value of a Lyapunov function during a numerical simulation. The chart is labeled with the time in seconds on the x-axis and the value of the Lyapunov function on the y-axis.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the Lyapunov function values decrease exponentially\n# Generating dummy time data from 0 to 1400\ntime = np.linspace(0, 1400, 200)\n# Generating dummy value data for each class, this is just for representation\nvalue_class_0 = np.exp(-time / 500)\nvalue_class_1 = np.exp(-time / 600)\nvalue_class_2 = np.exp(-time / 700)\n\n# Creating the plot\nplt.figure()\n\n# Plotting the data\nplt.plot(time, value_class_0, label='Class 0 Fully Actuated', linestyle='-', color='black')\nplt.plot(time, value_class_1, label='Class 1 Underactuated', linestyle='--', color='blue')\nplt.plot(time, value_class_2, label='Class 2 Underactuated', linestyle='--', color='red')\n\n# Adding a title and labels\nplt.title('Value of Lyapunov Function')\nplt.xlabel('Time (s)')\nplt.ylabel('V(t) - V(\u221e) (M)')\n\n# Adding a legend\nplt.legend()\n\n# Set the x-axis formatter to show the scale as 10^3\nplt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('lyapunov_function_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2108.05297/VLy2_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2109.07024/obstacle_distance.png",
        "caption": "Comparison of the change of the obstacle distance with respect to time when meeting the future colliding obstacle. The proposed method has a larger average minimum distance from obstacles.",
        "source": "icra/2109.07024/root.tex",
        "arxiv_id": "icra/2109.07024",
        "llava_output": "The image is a scatter plot that shows the change of the obstacle distance with respect to time when meeting the future colliding obstacle. The proposed method has a larger average minimum distance from obstacles.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical data - replace with your actual data\ntime = np.linspace(0, 1.5, 100)\nproposed_mean = 3 - 2 * np.exp(-1.5 * time)\nproposed_std = 0.3 + 0.1 * time\ndeterministic_mean = 3 - 2 * np.exp(-1.8 * time)\ndeterministic_std = 0.2 + 0.08 * time\n\n# Plotting the data\nplt.figure(figsize=(10, 5))\nplt.fill_between(time, proposed_mean - proposed_std, proposed_mean + proposed_std, color='blue', alpha=0.3, label='Proposed DPMPC')\nplt.fill_between(time, deterministic_mean - deterministic_std, deterministic_mean + deterministic_std, color='green', alpha=0.3, label='Deterministic DPMPC')\nplt.plot(time, proposed_mean, 'b', label='Proposed DPMPC')\nplt.plot(time, deterministic_mean, 'g', label='Deterministic DPMPC')\nplt.hlines(1.35, time[0], time[-1], colors='red', linestyles='dashed', label='min. safe distance')\n\n# Annotations and text\nplt.text(1.2, 1.2, 'Min. Dist. = 1.35m', color='red')\nplt.text(0.5, 1.5, 'Min. Dist. = 1.63m', color='red')\nplt.text(0.2, 4.5, 'Detour Region', color='black', fontsize=12)\nplt.text(1.3, 3, 'Track Back\\nRegion', color='black', fontsize=12)\n\n# Customizing the plot\nplt.xlabel('Time [s]')\nplt.ylabel('Obstacle Distance [m]')\nplt.title('Comparison of the change of the obstacle distance with respect to time')\nplt.legend()\nplt.ylim(0, 5)\nplt.xlim(0, 1.5)\nplt.grid(True)\n\n# Saving the plot to a file\nplt.savefig('plot.png', dpi=300) # Change 'plot.png' to your desired file path\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2109.07024/obstacle_distance_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2109.07024/distance_bar.png",
        "caption": "Histogram of distances when meeting obstacles. The proposed method ensures the quadrotor keeps a safe distance from obstacles.",
        "source": "icra/2109.07024/root.tex",
        "arxiv_id": "icra/2109.07024",
        "llava_output": "The image is a histogram showing the distance traveled by a quadrotor when it meets obstacles. The proposed method ensures that the quadrotor keeps a safe distance from obstacles.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generating random synthetic data for demonstration purposes.\nnp.random.seed(0)  # For reproducibility\ndata1 = np.random.normal(loc=2.5, scale=0.5, size=350)\ndata2 = np.random.normal(loc=2.0, scale=0.5, size=350)\n\n# Create figure and subplots\nfig, axs = plt.subplots(2, 1, figsize=(8, 6))\n\n# Plot histograms\naxs[0].hist(data1, bins=20, color='blue', label='Deterministic DPMPC')\naxs[1].hist(data2, bins=20, color='red', label='Proposed DPMPC')\n\n# Line indicating the collision zone threshold\nfor ax in axs:\n    ax.axvline(x=0.8, color='yellow', linestyle='--')\n\n# Adding text for collision zone and safe zone\n    ax.text(0.35, ax.get_ylim()[1]*0.9, 'Collision Zone', rotation=90, color='yellow', backgroundcolor='black')\n    ax.text(2.0, ax.get_ylim()[1]*0.9, 'Safe Zone', rotation=90, color='yellow', backgroundcolor='black')\n\n# Set labels and titles\naxs[0].set_title('Histogram of distances when meeting obstacles')\naxs[0].set_ylabel('Number of Samples')\naxs[1].set_ylabel('Number of Samples')\naxs[1].set_xlabel('Distance [m]')\n\n# Adding a legend to the subplots\naxs[0].legend(loc='upper right')\naxs[1].legend(loc='upper right')\n\n# Adjust the layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('histogram_plot.png', dpi=300)\nplt.show()",
        "code_cot": "",
        "type": "Box Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2109.07024/distance_bar_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2204.03698/dynamics.png",
        "caption": "Open loop trajectories of two ring finger joint angles $q_1$ and $q_3$ following previously recorded target positions of a trained policy. In blue we show $15$ individual real trajectories across three robot initializations, and in red $50$ individual simulation trajectories each with dynamics sampled from the domain randomization distribution used for training. The dashed lines indicate the range of the geometric randomization.",
        "source": "icra/2204.03698/2022-icra-manipulation.tex",
        "arxiv_id": "icra/2204.03698",
        "llava_output": "The image is a Line Chart that presents the experiment results.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulating random data for illustration purposes\ntime = np.linspace(0, 10, 100)  # Time from 0 to 10 seconds\nq1_real = np.sin(time) + np.random.normal(0, 0.05, size=time.shape)\nq1_sim = np.sin(time) + np.random.normal(0, 0.05, size=time.shape)\nq3_real = np.cos(time) + np.random.normal(0, 0.05, size=time.shape)\nq3_sim = np.cos(time) + np.random.normal(0, 0.05, size=time.shape)\n\n# Plotting the simulated data\nplt.figure(figsize=(12, 8))\n\n# Individual trajectories for q1 and q3 (real)\nfor _ in range(15):\n    plt.plot(time, np.sin(time) + np.random.normal(0, 0.05, size=time.shape), 'b', alpha=0.2)\n    plt.plot(time, np.cos(time) + np.random.normal(0, 0.05, size=time.shape), 'orange', alpha=0.2)\n\n# Individual simulation trajectories for q1 and q3 (sim)\nfor _ in range(50):\n    plt.plot(time, np.sin(time) + np.random.normal(0, 0.05, size=time.shape), 'r--', alpha=0.2)\n    plt.plot(time, np.cos(time) + np.random.normal(0, 0.05, size=time.shape), 'orange', dashes=[6, 2], alpha=0.2)\n\n# Mean trajectory for real and sim\nplt.plot(time, q1_real, 'b', label='q1 Real', linewidth=2)\nplt.plot(time, q1_sim, 'r', label='q1 Sim', linewidth=2)\nplt.plot(time, q3_real, 'orange', label='q3 Real', linewidth=2)\nplt.plot(time, q3_sim, 'orange', label='q3 Sim', linewidth=2, linestyle='--')\n\n# Adding dashed lines to indicate the range of geometric randomization (example range shown)\nplt.plot([0, 10], [0.4, 0.4], 'k--', alpha=0.5)\nplt.plot([0, 10], [-0.2, -0.2], 'k--', alpha=0.5)\n\n# Adding labels, title, and legend\nplt.xlabel('Time [s]')\nplt.ylabel('q [rad]')\nplt.title('Open loop trajectories of two ring finger joint angles')\nplt.legend()\n\n# Saving the figure with high resolution\nplt.savefig('ring_finger_trajectories.png', dpi=300)\n\n# Showing the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2204.03698/dynamics_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2207.00721/REPS_2_rho_theta_policy_0.png",
        "caption": "Convergence of Gaussian curves over training epochs for all the parameters of a robot in 1 experiment.",
        "source": "icra/2207.00721/RoboMath.tex",
        "arxiv_id": "icra/2207.00721",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data - replace this with your actual data\nepochs = np.linspace(1, 6, 100)  # Example policy update steps\nreward_mean = np.linspace(0, 100, 100)  # Example mean reward\ntheta1_mean = np.linspace(0, 0.25, 100)  # Example mean for theta1\ntheta2_mean = np.linspace(0.5, 1, 100)  # Example mean for theta2\ntheta3_mean = np.linspace(0, 0.5, 100)  # Example mean for theta3\n\n# Example standard deviations for fill_between shading\nstd_dev = 0.1  # Example standard deviation, replace with actual data\n\n# Creating subplots\nfig, axs = plt.subplots(4, 1, figsize=(10, 10), sharex=True)\n\n# Plot for reward\naxs[0].plot(epochs, reward_mean, color='blue')\naxs[0].set_title('Reward')\n\n# Plot for theta_1\naxs[1].plot(epochs, theta1_mean, color='orange')\naxs[1].fill_between(epochs, theta1_mean-std_dev, theta1_mean+std_dev, color='orange', alpha=0.3)\naxs[1].set_title(r'$\\theta_1$')\n\n# Plot for theta_2\naxs[2].plot(epochs, theta2_mean, color='green')\naxs[2].fill_between(epochs, theta2_mean-std_dev, theta2_mean+std_dev, color='green', alpha=0.3)\naxs[2].set_title(r'$\\theta_2$')\n\n# Plot for theta_3\naxs[3].plot(epochs, theta3_mean, color='purple')\naxs[3].fill_between(epochs, theta3_mean-std_dev, theta3_mean+std_dev, color='purple', alpha=0.3)\naxs[3].set_title(r'$\\theta_2$')\naxs[3].set_xlabel('Policy Updates')\n\n# Setting x-axis limits (replace with your actual limits based on your data)\naxs[-1].set_xlim(1, 6)\n\n# Adjust the layout\nplt.tight_layout()\n\n# Saving the figure with 300 DPI\nplt.savefig('convergence_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2207.00721/REPS_2_rho_theta_policy_0_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2207.00721/rewards_combined.png",
        "caption": "Gaussian plots for rewards over all the experiments over the 3 robots. There is significant overlap in the convergence trends across all 3 robots.",
        "source": "icra/2207.00721/RoboMath.tex",
        "arxiv_id": "icra/2207.00721",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Simulate some data\npolicy_updates = np.arange(1, 9)\n# Assume each robot's reward is Gaussian distributed around a mean value that increases with each policy update.\nmeans = {\n    \"Robot 1\": policy_updates * 10,\n    \"Robot 2\": policy_updates * 12,\n    \"Robot 3\": policy_updates * 11\n}\n# Just for the sake of example, assuming a fixed standard deviation for all robots\nstd_deviation = 5\n\n# Setup a figure\nplt.figure()\n\n# Plot each robot's data\nfor robot, color in zip(means.keys(), ['red', 'blue', 'orange']):\n    plt.plot(policy_updates, means[robot], label=robot, color=color)\n    plt.fill_between(policy_updates, means[robot] - std_deviation, means[robot] + std_deviation, color=color, alpha=0.2)\n\n# Adding labels and title\nplt.xlabel(\"Policy Updates\")\nplt.ylabel(\"Reward\")\nplt.title(\"Gaussian Plots for Rewards Over All Experiments Over the 3 Robots\")\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig(\"gaussian_plots.png\", dpi=300)\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2207.00721/rewards_combined_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2207.00721/REPS_2_rho_theta_policy_0.png",
        "caption": "Convergence of parameters for a single Robot.",
        "source": "icra/2207.00721/root.tex",
        "arxiv_id": "icra/2207.00721",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# This is mock data since the actual data is unknown\n# Create some mock data for the x-axis (policy updates)\npolicy_updates = np.linspace(1, 6, 100)\n\n# Create some mock data for the y-axis of each subplot\nreward = policy_updates ** 2\ntheta1 = np.sin(policy_updates) / policy_updates\ntheta2 = np.cos(policy_updates) / (policy_updates + 0.1)\ntheta3 = np.log(policy_updates + 1)\ntheta4 = np.exp(-policy_updates / 3)\n\n# Create confidence intervals around the mean values for illustrative purposes\nreward_conf = reward * 0.1\ntheta1_conf = theta1 * 0.3\ntheta2_conf = theta2 * 0.2\ntheta3_conf = theta3 * 0.15\ntheta4_conf = theta4 * 0.25\n\n# Create the figure and axes\nfig, axs = plt.subplots(4, 1, figsize=(8,10), sharex=True)\n\n# Plot each line with confidence intervals\naxs[0].plot(policy_updates, reward, color='blue')\naxs[0].fill_between(policy_updates, reward - reward_conf, reward + reward_conf, color='blue', alpha=0.2)\naxs[0].set_ylabel('Reward')\n\naxs[1].plot(policy_updates, theta1, color='orange')\naxs[1].fill_between(policy_updates, theta1 - theta1_conf, theta1 + theta1_conf, color='orange', alpha=0.2)\naxs[1].set_ylabel('theta1')\n\naxs[2].plot(policy_updates, theta2, color='green')\naxs[2].fill_between(policy_updates, theta2 - theta2_conf, theta2 + theta2_conf, color='green', alpha=0.2)\naxs[2].set_ylabel('theta2')\n\naxs[3].plot(policy_updates, theta3, color='red')\naxs[3].fill_between(policy_updates, theta3 - theta3_conf, theta3 + theta3_conf, color='red', alpha=0.2)\naxs[3].set_ylabel('theta3')\n\n# Set the X-axis label\naxs[-1].set_xlabel('Policy Updates')\n\n# Adjust layout to ensure proper spacing\nplt.tight_layout()\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('convergence_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2207.00721/REPS_2_rho_theta_policy_0_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2207.00721/rewards_combined.png",
        "caption": "Gaussian plots of rewards across different robots.",
        "source": "icra/2207.00721/root.tex",
        "arxiv_id": "icra/2207.00721",
        "llava_output": "1. The image is a scatter plot, showing the relationship between the number of policy updates and the reward received by the robots.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (replace with your data)\npolicy_updates = np.arange(1, 9)\nmean_rewards_robot1 = np.random.normal(50, 15, len(policy_updates))\nmean_rewards_robot2 = np.random.normal(60, 20, len(policy_updates))\nmean_rewards_robot3 = np.random.normal(70, 25, len(policy_updates))\n\n# Standard deviation (example - replace with your data)\nstd_robot1 = np.random.normal(5, 2, len(policy_updates))\nstd_robot2 = np.random.normal(5, 2, len(policy_updates))\nstd_robot3 = np.random.normal(5, 2, len(policy_updates))\n\n# Plot the mean reward lines for each robot\nplt.plot(policy_updates, mean_rewards_robot1, label='Robot 1', color='red')\nplt.plot(policy_updates, mean_rewards_robot2, label='Robot 2', color='green')\nplt.plot(policy_updates, mean_rewards_robot3, label='Robot 3', color='orange')\n\n# Fill between the mean and standard deviation for the Gaussian plots\nplt.fill_between(policy_updates, mean_rewards_robot1 - std_robot1, mean_rewards_robot1 + std_robot1, alpha=0.3, color='red')\nplt.fill_between(policy_updates, mean_rewards_robot2 - std_robot2, mean_rewards_robot2 + std_robot2, alpha=0.3, color='green')\nplt.fill_between(policy_updates, mean_rewards_robot3 - std_robot3, mean_rewards_robot3 + std_robot3, alpha=0.3, color='orange')\n\n# Labels and title\nplt.xlabel('Policy Updates')\nplt.ylabel('Reward')\nplt.title('Gaussian plots of rewards across different robots')\n\n# Legend\nplt.legend()\n\n# Save the figure with 300 dpi\nplt.savefig('gaussian_plots.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2207.00721/rewards_combined_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "icra_figures/2207.00721/TPUvsPP.png",
        "caption": "The comparison between end-effectors that are printed with TPU (a and b) and PP (c and d). Radius of the circle varies between 30 mm and 10 mm, colors indicating different radii (a and c). As the distance between the stylus pen and the touchpad reduces, the shape changes due to the compliance of the end-effector (b and d). The effect of the material is clearly observed. Since the PP end-effector cannot conform to the environment as well as TPU end-effector, it snapped off of the forearms, hence resulting with a non-uniform shape (green line).",
        "source": "icra/2207.00721/root.tex",
        "arxiv_id": "icra/2207.00721",
        "llava_output": "The image is a comparison between end-effectors that are printed with TPU and PP. The end-effectors are shown in different positions, with the radius of the circle varying between 30 mm and 10 mm. The colors indicate different radii, and the shapes of the end-",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to generate example data. Replace this with your actual data.\ndef generate_data(radius, deformation=0, points=100):\n    angles = np.linspace(0, 2 * np.pi, points)\n    x = radius * np.cos(angles) + deformation*np.random.rand(points)\n    y = radius * np.sin(angles) + deformation*np.random.rand(points)\n    return x, y\n\n# Plotting code\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\nradii = [10, 15, 20, 25, 30]\ndeformations = [0, 1, 2, 3, 4, 5, 6]\ncolors = ['blue', 'green', 'red', 'purple', 'orange']\ndeformation_colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink']\n\n# Upper left plot (a)\nfor i, radius in enumerate(radii):\n    x, y = generate_data(radius)\n    axs[0, 0].plot(x, y, label=f'r={radius} mm', color=colors[i])\n\n# Upper right plot (b)\nfor i, defm in enumerate(deformations):\n    x, y = generate_data(25, deformation=defm)  # Assuming 25 is the base radius for deformation\n    axs[0, 1].plot(x, y, label=f'z={-defm} mm', color=deformation_colors[i])\n\n# Lower left plot (c)\nfor i, radius in enumerate(radii):\n    x, y = generate_data(radius)\n    axs[1, 0].plot(x, y, label=f'r={radius} mm', color=colors[i])\n\n# Lower right plot (d)\nfor i, defm in enumerate(deformations[:3]):  # Assuming we have less data points here\n    x, y = generate_data(25, deformation=defm)  # Assuming 25 is the base radius for deformation\n    axs[1, 1].plot(x, y, label=f'z={-defm} mm', color=deformation_colors[i])\n\n# Set labels, titles, legend\nfor i, ax in enumerate(axs.flat):\n    ax.set(xlabel='x [mm]', ylabel='y [mm]')\n    ax.legend()\n    ax.set_aspect('equal', adjustable='datalim')\n    ax.grid(True)\n    ax.set_title(f'({chr(97+i)})')\n\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('comparison_plot.png', dpi=300)\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2207.00721/TPUvsPP_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 1,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "icra_figures/2209.04346/PacejkaFit.png",
        "caption": "Data points obtained from the steady-state cornering experiment and the resulting model fit of the Pacejka model. Outliers are marked red, inliers green, and the model prediction is shown in blue for a fixed load of 16.4\\,N on the front axle and 18.6\\,N on the rear axle.",
        "source": "icra/2209.04346/main.tex",
        "arxiv_id": "icra/2209.04346",
        "llava_output": "The image is a scatter plot that presents the results of the steady-state cornering experiment. The scatter plot shows the data points obtained from the experiment, with the Pacejka model's prediction in blue. The outliers are marked red, while the inliers are marked green. The plot is used to",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (you should replace this with your actual data)\nalpha = np.linspace(-0.6, 0.6, 100)\nf_y_front_inliers = np.sin(alpha) + np.random.normal(0, 1, alpha.size)\nf_y_rear_inliers = np.sin(alpha) + np.random.normal(0, 1, alpha.size)\nf_y_front_outliers = np.random.normal(0, 20, int(alpha.size / 10))\nf_y_rear_outliers = np.random.normal(0, 20, int(alpha.size / 10))\nalpha_outliers = np.random.uniform(-0.6, 0.6, int(alpha.size / 10))\nfront_model_prediction = np.sin(alpha)\nrear_model_prediction = np.sin(alpha)\n\n# Create subplots\nfig, axs = plt.subplots(2, 1, figsize=(10, 7.5)) # Adjust the figure size as needed\n\n# Plot for front tires\naxs[0].scatter(alpha, f_y_front_inliers, color='green', label='Inliers')\naxs[0].scatter(alpha_outliers, f_y_front_outliers, color='red', label='Outliers')\naxs[0].plot(alpha, front_model_prediction, color='blue', label='Model')\naxs[0].set_title('Front tires')\naxs[0].set_xlabel(r'$\\alpha$ [rad]')\naxs[0].set_ylabel(r'$F_y$ [N]')\n\n# Plot for rear tires\naxs[1].scatter(alpha, f_y_rear_inliers, color='green')\naxs[1].scatter(alpha_outliers, f_y_rear_outliers, color='red')\naxs[1].plot(alpha, rear_model_prediction, color='blue')\naxs[1].set_title('Rear tires')\naxs[1].set_xlabel(r'$\\alpha$ [rad]')\naxs[1].set_ylabel(r'$F_y$ [N]')\n\n# Fine-tune layout, show legends, and save\nfig.tight_layout()\naxs[0].legend()\naxs[1].legend()\nplt.savefig('tire_model_fit.png', dpi=300)  # Save as PNG with 300 DPI\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2209.04346/PacejkaFit_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2209.07003/guide_point.png",
        "caption": "Illustration of circle-based guide-point assignment. For the given collision trajectory, we first find the collision control points and search the collision-free paths. The guide points shown as purple points are the intersections between circle-based raycasting and the searched paths.",
        "source": "icra/2209.07003/main.tex",
        "arxiv_id": "icra/2209.07003",
        "llava_output": "The image is a scatter plot showing the guide points and the collision-free paths.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Setup the figure and axis\nfig, ax = plt.subplots(figsize=(10, 7))\n\n# Trajectory points\ntrajectory_points = [\n    (1, 2), (2, 3), (3, 5), (4, 6), (5, 6.5), (6, 7), (7, 7.3), \n    (8, 7.5), (9, 8), (10, 9), (11, 10)\n]\ntrajectory_colors = [\n    'green', 'orange', 'blue', 'purple', 'purple', 'blue', 'orange',\n    'green', 'green', 'green', 'green'\n]\n\n# Draw the trajectory\nfor p, c in zip(trajectory_points, trajectory_colors):\n    ax.plot(p[0], p[1], 'o', color=c)\n\n# Draw the guiding line\nguide_line = [(5, 6.5), (6, 7), (7, 7.3), (8, 7.5)]\nfor p1, p2 in zip(guide_line[:-1], guide_line[1:]):\n    ax.plot([p1[0], p2[0]], [p1[1], p2[1]], color='orange')\n\n# Draw the static obstacle\nstatic_obstacle = patches.Rectangle((5, 1), 3, 4, linewidth=1, edgecolor='black', facecolor='grey', label='Static Obstacle')\nax.add_patch(static_obstacle)\n\n# Draw guide point rays and intersections\nguide_point_rays = [(7, 1), (7, 3), (6, 3), (5, 3), (6, 1)]\nfor p in guide_point_rays:\n    ax.plot(p[0], p[1], 'o', color='purple')\n    ax.plot([p[0], 7], [p[1], 7.3], 'r--')\n\n# Add arrows and \u03b8 labels\nax.annotate('', xy=(7, 7.3), xytext=(6, 7), arrowprops=dict(arrowstyle='->', lw=1))\nax.annotate('', xy=(7, 7.3), xytext=(7, 3), arrowprops=dict(arrowstyle='->', lw=1))\nax.text(6.5, 7.1, r'$\\theta_{direct}$', color='black')\nax.text(7.1, 5, r'$\\theta_{indirect}$', color='black')\n\n# Final touches\nax.set_xlim(0, 12)\nax.set_ylim(0, 11)\nax.set_aspect('equal')\nplt.axis('off')\nplt.legend()\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2209.07003/guide_point_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "icra_figures/2209.07003/runtime.png",
        "caption": "The recorded average runtime for each component of our system. The entire system is able to run in real-time by the onboard computer.",
        "source": "icra/2209.07003/main.tex",
        "arxiv_id": "icra/2209.07003",
        "llava_output": "The image is a bar chart showing the average runtime for each component of a system. The chart is divided into three sections, each representing a different component. The chart is labeled with the component name and the corresponding average runtime in milliseconds.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the image\ncomponents = ['Obstacle Tracking', 'Mapping', 'Planning']\ni7_10750H = [12, 7, 10]  # Average runtimes for i7-10750H\nXaiver_NX = [20, 13, 4]   # Average runtimes for Xaiver NX\n\nbar_width = 0.35  # Width of the bars\nindex = range(len(components))\n\n# Create the bars\nfig, ax = plt.subplots()\nbar1 = ax.bar(index, i7_10750H, bar_width, label='i7-10750H', color='blue')\nbar2 = ax.bar([i + bar_width for i in index], Xaiver_NX, bar_width, label='Xaiver NX', color='orange')\n\n# Add labels, title and custom x-axis tick labels, etc.\nax.set_xlabel('Component')\nax.set_ylabel('Runtime [ms]')\nax.set_title('The recorded average runtime for each component of our system')\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(components)\nax.legend()\n\n# Adding the text labels on top of the bars\ndef add_labels(bars):\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate('{}'.format(height) + 'ms',\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nadd_labels(bar1)\nadd_labels(bar2)\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('bar_chart.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2209.07003/runtime_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 2,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2209.07003/vel_plot.png",
        "caption": "The velocity profile of a physical flight test for each axis. The velocity data are obtained using the onboard visual-inertial state estimation.",
        "source": "icra/2209.07003/main.tex",
        "arxiv_id": "icra/2209.07003",
        "llava_output": "The image is a line chart that presents the velocity profile of a physical flight test for each axis. The velocity data are obtained using the onboard visual-inertial state estimation. The chart shows the velocity of the flight test for the x, y, and z axes, with each axis represented by a different",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Example data, replace these with your actual velocity data\ntime = np.linspace(0, 18, 100)  # A sequence of time instances\nvx = np.sin(time)               # Replace with actual Vx data\nvy = np.cos(time)               # Replace with actual Vy data\nvz = np.sin(2 * time)           # Replace with actual Vz data\nvmagnitude = np.sqrt(vx**2 + vy**2 + vz**2)  # Calculate the magnitude of the velocity vector\n\n# Create the plot\nplt.figure()\nplt.plot(time, vx, label='Vx', color='blue')\nplt.plot(time, vy, label='Vy', color='orange')\nplt.plot(time, vz, label='Vz', color='green')\nplt.plot(time, vmagnitude, label='|V|', color='red')\n\n# Label the axes\nplt.xlabel('Time [s]')\nplt.ylabel('Velocity [m/s]')\n\n# Add a legend\nplt.legend()\n\n# Optionally, adjust the axes limits if necessary\nplt.xlim(0, time[-1])\nplt.ylim(-2, 2)\n\n# Save the figure with 300 DPI\nplt.savefig('velocity_profile.png', dpi=300)\n\n# Show the plot if desired\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2209.07003/vel_plot_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2209.08258/Prediction_Comparison.png",
        "caption": "Comparison of the failure ratio between our proposed method and the linear predictor in three different simulation environments.",
        "source": "icra/2209.08258/main.tex",
        "arxiv_id": "icra/2209.08258",
        "llava_output": "The image is a line chart that shows the failure ratio between the proposed method and the linear predictor in three different simulation environments.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data from the plot\nenvironments = ['Env. C', 'Env. B', 'Env. A']  # Order reversed for example\nfailure_ratio_proposed = [5.15, 11.6, 15.93]\nfailure_ratio_linear = [32.4, 46.7, 57.6]\n\n# Create horizontal bar chart\nfig, ax = plt.subplots()\nbar_width = 0.35\nindex = range(len(environments))\n\nbar1 = ax.barh(index, failure_ratio_proposed, bar_width, label='Environment-aware Prediction', color='red')\nbar2 = ax.barh(index, failure_ratio_linear, bar_width, left=failure_ratio_proposed, label='Linear Trajectory Prediction', color='blue')\n\n# Add the text for the failure ratios\nfor i in range(len(environments)):\n    ax.text(failure_ratio_proposed[i] / 2, i, f'{failure_ratio_proposed[i]}%', va='center', ha='center', color='white')\n    ax.text(failure_ratio_proposed[i] + failure_ratio_linear[i] / 2, i, f'{failure_ratio_linear[i]}%', va='center', ha='center', color='white')\n\n# Setting the labels and title\nax.set_xlabel('Failure Ratio (%)')\nax.set_yticks(index)\nax.set_yticklabels(environments)\nax.invert_yaxis()  # To match the order as in the provided plot\nax.set_xlim(0, 70)\n\n# Adding a legend\nplt.legend(loc=\"lower right\")\n\n# Saving the plot\nplt.tight_layout()\nplt.savefig('failure_ratio_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2209.08258/Prediction_Comparison_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2210.14055/blocks_world_comparison_latest_errbars_withgreedy.png",
        "caption": "Figure shows solve rate as a function of planning time. Table reports percentage of problems solved within 90 second timeout. All variants of $\\lifted$ use the LevinTS priority function with the learned policy. We report the mean and standard deviation across 5 random seeds for each method.",
        "source": "icra/2210.14055/root.tex",
        "arxiv_id": "icra/2210.14055",
        "llava_output": "The image is a line chart that shows the solve rate as a function of planning time.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Replace these arrays with the actual data to plot\ntime = np.linspace(0, 90, 91)  # Assuming data is measured every second\n\n# Placeholder data (randomly generated to resemble the graph's shape)\nnp.random.seed(0)\nadaptive = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\ninformed = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\nlazy_beam1 = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\nlazy_beam10 = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\nlazy_bfs = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\npolicy_only = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\nsearch_only = np.clip(np.cumsum(np.random.rand(91)), 0, 100)\n\n# Create the figure and the line that we will manipulate\nfig, ax = plt.subplots()\nax.plot(time, adaptive, label='adaptive')\nax.plot(time, informed, label='informed')\nax.plot(time, lazy_beam1, linestyle='-', label='LAZY(beam1)')\nax.plot(time, lazy_beam10, linestyle='--', label='LAZY(beam10)')\nax.plot(time, lazy_bfs, label='LAZY(bfs)')\nax.plot(time, policy_only, linestyle='-.', label='policy-only')\nax.plot(time, search_only, linestyle=':', label='search-only')\n\n# Add labels and legend\nax.set_xlabel('Time (s)')\nax.set_ylabel('% Solved')\nax.set_xlim([0, 90])\nax.set_ylim([0, 100])\nax.legend()\n\n# Save the figure\nplt.savefig('solve_rate.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2210.14055/blocks_world_comparison_latest_errbars_withgreedy_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 5,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "icra_figures/2210.14055/blocks_world_comparison_latest_errbars_withgreedy.png",
        "caption": "Figure shows solve rate as a function of planning time. Table reports percentage of problems solved within 90 second timeout. All variants of $\\lifted$ use the LevinTS priority function with the learned policy. We report the mean and standard deviation across 5 random seeds for each method.",
        "source": "icra/2210.14055/ICRA_2023_-_Camera_Ready/root.tex",
        "arxiv_id": "icra/2210.14055",
        "llava_output": "The image is a line chart showing the solve rate as a function of planning time.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder data - replace this with your actual data\ntimes = np.linspace(0, 90, 100)\nsolve_rates_adaptive = np.clip(1 - np.exp(-times / 20), 0, 1) * 100\nsolve_rates_informed = np.clip(1 - np.exp(-times / 25), 0, 1) * 100\nsolve_rates_lazy1 = np.clip(1 - np.exp(-times / 30), 0, 1) * 100\nsolve_rates_lazy10 = np.clip(1 - np.exp(-times / 35), 0, 1) * 100\nsolve_rates_lazy_bfs = np.clip(1 - np.exp(-times / 40), 0, 1) * 100\nsolve_rates_policy_only = np.clip(np.arctan(times / 50), 0, 1) * 100\nsolve_rates_search_only = np.clip(np.arctan(times / 55), 0, 1) * 100\n\n# Create the plot\nplt.figure(figsize=(12, 8))\nplt.plot(times, solve_rates_adaptive, label='adaptive', color='blue')\nplt.plot(times, solve_rates_informed, label='informed', color='orange')\nplt.plot(times, solve_rates_lazy1, label='LAZY(beam1)', color='green')\nplt.plot(times, solve_rates_lazy10, '--', label='LAZY(beam10)', color='magenta')\nplt.plot(times, solve_rates_lazy_bfs, label='LAZY(bfs)', color='purple')\nplt.plot(times, solve_rates_policy_only, '--', label='policy-only', color='gray')\nplt.plot(times, solve_rates_search_only, '--', label='search-only', color='pink')\n\n# Add details\nplt.xlabel('Time (s)')\nplt.ylabel('% Solved')\nplt.title('Solve Rate as a Function of Planning Time')\nplt.legend(loc='lower right')\n\n# Saving the plot with 300dpi\nplt.savefig('solve_rate_vs_time.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/icra/2210.14055/blocks_world_comparison_latest_errbars_withgreedy_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2106.04486/edge_plot_hashfn_time.png",
        "caption": "(a) Linear scalability with number of hash functions. (b) Linear scalability with number of edges.",
        "source": "kdd/2106.04486/sample-sigconf.tex",
        "arxiv_id": "kdd/2106.04486",
        "llava_output": "The image is a line chart that presents the results of an experiment. The chart shows the number of hash functions on the x-axis and the running time on the y-axis.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data for plotting\n# Please replace these with the actual values from the paper or your calculations\nnumber_of_hash_functions = [1, 2, 3, 4, 5]\nrunning_time_g = [5, 10, 20, 30, 40]  # Fake data for ANOEDGE-G\nrunning_time_l = [1, 1, 1, 1, 1]      # Fake data for ANOEDGE-L\nslope_g = 8.39  # Slope taken from the blue dashed line\nslope_l = 0.51  # Slope taken from the red dashed line\n\n# Plotting data\nplt.figure(figsize=(10, 7.5))  # Size to match the provided image\nplt.scatter(number_of_hash_functions, running_time_g, color='blue', marker='v', label='ANOEDGE-G')\nplt.scatter(number_of_hash_functions, running_time_l, color='red', marker='^', label='ANOEDGE-L')\nplt.plot(number_of_hash_functions, running_time_g, 'b--', label=f'slope={slope_g}')\nplt.plot(number_of_hash_functions, running_time_l, 'r-.', label=f'slope={slope_l}')\n\n# Labeling the axes\nplt.xlabel('Number of Hash Functions')\nplt.ylabel('Running Time (s)')\n\n# Add a legend\nplt.legend()\n\n# Save the figure with high resolution\nplt.savefig('plot.png', dpi=300)\n\n# Display the figure\nplt.show()",
        "code_cot": "",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2106.04486/edge_plot_hashfn_time_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2106.04486/graph_plot_topK_time.png",
        "caption": "(a) \\methodgraph-K scales linearly with factor $K$. (b) Linear scalability with number of hash functions. (c) Linear scalability with number of edges.",
        "source": "kdd/2106.04486/sample-sigconf.tex",
        "arxiv_id": "kdd/2106.04486",
        "llava_output": "The image is a line chart that presents the results of an experiment. The chart shows the running time of the algorithm as a function of the number of hash functions. The running time is plotted on the y-axis, while the number of hash functions is plotted on the x-axis. The chart also includes",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# You might need some sample data; here are dummy values:\nK = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nrunning_time = [0.42, 0.45, 0.48, 0.51, 0.54, 0.57, 0.60, 0.63, 0.66, 0.69]\n\n# Create the plot\nplt.figure()\nplt.plot(K, running_time, marker='v', linestyle='--', color='r', label='ANGRAPH-K')\n\n# Create a dashed line with a fixed slope for reference\nslope = 0.001\nintercept = running_time[0] - slope * K[0]\nreference_line = [slope * x + intercept for x in K]\nplt.plot(K, reference_line, 'r--', label='slope=0.001')\n\n# Add labels and legend\nplt.xlabel('K')\nplt.ylabel('Running Time (s)')\nplt.legend()\n\n# Save the figure with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Optionally display the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2106.04486/graph_plot_topK_time_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2112.06668/dropout_rate.png",
        "caption": "The impact of different dropout rates for CT4Rec and SASRec* on the Beauty dataset.",
        "source": "kdd/2112.06668/4-Results.tex",
        "arxiv_id": "kdd/2112.06668",
        "llava_output": "The image is a line chart showing the impact of different dropout rates for CT4Rec and SASRec on the Beauty dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Data (replace these with the actual values from your dataset)\ndropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nct4rec_ndcg = [0.15, 0.17, 0.19, 0.21, 0.20, 0.19, 0.18, 0.17, 0.16] # example values\nsasrec_ndcg = [0.14, 0.16, 0.18, 0.20, 0.22, 0.24, 0.25, 0.24, 0.23] # example values\nct4rec_hr = [0.22, 0.24, 0.26, 0.28, 0.30, 0.31, 0.30, 0.29, 0.28] # example values\nsasrec_hr = [0.21, 0.23, 0.25, 0.27, 0.29, 0.31, 0.32, 0.31, 0.30] # example values\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left plot (NDCG@10)\naxes[0].plot(dropout_rates, ct4rec_ndcg, marker='o', label='CT4Rec', color='blue')\naxes[0].plot(dropout_rates, sasrec_ndcg, marker='*', label='SASRec*', color='orange')\naxes[0].set_xlabel('dropout rate')\naxes[0].set_ylabel('NDCG@10')\naxes[0].set_title('NDCG@10')\naxes[0].legend()\n\n# Right plot (HR@10)\naxes[1].plot(dropout_rates, ct4rec_hr, marker='o', label='CT4Rec', color='blue')\naxes[1].plot(dropout_rates, sasrec_hr, marker='*', label='SASRec*', color='orange')\naxes[1].set_xlabel('dropout rate')\naxes[1].set_ylabel('HR@10')\naxes[1].set_title('HR@10')\naxes[1].legend()\n\n# Tight layout\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2112.06668/dropout_rate_direct.png",
        "eval": {
            "axes_grids_rating": 5,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2205.10053/pubmed_alpha.png",
        "caption": "Effect of $\\alpha$.",
        "source": "kdd/2205.10053/main.tex",
        "arxiv_id": "kdd/2205.10053",
        "llava_output": "The image is a scatter plot showing the effect of $\\alpha$ on the accuracy of the maskgaeae path. The plot has two lines, one in red and one in blue, representing the two different values of $\\alpha$. The data points are scattered around the lines, indicating the relationship between the accuracy and the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Placeholder accuracy values\nmaskgae_edge_mean = np.array([83, 82, 81, 82, 83, 81, 80]) # Replace with your values\nmaskgae_path_mean = np.array([82, 83, 83, 82, 84, 83, 82]) # Replace with your values\n\n# Placeholder standard deviation/error values\nmaskgae_edge_std = np.array([1, 0.5, 0.7, 0.6, 0.8, 0.5, 0.4]) # Replace with your values\nmaskgae_path_std = np.array([0.8, 0.9, 0.6, 0.7, 0.5, 0.6, 0.8]) # Replace with your values\n\n# Placeholder alpha values\nalphas = np.array([0, 2e-3, 4e-3, 6e-3, 8e-3, 10e-3, 12e-3]) # Replace with your values\n\n# Plot the lines with error bands\nplt.fill_between(alphas, maskgae_edge_mean - maskgae_edge_std, maskgae_edge_mean + maskgae_edge_std, color='blue', alpha=0.2)\nplt.fill_between(alphas, maskgae_path_mean - maskgae_path_std, maskgae_path_mean + maskgae_path_std, color='red', alpha=0.2)\n\nplt.plot(alphas, maskgae_edge_mean, 's-', color='blue', label='MaskGAE_edge')\nplt.plot(alphas, maskgae_path_mean, 'o-', color='red', label='MaskGAE_path')\n\n# Add legend\nplt.legend(loc='best')\n\n# Label the axes\nplt.xlabel(r'$\\alpha$')\nplt.ylabel('Accuracy (%)')\n\n# Add a title\nplt.title('Pubmed')\n\n# Customize the tick marks on the x-axis to match the plot in the image\nplt.xticks(alphas, labels=[f'{a:.1e}' for a in alphas])\n\n# Save the figure as a high-resolution PNG file\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2205.10053/pubmed_alpha_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2210.14309/ExpertGate.png",
        "caption": "Gate value of memorization-focused and generalization-focused experts for head (blue) and tail (orange) items. In CDN, head items put more weights on memorization-focused experts while tail items put more weights on generalization-focused experts",
        "source": "kdd/2210.14309/sample-sigconf.tex",
        "arxiv_id": "kdd/2210.14309",
        "llava_output": "The image is a line chart that presents the gate value of memorization-focused and generalization-focused experts for head (blue) and tail (orange) items.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Approximating example data values from the image\nmemorization_expert = [0.5, 0.35]  # head_gate, tail_gate values for Memorization-focused Expert\ngeneralization_expert = [0.45, 0.55]  # head_gate, tail_gate values for Generalization-focused Expert\n\n# Names of the bars\nbar_names = ['Memorization-focused Expert', 'Generalization-focused Expert']\n\n# Index for the bars\nbar_index = range(len(bar_names))\n\n# Bar width\nbar_width = 0.35\n\n# Create bars\nplt.bar(bar_index, memorization_expert, width=bar_width, label='head_gate', color='blue')\nplt.bar([x + bar_width for x in bar_index], generalization_expert, width=bar_width, label='tail_gate', color='orange')\n\n# Add title and labels\nplt.title('Mean for gate values')\nplt.xlabel('')\nplt.ylabel('')\n\n# Add xticks\nplt.xticks([r + bar_width / 2 for r in range(len(bar_names))], bar_names)\n\n# Create legend & Show graphic\nplt.legend()\n\n# Save the figure\nplt.savefig('gate_values.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2210.14309/ExpertGate_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2210.14309/gammaNDCG.png",
        "caption": "The NDCG@50 of head, tail and overall performance w.r.t different $\\gamma$.",
        "source": "kdd/2210.14309/sample-sigconf.tex",
        "arxiv_id": "kdd/2210.14309",
        "llava_output": "ndcg@50 of head, tail and overall performance w.r.t different gamma.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data - replace with your actual data\ngamma_values = [0, 5, 10, 15, 20, 25, 30]\noverall_performance = [8, 9, 10, 10, 10, 10, 10]\nhead_performance = [1, 4, 9, 9, 9, 9, 9]\ntail_performance = [2, 3, 2, 2, 2, 2, 2]\n\n# Plotting each line\nplt.plot(gamma_values, overall_performance, marker='s', color='blue', label='Overall')\nplt.plot(gamma_values, head_performance, marker='o', color='red', label='Head')\nplt.plot(gamma_values, tail_performance, marker='^', color='green', label='Tail')\n\n# Labeling axes\nplt.xlabel('$\\gamma$')\nplt.ylabel('NDCG@50')\nplt.title('The NDCG@50 of head, tail and overall performance w.r.t different $\\gamma$')\n\n# Adding a legend\nplt.legend()\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2210.14309/gammaNDCG_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2302.01416/mae_rmse_per_domain.png",
        "caption": "RMSE(top) and MAE(bottom) of GLM(blue) and our multimodal neural network(orange) evaluated on each domain.",
        "source": "kdd/2302.01416/preprint.tex",
        "arxiv_id": "kdd/2302.01416",
        "llava_output": "The image is a scatter plot, showing the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) of two different models, a GLM model and a multimodal neural network, evaluated on each domain. The scatter plot is color-coded, with blue representing the",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Replace these with the actual RMSE and MAE values for each model\nrmse_glm = np.random.rand(25) # Mock values, replace with actual data\nrmse_dl = np.random.rand(25) # Mock values, replace with actual data\n\nmae_glm = np.random.rand(25) # Mock values, replace with actual data\nmae_dl = np.random.rand(25) # Mock values, replace with actual data\n\n# Indexes for bar positions\nindices = np.arange(len(rmse_glm))\n\n# Bar width\nwidth = 0.35\n\nfig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n\n# RMSE Plot\nax[0].bar(indices - width / 2, rmse_glm, width, color='blue', label='GLM')\nax[0].bar(indices + width / 2, rmse_dl, width, color='orange', label='Best DL model')\nax[0].set_ylabel('RMSE of locations')\nax[0].set_title('RMSE (top) and MAE (bottom)')\nax[0].legend()\n\n# MAE Plot\nax[1].bar(indices - width / 2, mae_glm, width, color='blue', label='GLM')\nax[1].bar(indices + width / 2, mae_dl, width, color='orange', label='Best DL model')\nax[1].set_ylabel('MAE of locations')\nax[1].set_xlabel('Location index')\n\n# Setting x-axis ticks\nax[1].set_xticks(indices)\nax[1].legend()\n\nplt.tight_layout()\n\n# Save the figure as a png file with 300 dpi\nplt.savefig('rmse_mae_comparison.png', dpi=300)\n\n# Show the plot (omit if running in a script)\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.01416/mae_rmse_per_domain_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2302.02592/figure_5b.png",
        "caption": "The curves of cumulative reward and delivery completion rate during training (30,000 episodes).",
        "source": "kdd/2302.02592/src-projname.tex",
        "arxiv_id": "kdd/2302.02592",
        "llava_output": "The image is a line chart that shows the cumulative reward and delivery completion rate during training.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a dummy dataset\nepisodes = np.linspace(0, 30000, 1000)\ncumulative_rewards_mean = np.sin(episodes / 5000) * (3000 + episodes / 20)\ncumulative_rewards_std = np.random.uniform(100, 500, size=cumulative_rewards_mean.shape)\n\n# Normalize the x-axis to the scale seen in the original plot\nnormalized_episodes = episodes / 10000\n\n# Calculate the upper and lower bounds for the confidence interval\nupper_bound = cumulative_rewards_mean + cumulative_rewards_std\nlower_bound = cumulative_rewards_mean - cumulative_rewards_std\n\n# Plot the mean cumulative reward\nplt.plot(normalized_episodes, cumulative_rewards_mean, color='blue')\n\n# Plot the confidence interval area\nplt.fill_between(normalized_episodes, lower_bound, upper_bound, color='blue', alpha=0.3)\n\n# Customize the plot\nplt.title('Cumulative Reward')\nplt.xlabel('Episode')\nplt.ylabel('Cumulative Reward')\nplt.grid(True)\n\n# Save the plot to a PNG file with 300 DPI\nplt.savefig('cumulative_reward_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.02592/figure_5b_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2302.02592/figure6.png",
        "caption": "At different training episodes, the trends of cumulative reward at all 288 decision steps.",
        "source": "kdd/2302.02592/src-projname.tex",
        "arxiv_id": "kdd/2302.02592",
        "llava_output": "The image is a Line Chart that shows the trends of cumulative reward at all 288 decision steps at different training episodes.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to generate dummy data and confidence intervals\ndef generate_dummy_data_and_ci(episodes, steps):\n    np.random.seed(0)\n    data = np.cumsum(np.random.randn(steps) + episodes / 10000.0)\n    ci = 1.96 * np.std(data)/np.sqrt(np.arange(1,steps+1))\n    return data, data-ci, data+ci\n\n# Training episodes\nepisodes = [0, 0.5e4, 1.5e4, 2e4, 3e4]\n\n# Create some dummy data\nnp.random.seed(0)\nsteps = 100\ntimes = np.linspace(1, steps, steps)\n\n# Plot the data\nplt.figure(figsize=(10, 8))\nfor i, ep in enumerate(episodes):\n    data, lower, upper = generate_dummy_data_and_ci(ep, steps)\n    plt.plot(times, data, label=f'Episode={ep}')\n    plt.fill_between(times, lower, upper, alpha=0.2)\n\n# Log scale for x-axis\nplt.xscale('log')\n\n# Apply the customization for the plot: labels, title, legend, etc.\nplt.xlabel('Decision Step')\nplt.ylabel('Cumulative Reward')\nplt.title('Cumulative Reward at Different Training Episodes')\nplt.legend()\n\n# Save the figure as a .png file with 300 dpi\nplt.savefig('cumulative_reward_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.02592/figure6_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2302.02592/figure7a.png",
        "caption": "Effects of maximizing traffic value.",
        "source": "kdd/2302.02592/src-projname.tex",
        "arxiv_id": "kdd/2302.02592",
        "llava_output": "The image is a Line Chart showing the effects of maximizing traffic value.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Synthetic data to mimic the sample plot\nnp.random.seed(0)\ntime = np.linspace(0, 2.5, 100) * 100  # X-axis data\nctr_base = np.log(time + 1) / 200 + 0.045  # Y-axis data for the baseline\nctr_variant = ctr_base + np.random.uniform(-0.001, 0.001, size=time.shape)  # Y-axis data for the variant\n\n# Adding some variability to mimic the confidence intervals\nvariability = np.random.uniform(0.0005, 0.002, size=time.shape)\nlower_bound_base = ctr_base - variability\nupper_bound_base = ctr_base + variability\nlower_bound_variant = ctr_variant - variability\nupper_bound_variant = ctr_variant + variability\n\n# Plotting the lines\nplt.plot(time, ctr_base, label='RLTP-w/o-r4', color='green')\nplt.plot(time, ctr_variant, label='RLTP', color='blue')\n\n# Filling the area for the confidence intervals\nplt.fill_between(time, lower_bound_base, upper_bound_base, color='green', alpha=0.3)\nplt.fill_between(time, lower_bound_variant, upper_bound_variant, color='blue', alpha=0.3)\n\n# Labels & Title\nplt.xlabel('Time Window')\nplt.ylabel('CTR of Selected Traffic')\n\n# Legend\nplt.legend(loc='upper left')\n\n# Saving the plot with 300 DPI\nplt.savefig('traffic_value_maximizing.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.02592/figure7a_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2302.05549/running_time_v2.png",
        "caption": "Running Time of DistMS in Spark on a 10 Million dataset",
        "source": "kdd/2302.05549/sample-sigconf.tex",
        "arxiv_id": "kdd/2302.05549",
        "llava_output": "The image is a line chart showing the running time of DistMS in Spark on a 10 million dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data. You need to replace these with the actual values from your dataset.\nworkers = [10, 20, 40, 80, 160]  # This represents the number of e2-highmem-8 workers.\nrunning_time = [30, 22, 15, 10, 5]  # This represents the running time in minutes.\n\n# Create the plot\nplt.figure()\nplt.plot(workers, running_time, marker='o', color='blue', linewidth=2, markersize=8)\nplt.scatter(workers, running_time, color='orange')\n\n# Add title and labels\nplt.title('Running Time of DistMS in Spark')\nplt.xlabel('number of e2-highmem-8 workers')\nplt.ylabel('running time (minutes)')\n\n# Save the figure\nplt.savefig('distms_spark_runtime.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": null,
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.05549/running_time_v2_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "kdd_figures/2302.09178/xformer-comparison.png",
        "caption": "A comparison of AdamW, Adagrad and Adagrad with Clippy on the task for English to German translation.",
        "source": "kdd/2302.09178/appendix.tex",
        "arxiv_id": "kdd/2302.09178",
        "llava_output": "The image is a line chart showing the performance of three different algorithms for a machine learning task. The three algorithms are AdamW, Adagrad, and Adagrad with Clippy. The chart displays the learning rate and the number of steps taken by each algorithm.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data - Replace this with actual data from your dataset\niterations = np.linspace(0, 500000, 100)  # Simulated data, 100 points from 0 to 500000\nadamw_data = np.exp(-iterations / 150000)  # Simulated exponential decay\nadagrad_data = np.exp(-iterations / 120000)  # Simulated exponential decay with different rate\nclippy_data = np.exp(-iterations / 180000)  # Simulated exponential decay\n\n# Plotting the data\nplt.figure()\n\n# Plot each line with respective labels\nplt.plot(iterations, adamw_data, label=\"AdamW, warmup_steps=40000, learning_rate=0.1\")\nplt.plot(iterations, adagrad_data, label=\"Adagrad, warmup_steps=10000, learning_rate=0.1\")\nplt.plot(iterations, clippy_data, label=\"ClippyAdagrad, warmup_steps=10000, learning_rate=3.0\")\n\n# Customizing plot (you will need to adjust colors and styles to match your plot)\nplt.xlabel('iterations')\nplt.ylabel('validation_bleu')\nplt.title('Comparison of Optimizers')\nplt.legend()\n\n# Saving the figure\nplt.savefig('optimizer_comparison.png', dpi=300)\n\n# Show the plot if desired\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.09178/xformer-comparison_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2302.09178/divergence.png",
        "caption": "Example of loss divergence in our model and its impact on training loss (top) and AUC (bottom). In this example, model-a's loss micro-diverged then recovered, whereas model-b's loss fully-diverged.",
        "source": "kdd/2302.09178/background.tex",
        "arxiv_id": "kdd/2302.09178",
        "llava_output": "1. Line Chart</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming some synthetic data for illustration purposes\ntrain_steps = np.arange(0, 1000, 1)\nmodel_a_loss = np.where(train_steps < 200, 1.02 + np.random.normal(0, 0.01, size=train_steps.shape), 1.0)\nmodel_a_loss[250:255] = 1.4  # Adding a spike to represent the micro-divergence\nmodel_b_loss = 1.1 + np.where(train_steps > 800, (train_steps - 800) / 200, np.random.normal(0, 0.01, size=train_steps.shape))\nmodel_a_auc = np.where(train_steps < 200, 0.6 + train_steps * 0.0001, 0.62)\nmodel_b_auc = 0.6\n\n# Create subplots\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n\n# Plot the training loss for both models\nax1.plot(train_steps, model_a_loss, label='model-a', color='blue')\nax1.plot(train_steps, model_b_loss, label='model-b', color='orange')\nax1.set_ylabel('Train Loss')\nax1.legend(loc='upper right')\nax1.grid(True)\n\n# Plot the AUC for both models\nax2.plot(train_steps, model_a_auc, label='model-a', color='blue')\nax2.plot(train_steps, [model_b_auc] * len(train_steps), label='model-b', color='orange')\nax2.set_ylabel('Train AUC')\nax2.set_xlabel('Train Steps')\nax2.legend(loc='lower right')\nax2.grid(True)\n\n# Annotating the plot with the text from the image\ndx = 50  # Adjust the position of the text as needed\ndy = -0.05\nannotations = ['Loss beginning to\\nmicro-diverge for model-a', 'Loss recovered from\\nmicro-diverge for model-a',\n               'Loss beginning to\\nfully-diverge for model-b']\nx_positions = [200, 255, 800]\nfor i, ann in enumerate(annotations):\n    ax1.annotate(ann, (x_positions[i], model_a_loss.min()-dy), textcoords=\"offset points\", xytext=(-dx,0), ha='center')\n\n# Save the plot as a .png file with 300 dpi\nplt.tight_layout()\nplt.savefig('loss_auc_plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.09178/divergence_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "No",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "kdd_figures/2302.11159/dtw5.png",
        "caption": "Different Adjacency Graphs",
        "source": "kdd/2302.11159/methods.tex",
        "arxiv_id": "kdd/2302.11159",
        "llava_output": "The image is a scatter plot, showing the relationship between two variables.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Generating a random adjacency matrix with a pattern that somewhat resembles the image\nnp.random.seed(0)  # For reproducibility\nsize = 128\nmatrix = np.zeros((size, size))\n\n# Create a pattern similar to the one in the image (diagonal with clusters)\nfor i in range(size):\n    for j in range(i, size):\n        if i == j or np.random.rand() > 0.98:\n            cluster_size = np.random.randint(1, 4)\n            matrix[i:i+cluster_size, j:j+cluster_size] = 1\n\n# Symmetrize the matrix\nmatrix = np.triu(matrix) + np.triu(matrix, 1).T\n\n# Plotting the adjacency matrix\nplt.figure(figsize=(8, 8))\ncmap = mcolors.ListedColormap(['purple', 'yellow'])\nplt.imshow(matrix, cmap=cmap, interpolation='none')\nplt.colorbar()\nplt.axis('on')  # Include axis (you can turn it off with 'off')\n\n# Hiding the ticks while showing the axes.\nplt.xticks([])\nplt.yticks([])\n\n# Saving the figure as a PNG file with 300 DPI\nplt.savefig('adjacency_graphs.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.11159/dtw5_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2302.13522/full_gpu_stats.png",
        "caption": "GPU streaming multiprocessor utilization for IGB.",
        "source": "kdd/2302.13522/experiment.tex",
        "arxiv_id": "kdd/2302.13522",
        "llava_output": "The image is a Line Chart that shows GPU streaming multiprocessor utilization for IGB.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate random data to simulate GPU utilization\nnp.random.seed(0)\nmain_data = np.random.rand(1200) * 100\ninset_data = main_data[600:700]\n\n# Create the main plot\nfig, ax_main = plt.subplots(figsize=(10, 3))\nax_main.bar(np.arange(len(main_data)), main_data, color='blue')\nax_main.set_xlabel('Time in seconds')\nax_main.set_ylabel('SM Usage %')\nax_main.set_xlim(0, len(main_data))\nax_main.set_ylim(0, 100)\n\n# Create an inset plot with a portion of the main data\nax_inset = fig.add_axes([0.5, 0.5, 0.47, 0.47])  # This will need to be adjusted based on the figure dimensions\nax_inset.bar(np.arange(len(inset_data)), inset_data, color='blue')\nax_inset.set_xlim(0, len(inset_data))\nax_inset.set_ylim(0, 100)\n\n# Optional: Add a red boundary to the inset data on the main plot\nax_main.add_patch(plt.Rectangle((600, 0), 100, 100, fill=False, edgecolor='red', lw=2))\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('gpu_utilization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Histogram",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.13522/full_gpu_stats_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2302.13522/tiny_gpu_stats.png",
        "caption": "GPU streaming multiprocessor utilization for IGB-tiny.",
        "source": "kdd/2302.13522/experiment.tex",
        "arxiv_id": "kdd/2302.13522",
        "llava_output": "The image is a Line Chart showing GPU streaming multiprocessor utilization for IGB-tiny.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data\nx = np.linspace(0, 200, 200)\ny = np.random.normal(loc=40, scale=5, size=200)\n\n# Create main figure and axis\nfig, ax1 = plt.subplots(figsize=(10, 5))\n\n# Plot the main figure\nax1.plot(x, y, color='blue')\nax1.set_xlim(0, 200)\nax1.set_ylim(0, 100)\nax1.set_xlabel('Time in seconds')\nax1.set_ylabel('SM usage %')\nax1.set_title('GPU streaming multiprocessor utilization for IGB-tiny')\n\n# Inset for the zoomed area \nax2 = fig.add_axes([0.5, 0.5, 0.4, 0.4])  # You will need to adjust these values to match the location in your original plot\nax2.plot(x, y, color='blue')\nax2.set_xlim(90, 110)  # Set the x-limits to the zoomed-in region\nax2.set_ylim(30, 50)  # Set the y-limits to the zoomed-in region\n\n# Drawing the lines connecting the two plots\ncon_x = [90, 90, 110, 110]\ncon_y = [0, 50, 50, 0]\nline_color = 'red'\nline_width = 2\n\nax1.plot(con_x[:2], con_y[:2], line_color, linewidth=line_width)\nax1.plot(con_x[2:], con_y[2:], line_color, linewidth=line_width)\nax2.plot(con_x[:2], con_y[1:3], line_color, linewidth=line_width)\nax2.plot(con_x[2:], con_y[2:], line_color, linewidth=line_width)\n\n# Save the plot as a PNG file with 300dpi\nplt.savefig('gpu_utilization.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2302.13522/tiny_gpu_stats_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": null,
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2304.01166/chart1.png",
        "caption": "Results of the proposed model",
        "source": "kdd/2304.01166/Experiment_Details.tex",
        "arxiv_id": "kdd/2304.01166",
        "llava_output": "The image is a bar chart that presents the results of the proposed model.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from the image\nmodels = ['NB', 'LR', 'SVM', 'KNN', 'DT', 'RF']\nmilitary_data = [83.86, 95.63, 97.22, 98.89, 98.8, 99.49]\nunb_ci_cid_2017_data = [89.75, 96.33, 96.92, 99.26, 99.6, 99.62]\nnsl_kdd_data = [99.84, 99.99, 99.99, 99.99, 99.96, 99.96]\n\n# X locations for the groups\nind = np.arange(len(models))\nwidth = 0.25  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind - width, military_data, width, label='Military network environment Data', color='blue')\nrects2 = ax.bar(ind, unb_ci_cid_2017_data, width, label='UNB Intrusion Detection Evaluation CICID 2017', color='red')\nrects3 = ax.bar(ind + width, nsl_kdd_data, width, label='20% training data of NSL-KDD', color='yellow')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel('Models')\nax.set_ylabel('Scores')\nax.set_title('Results of the proposed model')\nax.set_xticks(ind)\nax.set_xticklabels(models)\nax.legend()\n\n# Attach a text label above each bar in rects1, displaying its height.\nfor rects in [rects1, rects2, rects3]:\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n# Save the figure as a PNG file with 300 dpi\nplt.tight_layout()\nplt.savefig('model_results.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2304.01166/chart1_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2304.01166/char21.png",
        "caption": "Correlation matrix after a proposed feature extraction method",
        "source": "kdd/2304.01166/Experiment_Details.tex",
        "arxiv_id": "kdd/2304.01166",
        "llava_output": "The image is a bar chart showing the results of a UNB Intrusion Detection Evaluation CICID 2017 experiment. The chart has several bars representing different categories, such as training data, testing data, and overall performance. The bars are labeled with the corresponding category names and",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example data (approximations from the image)\nmodels = ['NB', 'LR', 'SVM', 'KNN', 'DT', 'RF']\nUNB_CICID_2017 = [89.75, 95.33, 94.2, 90.9, 81.65, 81.9]\ntraining_data_NSL_KDD = [80.83, 87, 84, 79.9, 99.29, 99.62]\n\nn_groups = len(models)\nindex = np.arange(n_groups)\nbar_width = 0.35\n\nfig, ax = plt.subplots()\n\nbar1 = ax.bar(index, UNB_CICID_2017, bar_width, label='UNB Intrusion Detection Evaluation CICID 2017', color='blue')\nbar2 = ax.bar(index + bar_width, training_data_NSL_KDD, bar_width, label='20% training data of NSL-KDD', color='green')\n\n# Add the text for the labels, title and axes ticks\nax.set_xlabel('Models')\nax.set_ylabel('Performance')\nax.set_title('Model Performance on Two Datasets')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(models)\nax.legend()\n\n# Add data labels\ndef add_labels(bars):\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nadd_labels(bar1)\nadd_labels(bar2)\n\n# Save the figure\nplt.tight_layout()\nplt.savefig('bar_chart.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2304.01166/char21_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "kdd_figures/2305.18885/TA_mceg.png",
        "caption": "",
        "source": "kdd/2305.18885/0.0.KDD2023_full.tex",
        "arxiv_id": "kdd/2305.18885",
        "llava_output": "This is a line chart showing the distance of a moving object.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PolyCollection\n\n# Example data\n# Assuming each curve is based on Gaussian-like distributions, here's some dummy data.\nnp.random.seed(0)  # For reproducibility\nnum_curves = 6\nx = np.linspace(0, 1, 100)\nys = [np.exp(-((x - 0.5) ** 2) / (2 * (0.1 + (i / num_curves)) ** 2)) for i in range(num_curves)]\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Create a PolyCollection\nverts = []\nfor i, y in enumerate(ys):\n    verts.append(list(zip(x, y + i)))\n\npoly = PolyCollection(verts, facecolors=['purple'], alpha=0.6)\n\n# Add collection to axes\nax.add_collection(poly)\n\n# Set limits for x and y axis\nax.set_xlim([x.min(), x.max()])\nax.set_ylim([0, num_curves])\n\n# Set custom ticks on the y-axis to correspond with the 'L' levels in the original plot\nax.set_yticks(np.arange(num_curves))\nax.set_yticklabels([f\"L = {i}\" for i in range(num_curves)])\n\n# Label x-axis\nax.set_xlabel('Distance')\n\n# Remove y-ticks and set a grid only for y-axis, similar to the original plot\nax.tick_params(axis='y', which='both', length=0)\nax.yaxis.grid(True)\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2305.18885/TA_mceg_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "kdd_figures/2305.18885/TA.png",
        "caption": "TA",
        "source": "kdd/2305.18885/0.0.KDD2023_full.tex",
        "arxiv_id": "kdd/2305.18885",
        "llava_output": "300,000</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Categories and corresponding values\ncategories = ['overall', 'value', 'service', 'rooms', 'location', 'cleanliness', 'checkin', 'business']\nvalues = [32000, 30000, 31000, 29000, 30500, 28000, 18000, 15000] # These values are examples\n\n# Create the bar plot\nplt.figure(figsize=(6, 5))  # Adjust the figure size as needed\nbars = plt.bar(categories, values, color=['green'] + ['blue']*(len(values)-1))\n\n# Set the title and labels\nplt.title('TripAdvisor')\nplt.xticks(rotation=45)  # Rotate the x labels if needed\n\n# Save the plot as a .png file with 300 dpi\nplt.savefig('tripadvisor_plot.png', dpi=300, bbox_inches='tight')\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2305.18885/TA_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2306.02679/dbp_viz.png",
        "caption": "Visualization of the entity embeddings of five popular types in \\dbpfivem.",
        "source": "kdd/2306.02679/sect7_app.tex",
        "arxiv_id": "kdd/2306.02679",
        "llava_output": "1. The image is a scatter plot.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example synthetic data similar to the plot you provided\nnp.random.seed(0)\nn_points = 200\ndata = {\n    'Film': (np.random.normal(0, 0.1, size=(n_points, 2)), 'red'),\n    'City': (np.random.normal(0.2, 0.1, size=(n_points, 2)), 'blue'),\n    'Company': (np.random.normal(-0.2, 0.1, size=(n_points, 2)), 'green'),\n    'MusicalArtist': (np.random.normal(0.4, 0.1, size=(n_points, 2)), 'purple'),\n    'SoccerPlayer': (np.random.normal(-0.4, 0.1, size=(n_points, 2)), 'orange'),\n}\n\n# Create the scatter plot\nplt.figure(figsize=(10, 5))\nfor category, (points, color) in data.items():\n    plt.scatter(points[:, 0], points[:, 1], alpha=0.5, c=color, label=category, edgecolors='w')\n\n# Add the legend\nplt.legend(title='Types')\n\n# Set the title of the plot (if any)\nplt.title('Visualization of the entity embeddings of five popular types')\n\n# Set the labels for the axes\nplt.xlabel('Embedding Dimension 1')\nplt.ylabel('Embedding Dimension 2')\n\n# Save the plot to a PNG file with 300 DPI\nplt.savefig('entity_embeddings.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.02679/dbp_viz_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2306.04039/mips_vs_mol_gpu_util_mem_v1.png",
        "caption": "Infra efficiency in production: GPU utilization and peak memory scaling with serving FLOPs.",
        "source": "kdd/2306.04039/main.tex",
        "arxiv_id": "kdd/2306.04039",
        "llava_output": "The image is a line chart that shows the relationship between GPU utilization and peak memory scaling with serving FLOPs. The chart has a y-axis on the left side and a x-axis on the right side. The line on the chart represents the relationship between the two variables, with the peak memory scaling",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Example data points (you'll need to replace these with your actual data)\nserving_cost = [0.5, 1.0, 1.5, 2.0, 2.5]  # Just as an example\ngpu_utilization_this_work = [20, 50, 150, 180, 200]  # Just as an example\ngpu_utilization_mips = [0, 30, 100, 140, 170]  # Just as an example\nmemory_this_work = [10, 30, 40, 50, 60]  # Just as an example\nmemory_mips = [5, 25, 35, 45, 55]  # Just as an example\n\n# Create a new figure and axis\nfig, ax1 = plt.subplots()\n\n# Plot GPU utilization (TF/sec)\nax1.plot(serving_cost, gpu_utilization_this_work, 'ro-', label='GPU util (this work)')\nax1.plot(serving_cost, gpu_utilization_mips, 'bs-', label='GPU util (MIPS)')\nax1.set_xlabel('Serving Cost (TFLOPs)')\nax1.set_ylabel('GPU Utilization (TF/sec)', color='black')\nax1.tick_params('y', colors='black')\n\n# Create a second y-axis for memory usage\nax2 = ax1.twinx()\nax2.plot(serving_cost, memory_this_work, 'rs--', label='memory (this work)')\nax2.plot(serving_cost, memory_mips, 'bs--', label='memory (MIPS)')\nax2.set_ylabel('Peak GPU mem. usage (GB)', color='black')\nax2.tick_params('y', colors='black')\n\n# Add annotations\nax1.annotate('Baseline', (serving_cost[0], gpu_utilization_mips[0]), textcoords=\"offset points\", xytext=(-15,-10), ha='center')\nax1.annotate('This work', (serving_cost[-2], gpu_utilization_this_work[-2]), textcoords=\"offset points\", xytext=(5,10), ha='center')\n\n# Set the legend\nax1.legend(loc='upper left')\nax2.legend(loc='upper right')\n\n# Save the plot as a png file with 300dpi\nplt.savefig('gpu_utilization_memory.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04039/mips_vs_mol_gpu_util_mem_v1_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "kdd_figures/2306.04370/small_time_graph_auc.png",
        "caption": "Impact of period-varying modeling on results for different time periods on AUC metric in MT-small dataset.",
        "source": "kdd/2306.04370/Appendix.tex",
        "arxiv_id": "kdd/2306.04370",
        "llava_output": "The image is a line chart that shows the impact of period-varying modeling on results for different time periods on the AUC metric in the MT-small dataset. The chart includes various time periods, such as morning, noon, night, and late night, and the corresponding AUC values for",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data to mimic the visual appearance\ncategories = ['Morning', 'Noon', 'Night', 'Late Night', 'Full']\nn_categories = len(categories)\n\n# Random values around the chart heights from the image\nvalues_svd_gcn = np.random.uniform(0.81, 0.85, n_categories)\nvalues_svd_gcn_time = np.random.uniform(0.82, 0.86, n_categories)\nvalues_dvp_wot = np.random.uniform(0.83, 0.87, n_categories)\nvalues_dvp_t = np.random.uniform(0.82, 0.85, n_categories)\nvalues_dvp = np.random.uniform(0.84, 0.88, n_categories)\n\n# Bar settings\nbar_width = 0.15\nindex = np.arange(n_categories)\n\n# Plotting bars for each group\nplt.bar(index, values_svd_gcn, width=bar_width, label='SVD-GCN')\nplt.bar(index + bar_width, values_svd_gcn_time, width=bar_width, label='SVD-GCN(time emb)')\nplt.bar(index + 2 * bar_width, values_dvp_wot, width=bar_width, label='DPVP(w/o time)')\nplt.bar(index + 3 * bar_width, values_dvp_t, width=bar_width, label='DPVP(time emb)')\nplt.bar(index + 4 * bar_width, values_dvp, width=bar_width, label='DPVP')\n\n# Setting the x-axis labels\nplt.xticks(index + 2 * bar_width, categories)\n\n# Labeling the axes and the chart\nplt.xlabel('Time of Day')\nplt.ylabel('AUC')\nplt.title('Impact of period-varying modeling on results for different time periods on AUC metric in MT-small dataset')\n\n# Adding a legend\nplt.legend()\n\n# Saving the plot as a .png file with 300 dpi\nplt.savefig('auc_metric_mt_small_dataset.png', dpi=300)\n\n# Displaying the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04370/small_time_graph_auc_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 4,
            "color_rating": 3,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2306.04370/large_time_graph_auc.png",
        "caption": "Impact of period-varying modeling on results for different time periods on AUC metric in MT-large dataset.",
        "source": "kdd/2306.04370/Appendix.tex",
        "arxiv_id": "kdd/2306.04370",
        "llava_output": "This is a Bar Chart that presents the impact of period-varying modeling on results for different time periods on AUC metric in MT-large dataset.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Example categorical data\ntime_periods = ['Morning', 'Noon', 'Night', 'Late Night', 'Full']\nmodels = ['SVD-GCN', 'SVD-GCN(time emb)', 'DPVP(w/o time)', 'DPVP(time emb)', 'DPVP']\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n\n# Example data, replace with actual values\ndata = np.array([\n    [0.87, 0.86, 0.85, 0.88, 0.87],  # SVD-GCN\n    [0.86, 0.85, 0.87, 0.89, 0.88],  # SVD-GCN (time emb)\n    [0.85, 0.84, 0.85, 0.86, 0.86],  # DPVP (w/o time)\n    [0.87, 0.87, 0.88, 0.87, 0.89],  # DPVP (time emb)\n    [0.88, 0.89, 0.86, 0.85, 0.87]   # DPVP\n])\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Number of bars per group\nn_bars = len(models)\n\n# The width of a single bar\nbar_width = 1 / (n_bars + 1)\n\n# Positions of the left bar-boundaries\nbar_l = [i + 1 for i in range(len(data[0]))]\n\n# Positions of the centers of the bars\ntick_pos = [i + (bar_width / 2) for i in bar_l]\n\nfor i in range(n_bars):\n    # Plot bars\n    ax.bar([p + bar_width * i for p in bar_l],\n           data[i],\n           width=bar_width,\n           label=models[i],\n           color=colors[i])\n\n# Set the labels for the x ticks\nax.set_xticks(tick_pos)\nax.set_xticklabels(time_periods)\n\n# Set the label and legends\nax.set_ylabel('AUC')\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\n# Set a buffer around the edge\nplt.xlim(min(tick_pos) - bar_width, max(tick_pos) + bar_width * n_bars)\n\n# Save the figure with high resolution\nplt.savefig('bar_chart.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04370/large_time_graph_auc_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2306.04370/time.png",
        "caption": "Proportion distribution of clicks between users and five major food categories in four different time periods. Note that the sum of the proportions of the five food categories in each time period is equal to 1.",
        "source": "kdd/2306.04370/Introduction.tex",
        "arxiv_id": "kdd/2306.04370",
        "llava_output": "The image is a bar chart showing the proportion distribution of clicks between users and five major food categories in four different time periods.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\ncategories = ['Baozi', 'Porridge', 'Noddle', 'Dish', 'Barbecue']\nmorning_clicks = [0.35, 0.15, 0.10, 0.20, 0.05]\nnoon_clicks = [0.20, 0.10, 0.15, 0.30, 0.10]\nnight_clicks = [0.15, 0.20, 0.25, 0.25, 0.15]\nlate_night_clicks = [0.10, 0.05, 0.20, 0.25, 0.20]\n\nbar_width = 0.2\nindex = np.arange(len(categories))\n\n# Creating the bar chart\nplt.bar(index, morning_clicks, bar_width, label='Morning', color='blue')\nplt.bar(index + bar_width, noon_clicks, bar_width, label='Noon', color='orange')\nplt.bar(index + 2*bar_width, night_clicks, bar_width, label='Night', color='green')\nplt.bar(index + 3*bar_width, late_night_clicks, bar_width, label='Late Night', color='red')\n\n# Adding labels and title\nplt.xlabel('Food Category')\nplt.ylabel('Proportion of Clicks')\nplt.title('Distribution of Clicks among Food Categories')\nplt.xticks(index + bar_width + bar_width/2, categories)\nplt.legend()\n\n# Adjust layout to make room for the legend\nplt.tight_layout()\n\n# Saving the figure\nplt.savefig('clicks_distribution.png', dpi=300)\n\n# Displaying the plot\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04370/time_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "kdd_figures/2306.04643/benford.png",
        "caption": "Our observation of the dataset's first digit distribution vs. expectation of the distribution according to Benford's law.",
        "source": "kdd/2306.04643/main.tex",
        "arxiv_id": "kdd/2306.04643",
        "llava_output": "The image is a scatter plot that shows the distribution of the first digit of a dataset and the expected distribution according to Benford's law.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Dummy data for demonstration purposes\nfirst_digits = list(range(1, 10))  # Possible first digits (1-9)\nour_results = [30, 25, 20, 10, 5, 4, 3, 2, 1]  # Replace with your actual data\nbenford = [30, 17.5, 12.5, 10, 7.5, 6.5, 5.5, 5, 4.5]  # Replace with your actual data\n\n# Create the line plot\nplt.figure()\nplt.plot(first_digits, our_results, 'o-', label='Our result', color='blue')\nplt.plot(first_digits, benford, 'o-', label='Benford', color='red')\n\n# Configure the plot\nplt.xlabel('First Digit')\nplt.ylabel('Percentage')\nplt.title('Our observation of the dataset\\'s first digit distribution vs. expectation')\nplt.legend()\nplt.grid(True)\n\n# Save the plot as a PNG file with 300 DPI\nplt.savefig('benfords_law_comparison.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04643/benford_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "kdd_figures/2306.04643/select_k.png",
        "caption": "WCSS and DBI vs Number of Cluster Plot.",
        "source": "kdd/2306.04643/main.tex",
        "arxiv_id": "kdd/2306.04643",
        "llava_output": "16 6 5 4 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Placeholder data (replace with your actual values)\nclusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\nwcss = [1000000, 800000, 600000, 700000, 500000, 450000, 430000, 420000, 410000, 400000, 390000, 380000, 370000]  # replace with your WCSS values\ndbi = [1.7, 1.6, 1.5, 1.4, 1.5, 1.4, 1.3, 1.4, 1.5, 1.4, 1.3, 1.4, 1.5]  # replace with your DBI values\n\nfig, ax1 = plt.subplots()\n\n# Plot WCSS\ncolor = 'tab:blue'\nax1.set_xlabel('Number of Clusters')\nax1.set_ylabel('WCSS', color=color)\nax1.plot(clusters, wcss, color=color, marker='o')\nax1.tick_params(axis='y', labelcolor=color)\n\n# Create a twin axis for the DBI\nax2 = ax1.twinx()  \ncolor = 'tab:red'\nax2.set_ylabel('DBI', color=color)\nax2.plot(clusters, dbi, color=color, marker='o')\nax2.tick_params(axis='y', labelcolor=color)\n\n# Ensure tight layout and save the figure\nfig.tight_layout()\nplt.savefig('wcss_dbi_vs_number_of_clusters.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04643/select_k_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "kdd_figures/2306.04643/PCA.png",
        "caption": "PCA Visualization of the Clustering Result.",
        "source": "kdd/2306.04643/main.tex",
        "arxiv_id": "kdd/2306.04643",
        "llava_output": "The image is a scatter plot showing the clustering results of a PCA analysis.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming you have the following data (replace with your actual PCA results and labels)\n# Generate some random PCA data for illustration purposes\nnp.random.seed(0)\npca_data = np.random.rand(150, 2) * 100\nlabels = np.random.randint(0, 7, 150)\n\n# Each label corresponds to a color\ncolors = ['green', 'red', 'cyan', 'black', 'magenta', 'blue', 'purple']\n\n# Start plotting\nplt.figure(figsize=(10, 8))\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('PC2')\nplt.ylabel('PC1')\nplt.title('Clusters by PCA')\n\n# Plot each point, color it based on its cluster label\nfor label in np.unique(labels):\n    idx = labels == label\n    plt.scatter(pca_data[idx, 1], pca_data[idx, 0], label=str(label), alpha=0.5, edgecolors='w', c=colors[label])\n\n# Add a legend with a title\nplt.legend(title='labels', loc='upper left', bbox_to_anchor=(1, 1))\n\n# Save the figure with 300 DPI\nplt.savefig('pca_clusters.png', dpi=300, bbox_inches='tight')\n\n# Optionally, show the plot\nplt.show()",
        "code_cot": "",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04643/PCA_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 2
        }
    },
    {
        "figure_path": "kdd_figures/2306.04643/validation_sse.png",
        "caption": "Cross Validation SSE.",
        "source": "kdd/2306.04643/main.tex",
        "arxiv_id": "kdd/2306.04643",
        "llava_output": "1. The image is a line chart showing the results of a train test.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# This is sample data; you need to replace it with your actual data\ncross_val_iterations = list(range(1, 11))  # Example iterations from 1 to 10\ntrain_sse = [0.4, 0.5, 0.45, 0.6, 0.35, 0.4, 0.55, 0.45, 0.4, 0.6]  # Sample training SSE\ntest_sse = [0.6, 0.7, 0.65, 0.8, 0.55, 0.6, 0.75, 0.65, 0.6, 0.8]  # Sample testing SSE\n\nplt.plot(cross_val_iterations, train_sse, label='Train', color='blue')\nplt.plot(cross_val_iterations, test_sse, label='Test', color='red')\nplt.xlabel('Cross Validation Iterations')\nplt.ylabel('SSE')\nplt.title('Cross Validation SSE')\nplt.legend()\nplt.grid(True)\n\n# Save the figure as a png file with 300 dpi\nplt.savefig('cross_validation_sse.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.04643/validation_sse_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2306.09364/patch_embedd_fig.jpg",
        "caption": "Correlation between Patch time-series and its associated embeddings.",
        "source": "kdd/2306.09364/main.tex",
        "arxiv_id": "kdd/2306.09364",
        "llava_output": "The image is a scatter plot, showing the correlation between patch time-series and its associated embeddings.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Mock time-series data\ntime = np.linspace(0, 7, 100)\nseries1 = np.sin(time) + np.random.normal(0, 0.1, time.shape)\nseries2 = np.cos(time) + np.random.normal(0, 0.1, time.shape)\nseries3 = np.sin(2 * time) + np.random.normal(0, 0.1, time.shape)\n\n# Mock t-SNE embedding clusters\ntsne_x = np.random.normal(size=50)\ntsne_y = np.random.normal(size=50)\ntsne_cluster = np.random.choice([0, 1, 2, 3, 4], size=50)\n\n# Set color map for clusters\ncolors = ['green', 'red', 'blue', 'orange', 'black']\n\nfig = plt.figure(figsize=(10, 6))\n\n# Subplot for time series\nax1 = fig.add_subplot(121)\nax1.plot(time, series1, 'r-', alpha=0.1)\nax1.plot(time, series2, 'b-', alpha=0.1)\nax1.plot(time, series3, 'g-', alpha=0.1)\nax1.set_title(\"(a) Patch timeseries\")\nax1.set_xlabel(\"Time\")\n\n# Subplot for t-SNE clusters\nax2 = fig.add_subplot(122)\nfor cluster in np.unique(tsne_cluster):\n    idx = tsne_cluster == cluster\n    ax2.scatter(tsne_x[idx], tsne_y[idx], c=colors[cluster], label=f\"cluster_{cluster}\")\n\nax2.set_title(\"(b) Patch Embedding clusters (t-SNE)\")\nax2.legend()\n\n# Manually adding connectors for illustrative purposes\nfor i in range(5):\n    con_x = [time[-1], tsne_x[i]]\n    con_y = [series1[-1], tsne_y[i]]\n    ax1.plot(con_x, con_y, 'k--', alpha=0.5)\n\n    con_x = [time[-1], tsne_x[i + 15]]\n    con_y = [series2[-1], tsne_y[i + 15]]\n    ax1.plot(con_x, con_y, 'k--', alpha=0.5)\n\n    con_x = [time[-1], tsne_x[i + 30]]\n    con_y = [series3[-1], tsne_y[i + 30]]\n    ax1.plot(con_x, con_y, 'k--', alpha=0.5)\n\n# Adjust layout to prevent overlapping content\nplt.tight_layout()\n\n# Save figure with 300dpi\nplt.savefig('cluster_timeseries.png', dpi=300)\n\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.09364/patch_embedd_fig_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 1,
            "plot_type_correct": "No",
            "text_elements_rating": 1,
            "color_rating": 1,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2306.10079/image_encoder.png",
        "caption": "The precision of top-3/5 tags for the POIs on on MPTD2 predicted by the M3PTs with different image encoders.",
        "source": "kdd/2306.10079/042exp_correct.tex",
        "arxiv_id": "kdd/2306.10079",
        "llava_output": "The image is a bar chart that presents the precision of top-3/5 tags for the POIs on MPTD2 predicted by the M3PTs with different image encoders.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Accuracy values\ntop3_accuracy = [28.21, 43.05, 45.48, 84.19]\ntop5_accuracy = [45.19, 61.46, 63.04, 91.07]\n\n# X-axis labels\nx_labels = ['ViT', 'CLIP', 'BLIP', 'Ours']\n\n# X locations for the groups\nind = np.arange(len(top3_accuracy))\nwidth = 0.35\n\n# Plot\nfig, ax = plt.subplots()\n\n# Create bars\nbars1 = ax.bar(ind, top3_accuracy, width, label='Top-3')\nbars2 = ax.bar(ind + width, top5_accuracy, width, label='Top-5')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel('Image encoder')\nax.set_ylabel('Accuracy (%)')\nax.set_title('The precision of top-3/5 tags for the POIs on MPTD2 predicted by the M3PTs with different image encoders')\nax.set_xticks(ind + width / 2)\nax.set_xticklabels(x_labels)\nax.legend()\n\n# Add text annotations on top of the bars\nfor bar in bars1 + bars2:\n    height = bar.get_height()\n    ax.annotate('{:.2f}'.format(height),\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\n# Save the figure with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot if necessary\nplt.show()",
        "code_cot": "",
        "type": "Bar Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.10079/image_encoder_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 4,
            "numerical_accuracy_rating": 4,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "kdd_figures/2306.10079/score1.png",
        "caption": "The tuning results of prediction threshold $\\pi$ on the two datasets.",
        "source": "kdd/2306.10079/042exp_correct.tex",
        "arxiv_id": "kdd/2306.10079",
        "llava_output": "The image is a scatter plot showing the tuning results of prediction threshold $\\pi$ on the two datasets.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\n\n# Sample data\nthresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\nmap_scores = [60, 75, 85, 80, 70, 65]  # Replace with actual mAP scores\nm_f1_scores = [25, 45, 75, 55, 45, 35]  # Replace with actual M-F1 scores\nf1_e_scores = [25, 35, 65, 55, 50, 40]  # Replace with actual F1-e scores\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, map_scores, marker='o', color='red', label='mAP')\nplt.plot(thresholds, m_f1_scores, marker='s', color='purple', label='M-F1')\nplt.plot(thresholds, f1_e_scores, marker='*', color='green', label='F1-e')\n\n# Dashed line at pi=0.5\nplt.axvline(x=0.5, color='blue', linestyle='--', linewidth=1)\n\n# Add legend\nplt.legend(loc='upper right')\n\n# Label axes\nplt.xlabel(r'$\\pi$')\nplt.ylabel('Score (%)')\n\n# Set score limits and grid\nplt.ylim(20, 100)\nplt.grid(True)\n\n# Save the figure with 300 dpi\nplt.savefig('tuning_results.png', dpi=300)\n\n# Show the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.10079/score1_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 3,
            "line_styles_rating": 3,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2306.14126/non-defense.png",
        "caption": "Comparison of traffic forecasting model performance under adversarial attack. (a) Results without defense, showing biased predictions under attack. (b) Results with adversarial training, showing improved robustness and similar predictions to original model.",
        "source": "kdd/2306.14126/sample-lualatex.tex",
        "arxiv_id": "kdd/2306.14126",
        "llava_output": "The image is a Line Chart that presents the results of the experiment comparing the traffic forecasting model performance under adversarial attack. The chart shows the performance of the model without defense (a) and with adversarial training (b), allowing for a comparison of the robustness and accuracy of the predictions.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate some random data to simulate the plot\nnp.random.seed(0)  # For reproducibility\ntime_slots = np.arange(0, 300, 1)\nground_truth = np.random.normal(50, 10, len(time_slots)).cumsum() / 100\nground_truth = (ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min()) * 100\nprediction = ground_truth + np.random.normal(0, 5, len(time_slots))\nadversarial_prediction = prediction + np.random.normal(0, 20, len(time_slots))\n\n# Create the plot\nfig, ax = plt.subplots()\nax.plot(time_slots, prediction, label='prediction', color='red', linewidth=2)\nax.plot(time_slots, ground_truth, label='ground truth', color='blue', linestyle='--', linewidth=2)\nax.plot(time_slots, adversarial_prediction, label='adversarial prediction', color='green', linestyle='-.', linewidth=2)\n\nax.set_xlabel('Time Slot')\nax.set_ylabel('Traffic Speed')\nax.set_title('Comparison of Traffic Forecasting Model Performance')\nax.legend()\n\n# Save the plot as a PNG file with 300 dpi\nplt.savefig('traffic_forecasting_comparison.png', dpi=300)\nplt.close()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2306.14126/non-defense_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 2,
            "plot_type_correct": "Yes",
            "text_elements_rating": 2,
            "color_rating": 2,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "kdd_figures/2307.00653/MainGraph.png",
        "caption": "Comparison of NLM and backtracking convergence time",
        "source": "kdd/2307.00653/biblio.tex",
        "arxiv_id": "kdd/2307.00653",
        "llava_output": "The image is a Line Chart that compares the convergence time of NLM and backtracking.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# This is placeholder data. You need to replace it with your actual data.\nnumber_of_grids = np.arange(1, 51, 1)  # Change this as per your actual number of grids.\nrun_time_nlm = np.random.rand(50)  # Replace with actual NLM run times.\nrun_time_backtrack = np.random.rand(50)  # Replace with actual Backtrack run times.\n\n# Plotting both lines.\nplt.plot(number_of_grids, run_time_nlm, label='NLM', color='orange')\nplt.plot(number_of_grids, run_time_backtrack, label='Backtrack', color='blue')\n\n# Adding title and labels\nplt.title(\"Run Time: NLM vs Backtracking (Number of empty cells: 3)\")\nplt.xlabel(\"Number Of Grids\")\nplt.ylabel(\"Run Time\")\n\n# Showing the legend\nplt.legend()\n\n# Save the figure with a resolution of 300 DPI\nplt.savefig('comparison_plot.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2307.00653/MainGraph_direct.png",
        "eval": {
            "axes_grids_rating": 3,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 1,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 1,
            "figure_adaptability_rating": 1
        }
    },
    {
        "figure_path": "kdd_figures/2307.10213/Figure_2.jpg",
        "caption": "Confusion matrix for hate speech detection.",
        "source": "kdd/2307.10213/paper.tex",
        "arxiv_id": "kdd/2307.10213",
        "llava_output": "The image is a confusion matrix for hate speech detection. It shows the number of actual hate speech and actual not hate speech, as well as the number of predicted hate speech and predicted not hate speech. The matrix is presented in a table format with the actual and predicted values for each category.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Confusion matrix data\nmatrix = np.array([[45, 5], [15, 35]])\n\n# Create figure and axis object\nfig, ax = plt.subplots()\n\n# Display the confusion matrix using a heatmap\ncax = ax.matshow(matrix, cmap='Blues')\n\n# Add color bar\nplt.colorbar(cax)\n\n# Add text annotations inside the squares\nfor (i, j), val in np.ndenumerate(matrix):\n    ax.text(j, i, f'{val}', ha='center', va='center', color='black')\n\n# Set axis labels\nax.set_xlabel('Predicted labels')\nax.xaxis.set_label_position('top') \nax.set_ylabel('Actual labels')\n\n# Set tick labels\nax.set_xticks([0, 1])\nax.set_xticklabels(['Hate', 'Not Hate'])\nax.set_yticks([0, 1])\nax.set_yticklabels(['Actual Hate', 'Actual Not Hate'])\n\n# Rotate the tick labels for better visibility if necessary\nplt.xticks(rotation=0)\n\n# Set title\nplt.title('Confusion Matrix for hate speech detection')\n\n# Save the plot to a PNG file\nplt.savefig('confusion_matrix.png', dpi=300)\n\n# Display the plot\nplt.show()",
        "code_cot": "",
        "type": "Heat Map",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/kdd/2307.10213/Figure_2_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 5,
            "numerical_accuracy_rating": 5,
            "figure_adaptability_rating": 5
        }
    },
    {
        "figure_path": "neurips_figures/2307.04204/linear_depth2_width256_scale3.png",
        "caption": "$\\alpha=10$",
        "source": "neurips/2307.04204/3_GDalign.tex",
        "arxiv_id": "neurips/2307.04204",
        "llava_output": "This image is a scatter plot, showing the relationship between two variables. The x-axis represents the global minimum, and the y-axis represents the tanh(q). The scatter plot is filled with many small dots, indicating the distribution of the data points. The plot is labeled with the equation for",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the data for 'p' and the corresponding 'q' values are given or calculated\np = np.linspace(-2.5, 2.5, 100)  # Replace with your actual data or computation\nq = np.tanh(p) / p  # Replace with your actual data or computation\n\n# Points that represent the global minima, assuming known\n# You'll have to replace these with the actual coordinates\nglobal_minima_p = [-1, 1]  # Placeholder values\nglobal_minima_q = [0.5, 0.5]  # Placeholder values\n\n# Create the plot\nplt.figure(figsize=(6, 6))\nplt.plot(p, q, label=r'$q = \\frac{\\tanh(\\rho)}{\\rho}$', color='black')  # tanh(p)/p curve\nplt.scatter(global_minima_p, global_minima_q, color='red', label='Global minima')  # Global minima points\n\n# Customize the dashed line representing the global minima\nfor p_val, q_val in zip(global_minima_p, global_minima_q):\n    plt.axvline(x=p_val, color='black', linestyle='--', linewidth=1)\n\n# Additional points scattered, replace these with your actual data if needed\nadditional_p = np.random.uniform(-2.5, 2.5, 10)  # Placeholder values\nadditional_q = np.random.uniform(0.5, 2.5, 10)  # Placeholder values\nplt.scatter(additional_p, additional_q, c=np.random.rand(10, 3), edgecolor='none')\n\n# Set the x and y axis labels\nplt.xlabel(r'$\\rho$')\nplt.ylabel(r'$q$')\n\n# Add the legend\nplt.legend()\n\n# Set the title with alpha parameter\nplt.title(r'$\\alpha=10$')\n\n# Save the figure with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Set up the figure and axis\nfig, ax = plt.subplots()\n\n# Define the range for p\np = np.linspace(-3, 3, 400)\n\n# Plot the tanh(p) function\nax.plot(p, np.tanh(p), label='q = tanh(p)', color='black')\n\n# Let's assume the dashed line represents some function, I'll use p as an example\n# ax.plot(p, p, 'k--', label='p')  # Example function\n\n# For global minima, let's assume it's at p = 0, q = 0\nglobal_minima_p = 0\nglobal_minima_q = 0\nax.plot(global_minima_p, global_minima_q, 'ro', label='Global minima')\n\n# Add scatter points (randomly placed for the example)\nscatter_p = np.random.rand(10) * 2 - 1  # random values between -1 and 1\nscatter_q = np.random.rand(10) * 2 + 0.5  # random values between 0.5 and 2.5\nax.scatter(scatter_p, scatter_q)\n\n# Add a vertical dashed line at p = 0\nax.axvline(0, color='black', linestyle='--')\n\n# Add text for the global minima annotation\nax.text(-0.5, 0.1, 'Global minima', color='red', ha='center')\n\n# Set the legend\nax.legend()\n\n# Set the x and y axis labels\nax.set_xlabel(r'$\\rho$')\nax.set_ylabel(r'$q$')\n\n# Set the title with the caption alpha value\nax.set_title(r'$\\alpha=10$')\n\n# Save the figure with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/neurips/2307.04204/linear_depth2_width256_scale3_direct.png",
        "eval": {
            "axes_grids_rating": 2,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 4,
            "color_rating": 5,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 3
        }
    },
    {
        "figure_path": "neurips_figures/2307.04204/depth3_width64_scale5.png",
        "caption": "$m=256$, $L=3$",
        "source": "neurips/2307.04204/3_GDalign.tex",
        "arxiv_id": "neurips/2307.04204",
        "llava_output": "The image is a scatter plot showing the relationship between two variables. The x-axis represents the global minimization, and the y-axis represents the tanh(q/p). The plot consists of multiple points scattered across the graph, indicating the relationship between the two variables.</s>",
        "is_data_visual": "Yes",
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming the function to be plotted is q = tanh(p)/p\np = np.linspace(-10, 10, 400)\nq = np.tanh(p) / p\nq[p == 0] = 1  # Define the limit at p=0\n\n# Generate some random scatter points for illustration\nnp.random.seed(0)  # For reproducibility\nscatter_p = np.random.uniform(-10, 10, size=100)\nscatter_q = np.random.uniform(0, 1, size=100)\n\n# Create the figure and the line plot\nplt.figure(figsize=(6, 6))\nplt.plot(p, q, label=r'$q = \\frac{\\tanh(p)}{p}$')\nplt.scatter(scatter_p, scatter_q, c=np.random.rand(100,3))  # random colors\n\n# Add a vertical line for the global minimum\nplt.axvline(x=0, color='k', linestyle='--', label='Global minima')\n\n# Set the x and y axis labels\nplt.xlabel(r'$p$')\nplt.ylabel(r'$q$')\n\n# Add the legend\nplt.legend()\n\n# Set the x and y limits of the plot\nplt.xlim(-10, 10)\nplt.ylim(0, 1)\n\n# Add grid\nplt.grid(True)\n\n# Add caption\nplt.figtext(0.5, 0.01, r'$m=256$, $L=3$', wrap=True, horizontalalignment='center', fontsize=10)\n\n# Save the figure\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Define the tanh(\u03c1) function and \u03c1 range\ndef tanh_rho(rho):\n    return np.tanh(rho) / rho\n\nrho_values = np.linspace(-10, 10, 400)\nrho_values = rho_values[rho_values != 0]  # Avoid division by zero\ntanh_values = tanh_rho(rho_values)\n\n# Step 2: Plot the tanh(\u03c1) function\nplt.figure(figsize=(6, 6))\nplt.plot(rho_values, tanh_values, label='q = tanh(\u03c1)/\u03c1')\n\n# Step 3: Generate and plot random scattered points\n# Let's assume these points are randomly scattered around the tanh(\u03c1) curve\nnp.random.seed(0)  # For reproducibility\nscatter_rho = np.random.uniform(-10, 10, 256)\nscatter_q = tanh_rho(scatter_rho) + np.random.normal(0, 0.05, scatter_rho.size)\nplt.scatter(scatter_rho, scatter_q, c=np.random.rand(256), s=10)  # Random colors\n\n# Step 4: Add the dashed line for global minima\nplt.axvline(x=0, color='k', linestyle='--', label='Global minima')\n\n# Step 5: Add legend, labels, and title\nplt.legend()\nplt.xlabel('\u03c1')\nplt.ylabel('q')\nplt.title('$m=256$, $L=3$')\n\n# Step 6: Save the plot as a PNG file\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "type": "Scatter Plot",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/neurips/2307.04204/depth3_width64_scale5_direct.png",
        "eval": {
            "axes_grids_rating": 1,
            "tick_marks_rating": 3,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 5,
            "line_styles_rating": 2,
            "numerical_accuracy_rating": 2,
            "figure_adaptability_rating": 4
        }
    },
    {
        "figure_path": "neurips_figures/2307.04204/single_depth3_width256_scale0.5.png",
        "caption": "$\\alpha=1.0$",
        "source": "neurips/2307.04204/A1_exp.tex",
        "arxiv_id": "neurips/2307.04204",
        "llava_output": "The image is a scatter plot showing the relationship between two variables.</s>",
        "is_data_visual": "Yes",
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a figure and axis with a specific size\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Generate some data points for p and q with a random scatter\np = np.linspace(-2, 2, 100)\nq = np.tanh(p)  # This is the tanh function\n\n# Add some noise to q to simulate the scatter data\nnoise = np.random.normal(0, 0.02, p.shape)\nq_noisy = q + noise\n\n# Plot the tanh function\nax.plot(p, q, label='q = tanh(p)', color='black')\n\n# Add some random points for illustrative purposes\nfor _ in range(20):\n    scatter_p = np.random.uniform(-2, 2)\n    scatter_q = np.tanh(scatter_p) + np.random.normal(0, 0.05)\n    ax.scatter(scatter_p, scatter_q, color=np.random.rand(3,))\n\n# Draw a vertical dashed line at p = 0\nax.axvline(0, color='black', linestyle='--', label=r'$\\rho = 0$')\n\n# Add legend\nax.legend()\n\n# Set the same scale for both axes\nax.axis('equal')\n\n# Set the limits for the x and y axes\nax.set_xlim(-2.5, 2.5)\nax.set_ylim(0.5, 1.0)\n\n# Set the labels for the axes\nax.set_xlabel(r'$\\rho$')\nax.set_ylabel(r'$q$')\n\n# Add the caption\nfig.text(0.5, 0.02, r'$\\alpha=1.0$', ha='center')\n\n# Save the figure with 300 DPI\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot\nplt.show()\n",
        "code_cot": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Prepare the data\np = np.linspace(-2.5, 2.5, 100)\nq = np.tanh(p)\n\n# Create the plot\nplt.figure()\nplt.plot(p, q, label='q = tanh(p)')\nplt.axvline(0, color='k', linestyle='--', label=r'$\\rho = 0$')\n\n# Scatter some points (placeholders)\nscatter_points_p = np.random.uniform(-2, 2, 10)\nscatter_points_q = np.tanh(scatter_points_p)\nplt.scatter(scatter_points_p, scatter_points_q, c='r')\n\n# Customize the plot\nplt.xlim([-2.5, 2.5])\nplt.ylim([0, 1])\nplt.xlabel('p')\nplt.ylabel('q')\nplt.title(r'$\\alpha=1.0$')\nplt.legend()\n\n# Save the plot\nplt.savefig('plot.png', dpi=300)\n\n# Show the plot (optional)\nplt.show()\n",
        "type": "Line Chart",
        "if_annotate": true,
        "runnable": true,
        "output_figure_path": "chat-gpt-v/neurips/2307.04204/single_depth3_width256_scale0.5_direct.png",
        "eval": {
            "axes_grids_rating": 4,
            "tick_marks_rating": 4,
            "plot_type_correct": "Yes",
            "text_elements_rating": 3,
            "color_rating": 4,
            "line_styles_rating": 1,
            "numerical_accuracy_rating": 3,
            "figure_adaptability_rating": 4
        }
    }
]